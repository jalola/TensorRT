{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e954d2-755c-43a5-ac58-81ef578dcc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "409c5807-387b-4c31-bafd-d198fe48ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d309ef6f-e25a-4b9c-98d3-dac8847f71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/ec2-user/working/lib/python/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e00d1972-61c0-45c2-9d0e-860081d732cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/working/lib/python/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# huggingface\n",
    "# from transformers import (\n",
    "#     GPT2LMHeadModel,\n",
    "#     GPT2Tokenizer,\n",
    "#     GPT2Config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510deb98-a498-45e4-9b32-8e5d92782f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b4a5a5-f937-439e-b223-a8e0e78cff15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.33.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e814cc8c-e40b-45f5-93f0-a9dee3cc6aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 16,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 128,\n",
       "  \"max_position_embeddings\": 128,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.33.2\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"llama-xs\"\n",
    "max_length = 20\n",
    "\n",
    "xs_config = LlamaConfig(\n",
    "    hidden_size=4*4,\n",
    "    intermediate_size=128,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    num_key_value_heads=None,\n",
    "    hidden_act='silu',\n",
    "    max_position_embeddings=128,\n",
    "    initializer_range=0.02,\n",
    "    rms_norm_eps=1e-06,\n",
    "    use_cache=False,\n",
    "    pad_token_id=None,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pretraining_tp=1,\n",
    "    tie_word_embeddings=False,\n",
    "    rope_theta=10000.0,\n",
    ")\n",
    "xs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40299b3-baa6-4661-9ff0-a741fdd8987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM(xs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1ce361-bcdf-4a82-8084-496a670110b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42fc4dc-978a-42b3-aa04-d6cd90634cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hey, are you conscious? Can you talk to me?ï¿½ bowl waited 1024secut pitch !=345 mph Censusispers climbAround Cel principavor African reproduction'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/llama-xs/pytorch\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(model_name)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4af5ada0-9358-46f0-a682-f9722cd5f027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/llama-xs/pytorch'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c5766-97ed-4d04-bab5-7fa18e89dee8",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e43067c2-ecd9-4bd6-9047-a3f74621931b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# carry out inference with a single sample\n",
    "input_str = \"Hello, my dog is a dog\"\n",
    "inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d347ddf-4504-4ab7-b15b-29d218bdd7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15496,    11,   616,  3290,   318,   257,  3290]]),\n",
       " torch.Size([1, 7]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf83454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu\n"
     ]
    }
   ],
   "source": [
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    print(\"Using cpu\")\n",
    "    model = model.cpu()\n",
    "    input_ids = input_ids.cpu()\n",
    "    inputs = inputs.to('cpu')\n",
    "else:\n",
    "    print(\"Using gpu\")\n",
    "    model = model.cuda()\n",
    "    input_ids = input_ids.cuda()\n",
    "    inputs = inputs.to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6c2ea-3450-4b8b-9cc8-09943d967ece",
   "metadata": {},
   "source": [
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b844f057-e768-467d-9185-68fb4c74b5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs['input_ids'], use_cache = False)\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "717b2f68-9d92-474e-9937-8b42a1c60d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0661,  0.1066,  0.0253,  ..., -0.0424, -0.0400,  0.0622],\n",
       "          [ 0.0667,  0.0418,  0.1152,  ..., -0.0344,  0.1413,  0.0101],\n",
       "          [-0.0968, -0.0149,  0.0534,  ..., -0.0528,  0.1104, -0.0157],\n",
       "          ...,\n",
       "          [-0.0481, -0.0632, -0.0397,  ..., -0.0672,  0.1063, -0.0620],\n",
       "          [-0.0099, -0.0088, -0.0292,  ...,  0.0577, -0.0259, -0.0685],\n",
       "          [-0.0105, -0.0950, -0.2092,  ..., -0.0007, -0.1921, -0.0915]]],\n",
       "        device='cuda:0'),\n",
       " torch.Size([1, 7, 32000]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX format\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format: ONNX.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "At a high level, the steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU with the TensorRT engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b79f67ad-080a-4d85-926f-d2812e672794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkMetadata(variant='llama-xs', precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=False))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "from GPT2.GPT2ModelConfig import GPT2Metadata\n",
    "metadata = NetworkMetadata(variant=model_name, precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=False))\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58be125b-7d33-49ad-af84-1558f88251f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/working/lib/python/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Module\n",
    "from transformers.generation_utils import GenerationMixin\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "class TorchModule(Module, GenerationMixin):\n",
    "    \"\"\"\n",
    "    A simplied definition of Llama.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llama_model, lm_head, config):\n",
    "        super().__init__()\n",
    "        self.llama_model = llama_model\n",
    "        self.lm_head = lm_head\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda') # WAR to avoid beam search in framework\n",
    "        self.main_input_name = \"input_ids\" # For better HuggingFace version compatibility\n",
    "\n",
    "    # def prepare_inputs_for_generation(self, input_ids, past = None, use_cache=None, **kwargs):\n",
    "    #     # Todo (@pchadha): add position_ids, token_type_ids support\n",
    "    #     # cut decoder_input_ids if past is used\n",
    "    #     if past is not None:\n",
    "    #         input_ids = input_ids[:, -1:]\n",
    "\n",
    "    #     return {\n",
    "    #         \"input_ids\": input_ids,\n",
    "    #         \"use_cache\": use_cache,\n",
    "    #         \"past_key_values\": past\n",
    "    #     }\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        outputs = self.llama_model(input_ids, **kwargs)\n",
    "        hidden_states = outputs[0]\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            logits=lm_logits, \n",
    "            past_key_values=outputs.past_key_values\n",
    "        )\n",
    "\n",
    "    # def _reorder_cache(self, past, beam_idx):\n",
    "    #     \"\"\"\n",
    "    #     This function is used to re-order the :obj:`past_key_values` cache if\n",
    "    #     :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n",
    "    #     called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n",
    "    #     \"\"\"\n",
    "    #     return tuple(\n",
    "    #         tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "    #         for layer_past in past\n",
    "    #     )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "238d93a9-e511-4bcf-a2ca-6ca0eeccd567",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = ('input_ids',)\n",
    "output_names = ('logits',)\n",
    "input_dynamic_axis = {'input_ids': {0: 'batch', 1: 'sequence'}}\n",
    "output_dynamic_axis = {'logits': {0: 'batch', 1: 'sequence'}}\n",
    "\n",
    "opt_args = {}\n",
    "\n",
    "output_fpath = ('./models/{}/ONNX/{}.onnx'.format(model_name, model_name))\n",
    "output_fpath_sim = ('./models/{}/ONNX/{}_sim.onnx'.format(model_name, model_name))\n",
    "Path(output_fpath).parent.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68c4ba2c-8450-40d6-80b3-a8eeebd5cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = TorchModule(model.model, model.lm_head, xs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71e01fc7-3670-4db1-9798-7bded28fca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/working/lib/python/transformers/models/llama/modeling_llama.py:597: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/ec2-user/working/lib/python/transformers/models/llama/modeling_llama.py:119: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/home/ec2-user/working/lib/python/transformers/models/llama/modeling_llama.py:350: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "/home/ec2-user/working/lib/python/transformers/models/llama/modeling_llama.py:357: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "/home/ec2-user/working/lib/python/transformers/models/llama/modeling_llama.py:367: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n"
     ]
    }
   ],
   "source": [
    "old_forward = llama_model.forward\n",
    "def _export_forward(input_ids, **kwargs):\n",
    "    kwargs[\"use_cache\"] = False\n",
    "    result = old_forward(input_ids, **kwargs)\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "llama_model.forward = _export_forward\n",
    "\n",
    "torch.onnx.export(\n",
    "    llama_model,\n",
    "    input_ids,\n",
    "    output_fpath,\n",
    "    opset_version=13,\n",
    "    do_constant_folding=True,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes={\n",
    "        **input_dynamic_axis,\n",
    "        **output_dynamic_axis,\n",
    "    },\n",
    "    training=torch.onnx.TrainingMode.EVAL,\n",
    "    **opt_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba8a16bc-165f-4d58-87af-e847c9a179dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/llama-xs/ONNX/llama-xs.onnx'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed8bc4ef-c2e7-4bfc-8c12-81e0e2d26c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !onnxsim $output_fpath $output_fpath_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93a14724-5283-44fe-840c-ff9b77438894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.1M\n",
      "-rw-r--r-- 1 root root  575 Sep 20 20:51 config.json\n",
      "-rw-r--r-- 1 root root  133 Sep 20 20:51 generation_config.json\n",
      "-rw-r--r-- 1 root root 4.1M Sep 20 20:51 pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $pytorch_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eb21049-5722-4e78-b073-45ee53ea4fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 4.2M Sep 20 20:51 ./models/llama-xs/ONNX/llama-xs.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $output_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cab5652c-3e18-4fe9-bc9b-2bc1a9d5dc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.342756"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(output_fpath).stat().st_size / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b04de1-e887-445c-9bc8-e2a7e0fca7ea",
   "metadata": {},
   "source": [
    "Let's take a look at the onnx file and investigate its input and output. You should see that \"input_ids\" as the input, and \"logits\" as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29c82042-b6d7-4739-beec-2f1093f69440",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = output_fpath\n",
    "onnx_path_sim = output_fpath_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e4fff25-97da-4f9f-ae98-e918745faebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03c409e6-d312-4cc7-b13f-4621609d5633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2314caaf-836d-4140-93e4-4b3f4c931347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input_ids\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fe7a8d4-2bc3-49fc-863a-0e7f4be6565e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"logits\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 32000\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.graph.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT engine\n",
    "\n",
    "Now we are ready to parse the ONNX model and convert it to an optimized TensorRT model.\n",
    "\n",
    "Since the model contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile.\n",
    "\n",
    "Note: As TensorRT carries out many optimization, this conversion process for the larger model might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from polygraphy.backend.trt import Profile\n",
    "from tensorrt import PreviewFeature\n",
    "from GPT2.export import GPT2ONNXFile, GPT2TRTEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./models/$model_name/trt-engine\n",
    "trt_engine_folder = './models/{}/trt-engine'.format(model_name)\n",
    "\n",
    "# Create optimization profile for dynamic shape input. Can modify batch_size / max_sequence_length to build engines for different shapes\n",
    "batch_size = 1\n",
    "disable_preview_dynamic_shapes = False # preview_dynamic_shapes optimizes the trt engine building time\n",
    "# We can either use input length as the optimal length, or use max_length // 2. \n",
    "# In T5 or BART, input_length is better, but in GPT-2, max_length // 2 is better because we need to generate max_length number of tokens\n",
    "\n",
    "use_input_length = False\n",
    "opt_length = input_id.shape[1] if use_input_length else max_length // 2 \n",
    "# Create different engine tags for different configurations\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noPreviewFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features += [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "\n",
    "profiles = [Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, opt_length), # Optimized based on the inputs. \n",
    "    max=(batch_size, max_length),\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5538106b-3ae4-4d5f-b0ee-1f76174dcecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Profile().add('input_ids', min=(1, 1), opt=(1, 10), max=(1, 20))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7852a4d8-ebcd-46be-b725-3ed77fc621a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805: 1>,\n",
       " <PreviewFeature.FASTER_DYNAMIC_SHAPES_0805: 0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a5934f0-46d3-45d7-8dd5-6cf81de61e66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] 'colored' module is not installed, will not use colors when logging. To enable colors, please install the 'colored' module: python3 -m pip install colored\n",
      "[V] Loaded Module: tensorrt | Version: 8.6.1 | Path: ['/usr/local/lib/python3.10/dist-packages/tensorrt']\n",
      "[V] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 2075, GPU 213 (MiB)\n",
      "[X] Trying to load shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[X] Loaded shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[V] [MemUsageChange] Init builder kernel library: CPU +889, GPU +172, now: CPU 3041, GPU 385 (MiB)\n",
      "[X] CUDA lazy loading is enabled.\n",
      "[V] ----------------------------------------------------------------\n",
      "[V] Input filename:   ./models/llama-xs/ONNX/llama-xs.onnx\n",
      "[V] ONNX IR version:  0.0.7\n",
      "[V] Opset version:    13\n",
      "[V] Producer name:    pytorch\n",
      "[V] Producer version: 1.13.1\n",
      "[V] Domain:           \n",
      "[V] Model version:    0\n",
      "[V] Doc string:       \n",
      "[V] ----------------------------------------------------------------\n",
      "[X] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1\n",
      "[X] Registered plugin creator - ::BatchedNMS_TRT version 1\n",
      "[X] Registered plugin creator - ::BatchTilePlugin_TRT version 1\n",
      "[X] Registered plugin creator - ::Clip_TRT version 1\n",
      "[X] Registered plugin creator - ::CoordConvAC version 1\n",
      "[X] Registered plugin creator - ::CropAndResizeDynamic version 1\n",
      "[X] Registered plugin creator - ::CropAndResize version 1\n",
      "[X] Registered plugin creator - ::DecodeBbox3DPlugin version 1\n",
      "[X] Registered plugin creator - ::DetectionLayer_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_TRT version 1\n",
      "[X] Registered plugin creator - ::FlattenConcat_TRT version 1\n",
      "[X] Registered plugin creator - ::GenerateDetection_TRT version 1\n",
      "[X] Registered plugin creator - ::GridAnchor_TRT version 1\n",
      "[X] Registered plugin creator - ::GridAnchorRect_TRT version 1\n",
      "[X] Registered plugin creator - ::InstanceNormalization_TRT version 1\n",
      "[X] Registered plugin creator - ::InstanceNormalization_TRT version 2\n",
      "[X] Registered plugin creator - ::LReLU_TRT version 1\n",
      "[X] Registered plugin creator - ::ModulatedDeformConv2d version 1\n",
      "[X] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1\n",
      "[X] Registered plugin creator - ::MultilevelProposeROI_TRT version 1\n",
      "[X] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[X] Registered plugin creator - ::NMSDynamic_TRT version 1\n",
      "[X] Registered plugin creator - ::NMS_TRT version 1\n",
      "[X] Registered plugin creator - ::Normalize_TRT version 1\n",
      "[X] Registered plugin creator - ::PillarScatterPlugin version 1\n",
      "[X] Registered plugin creator - ::PriorBox_TRT version 1\n",
      "[X] Registered plugin creator - ::ProposalDynamic version 1\n",
      "[X] Registered plugin creator - ::ProposalLayer_TRT version 1\n",
      "[X] Registered plugin creator - ::Proposal version 1\n",
      "[X] Registered plugin creator - ::PyramidROIAlign_TRT version 1\n",
      "[X] Registered plugin creator - ::Region_TRT version 1\n",
      "[X] Registered plugin creator - ::Reorg_TRT version 1\n",
      "[X] Registered plugin creator - ::ResizeNearest_TRT version 1\n",
      "[X] Registered plugin creator - ::ROIAlign_TRT version 1\n",
      "[X] Registered plugin creator - ::RPROI_TRT version 1\n",
      "[X] Registered plugin creator - ::ScatterND version 1\n",
      "[X] Registered plugin creator - ::SpecialSlice_TRT version 1\n",
      "[X] Registered plugin creator - ::Split version 1\n",
      "[X] Registered plugin creator - ::VoxelGeneratorPlugin version 1\n",
      "[X] Adding network input: input_ids with dtype: int32, dimensions: (-1, -1)\n",
      "[X] Registering tensor: input_ids for ONNX tensor: input_ids\n",
      "[X] Importing initializer: llama_model.embed_tokens.weight\n",
      "[X] Importing initializer: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Importing initializer: onnx::MatMul_907\n",
      "[X] Importing initializer: onnx::MatMul_908\n",
      "[X] Importing initializer: onnx::MatMul_909\n",
      "[X] Importing initializer: onnx::MatMul_929\n",
      "[X] Importing initializer: onnx::MatMul_930\n",
      "[X] Importing initializer: onnx::MatMul_931\n",
      "[X] Importing initializer: onnx::MatMul_932\n",
      "[X] Importing initializer: onnx::MatMul_933\n",
      "[X] Importing initializer: onnx::MatMul_934\n",
      "[X] Importing initializer: onnx::MatMul_935\n",
      "[X] Importing initializer: onnx::MatMul_955\n",
      "[X] Importing initializer: onnx::MatMul_956\n",
      "[X] Importing initializer: onnx::MatMul_957\n",
      "[X] Importing initializer: onnx::MatMul_958\n",
      "[X] Importing initializer: onnx::MatMul_959\n",
      "[X] Importing initializer: onnx::MatMul_960\n",
      "[X] Importing initializer: onnx::MatMul_961\n",
      "[X] Importing initializer: onnx::MatMul_981\n",
      "[X] Importing initializer: onnx::MatMul_982\n",
      "[X] Importing initializer: onnx::MatMul_983\n",
      "[X] Importing initializer: onnx::MatMul_984\n",
      "[X] Importing initializer: onnx::MatMul_985\n",
      "[X] Importing initializer: onnx::MatMul_986\n",
      "[X] Importing initializer: onnx::MatMul_987\n",
      "[X] Importing initializer: onnx::MatMul_1007\n",
      "[X] Importing initializer: onnx::MatMul_1008\n",
      "[X] Importing initializer: onnx::MatMul_1009\n",
      "[X] Importing initializer: onnx::MatMul_1010\n",
      "[X] Importing initializer: onnx::MatMul_1011\n",
      "[X] Parsing node: Identity_0 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_0 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Registering layer: llama_model.layers.0.input_layernorm.weight for ONNX node: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Registering layer: Identity_0 for ONNX node: Identity_0\n",
      "[X] Registering tensor: llama_model.norm.weight for ONNX tensor: llama_model.norm.weight\n",
      "[X] Identity_0 [Identity] outputs: [llama_model.norm.weight -> (16)[FLOAT]], \n",
      "[X] Parsing node: Identity_1 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_1 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Registering layer: Identity_1 for ONNX node: Identity_1\n",
      "[X] Registering tensor: llama_model.layers.3.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.3.post_attention_layernorm.weight\n",
      "[X] Identity_1 [Identity] outputs: [llama_model.layers.3.post_attention_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Parsing node: Identity_2 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_2 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Registering layer: Identity_2 for ONNX node: Identity_2\n",
      "[X] Registering tensor: llama_model.layers.3.input_layernorm.weight for ONNX tensor: llama_model.layers.3.input_layernorm.weight\n",
      "[X] Identity_2 [Identity] outputs: [llama_model.layers.3.input_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Parsing node: Identity_3 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_3 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Registering layer: Identity_3 for ONNX node: Identity_3\n",
      "[X] Registering tensor: llama_model.layers.2.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.2.post_attention_layernorm.weight\n",
      "[X] Identity_3 [Identity] outputs: [llama_model.layers.2.post_attention_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Parsing node: Identity_4 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_4 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Registering layer: Identity_4 for ONNX node: Identity_4\n",
      "[X] Registering tensor: llama_model.layers.2.input_layernorm.weight for ONNX tensor: llama_model.layers.2.input_layernorm.weight\n",
      "[X] Identity_4 [Identity] outputs: [llama_model.layers.2.input_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Parsing node: Identity_5 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_5 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Registering layer: Identity_5 for ONNX node: Identity_5\n",
      "[X] Registering tensor: llama_model.layers.1.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.1.post_attention_layernorm.weight\n",
      "[X] Identity_5 [Identity] outputs: [llama_model.layers.1.post_attention_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Parsing node: Identity_6 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_6 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Registering layer: Identity_6 for ONNX node: Identity_6\n",
      "[X] Registering tensor: llama_model.layers.1.input_layernorm.weight for ONNX tensor: llama_model.layers.1.input_layernorm.weight\n",
      "[X] Identity_6 [Identity] outputs: [llama_model.layers.1.input_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Parsing node: Identity_7 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_7 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Registering layer: Identity_7 for ONNX node: Identity_7\n",
      "[X] Registering tensor: llama_model.layers.0.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.0.post_attention_layernorm.weight\n",
      "[X] Identity_7 [Identity] outputs: [llama_model.layers.0.post_attention_layernorm.weight -> (16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant [Constant]\n",
      "[X] llama_model/Constant [Constant] inputs: \n",
      "[W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[X] llama_model/Constant [Constant] outputs: [llama_model/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Shape [Shape]\n",
      "[X] Searching for input: input_ids\n",
      "[X] llama_model/Shape [Shape] inputs: [input_ids -> (-1, -1)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape for ONNX node: llama_model/Shape\n",
      "[X] Registering tensor: llama_model/Shape_output_0 for ONNX tensor: llama_model/Shape_output_0\n",
      "[X] llama_model/Shape [Shape] outputs: [llama_model/Shape_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_1 [Constant]\n",
      "[X] llama_model/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/Constant_1 [Constant] outputs: [llama_model/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather [Gather]\n",
      "[X] Searching for input: llama_model/Shape_output_0\n",
      "[X] Searching for input: llama_model/Constant_1_output_0\n",
      "[X] llama_model/Gather [Gather] inputs: [llama_model/Shape_output_0 -> (2)[INT32]], [llama_model/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_1_output_0 for ONNX node: llama_model/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather for ONNX node: llama_model/Gather\n",
      "[X] Registering tensor: llama_model/Gather_output_0 for ONNX tensor: llama_model/Gather_output_0\n",
      "[X] llama_model/Gather [Gather] outputs: [llama_model/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_1 [Shape]\n",
      "[X] Searching for input: input_ids\n",
      "[X] llama_model/Shape_1 [Shape] inputs: [input_ids -> (-1, -1)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape_1 for ONNX node: llama_model/Shape_1\n",
      "[X] Registering tensor: llama_model/Shape_1_output_0 for ONNX tensor: llama_model/Shape_1_output_0\n",
      "[X] llama_model/Shape_1 [Shape] outputs: [llama_model/Shape_1_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_2 [Constant]\n",
      "[X] llama_model/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/Constant_2 [Constant] outputs: [llama_model/Constant_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_2_output_0\n",
      "[X] llama_model/Gather_1 [Gather] inputs: [llama_model/Shape_1_output_0 -> (2)[INT32]], [llama_model/Constant_2_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_2_output_0 for ONNX node: llama_model/Constant_2_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather_1 for ONNX node: llama_model/Gather_1\n",
      "[X] Registering tensor: llama_model/Gather_1_output_0 for ONNX tensor: llama_model/Gather_1_output_0\n",
      "[X] llama_model/Gather_1 [Gather] outputs: [llama_model/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_3 [Constant]\n",
      "[X] llama_model/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/Constant_3 [Constant] outputs: [llama_model/Constant_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Cast [Cast]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] llama_model/Cast [Cast] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/Cast for ONNX node: llama_model/Cast\n",
      "[X] Registering tensor: llama_model/Cast_output_0 for ONNX tensor: llama_model/Cast_output_0\n",
      "[X] llama_model/Cast [Cast] outputs: [llama_model/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_4 [Constant]\n",
      "[X] llama_model/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/Constant_4 [Constant] outputs: [llama_model/Constant_4_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Range [Range]\n",
      "[X] Searching for input: llama_model/Constant_3_output_0\n",
      "[X] Searching for input: llama_model/Cast_output_0\n",
      "[X] Searching for input: llama_model/Constant_4_output_0\n",
      "[X] llama_model/Range [Range] inputs: [llama_model/Constant_3_output_0 -> ()[INT32]], [llama_model/Cast_output_0 -> ()[INT32]], [llama_model/Constant_4_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Range for ONNX node: llama_model/Range\n",
      "[X] Registering tensor: llama_model/Range_output_0 for ONNX tensor: llama_model/Range_output_0\n",
      "[X] llama_model/Range [Range] outputs: [llama_model/Range_output_0 -> (-1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_5 [Constant]\n",
      "[X] llama_model/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/Constant_5 [Constant] outputs: [llama_model/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Range_output_0\n",
      "[X] Searching for input: llama_model/Constant_5_output_0\n",
      "[X] llama_model/Unsqueeze [Unsqueeze] inputs: [llama_model/Range_output_0 -> (-1)[INT32]], [llama_model/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_,), unsqueezing to: (_, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze for ONNX node: llama_model/Unsqueeze\n",
      "[X] Registering tensor: llama_model/Unsqueeze_output_0 for ONNX tensor: llama_model/Unsqueeze_output_0\n",
      "[X] llama_model/Unsqueeze [Unsqueeze] outputs: [llama_model/Unsqueeze_output_0 -> (1, -1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_6 [Constant]\n",
      "[X] llama_model/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/Constant_6 [Constant] outputs: [llama_model/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_22 [Constant]\n",
      "[X] Constant_22 [Constant] inputs: \n",
      "[X] Constant_22 [Constant] outputs: [onnx::Unsqueeze_58 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_58\n",
      "[X] llama_model/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_58 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_1 for ONNX node: llama_model/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/Unsqueeze_1_output_0 for ONNX tensor: llama_model/Unsqueeze_1_output_0\n",
      "[X] llama_model/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat [Concat]\n",
      "[X] Searching for input: llama_model/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_1_output_0\n",
      "[X] llama_model/Concat [Concat] inputs: [llama_model/Constant_6_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_6_output_0 for ONNX node: llama_model/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/Concat for ONNX node: llama_model/Concat\n",
      "[X] Registering tensor: llama_model/Concat_output_0 for ONNX tensor: llama_model/Concat_output_0\n",
      "[X] llama_model/Concat [Concat] outputs: [llama_model/Concat_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/Concat_output_0\n",
      "[X] llama_model/Reshape [Reshape] inputs: [llama_model/Unsqueeze_output_0 -> (1, -1)[INT32]], [llama_model/Concat_output_0 -> (2)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape for ONNX node: llama_model/Reshape\n",
      "[X] Registering tensor: llama_model/Reshape_output_0 for ONNX tensor: llama_model/Reshape_output_0\n",
      "[X] llama_model/Reshape [Reshape] outputs: [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Parsing node: llama_model/embed_tokens/Gather [Gather]\n",
      "[X] Searching for input: llama_model.embed_tokens.weight\n",
      "[X] Searching for input: input_ids\n",
      "[X] llama_model/embed_tokens/Gather [Gather] inputs: [llama_model.embed_tokens.weight -> (32000, 16)[FLOAT]], [input_ids -> (-1, -1)[INT32]], \n",
      "[X] Registering layer: llama_model.embed_tokens.weight for ONNX node: llama_model.embed_tokens.weight\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/embed_tokens/Gather for ONNX node: llama_model/embed_tokens/Gather\n",
      "[X] Registering tensor: llama_model/embed_tokens/Gather_output_0 for ONNX tensor: llama_model/embed_tokens/Gather_output_0\n",
      "[X] llama_model/embed_tokens/Gather [Gather] outputs: [llama_model/embed_tokens/Gather_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: Constant_27 [Constant]\n",
      "[X] Constant_27 [Constant] inputs: \n",
      "[X] Constant_27 [Constant] outputs: [onnx::Unsqueeze_63 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_63\n",
      "[X] llama_model/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_63 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_2 for ONNX node: llama_model/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/Unsqueeze_2_output_0 for ONNX tensor: llama_model/Unsqueeze_2_output_0\n",
      "[X] llama_model/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_29 [Constant]\n",
      "[X] Constant_29 [Constant] inputs: \n",
      "[X] Constant_29 [Constant] outputs: [onnx::Unsqueeze_65 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_65\n",
      "[X] llama_model/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_65 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_3 for ONNX node: llama_model/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/Unsqueeze_3_output_0 for ONNX tensor: llama_model/Unsqueeze_3_output_0\n",
      "[X] llama_model/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_3_output_0\n",
      "[X] llama_model/Concat_1 [Concat] inputs: [llama_model/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Concat_1 for ONNX node: llama_model/Concat_1\n",
      "[X] Registering tensor: llama_model/Concat_1_output_0 for ONNX tensor: llama_model/Concat_1_output_0\n",
      "[X] llama_model/Concat_1 [Concat] outputs: [llama_model/Concat_1_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Concat_1_output_0\n",
      "[X] llama_model/ConstantOfShape [ConstantOfShape] inputs: [llama_model/Concat_1_output_0 -> (2)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_output_0 for ONNX tensor: llama_model/ConstantOfShape_output_0\n",
      "[X] llama_model/ConstantOfShape [ConstantOfShape] outputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Parsing node: Constant_33 [Constant]\n",
      "[X] Constant_33 [Constant] inputs: \n",
      "[X] Constant_33 [Constant] outputs: [onnx::Unsqueeze_69 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_69\n",
      "[X] llama_model/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_69 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_4 for ONNX node: llama_model/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/Unsqueeze_4_output_0 for ONNX tensor: llama_model/Unsqueeze_4_output_0\n",
      "[X] llama_model/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_35 [Constant]\n",
      "[X] Constant_35 [Constant] inputs: \n",
      "[X] Constant_35 [Constant] outputs: [onnx::Unsqueeze_71 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_71\n",
      "[X] llama_model/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_71 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_5 for ONNX node: llama_model/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/Unsqueeze_5_output_0 for ONNX tensor: llama_model/Unsqueeze_5_output_0\n",
      "[X] llama_model/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_5_output_0\n",
      "[X] llama_model/Concat_2 [Concat] inputs: [llama_model/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Concat_2 for ONNX node: llama_model/Concat_2\n",
      "[X] Registering tensor: llama_model/Concat_2_output_0 for ONNX tensor: llama_model/Concat_2_output_0\n",
      "[X] llama_model/Concat_2 [Concat] outputs: [llama_model/Concat_2_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape_1 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Concat_2_output_0\n",
      "[X] llama_model/ConstantOfShape_1 [ConstantOfShape] inputs: [llama_model/Concat_2_output_0 -> (2)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_1_output_0 for ONNX tensor: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/ConstantOfShape_1 [ConstantOfShape] outputs: [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/Shape_2 [Shape] inputs: [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Shape_2 for ONNX node: llama_model/Shape_2\n",
      "[X] Registering tensor: llama_model/Shape_2_output_0 for ONNX tensor: llama_model/Shape_2_output_0\n",
      "[X] llama_model/Shape_2 [Shape] outputs: [llama_model/Shape_2_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_7 [Constant]\n",
      "[X] llama_model/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/Constant_7 [Constant] outputs: [llama_model/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_8 [Constant]\n",
      "[X] llama_model/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/Constant_8 [Constant] outputs: [llama_model/Constant_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_9 [Constant]\n",
      "[X] llama_model/Constant_9 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[W] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n",
      "[X] llama_model/Constant_9 [Constant] outputs: [llama_model/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Slice [Slice]\n",
      "[X] Searching for input: llama_model/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_8_output_0\n",
      "[X] Searching for input: llama_model/Constant_9_output_0\n",
      "[X] Searching for input: llama_model/Constant_7_output_0\n",
      "[X] llama_model/Slice [Slice] inputs: [llama_model/Shape_2_output_0 -> (2)[INT32]], [llama_model/Constant_8_output_0 -> (1)[INT32]], [llama_model/Constant_9_output_0 -> (1)[INT32]], [llama_model/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Slice for ONNX node: llama_model/Slice\n",
      "[X] Registering tensor: llama_model/Slice_output_0 for ONNX tensor: llama_model/Slice_output_0\n",
      "[X] llama_model/Slice [Slice] outputs: [llama_model/Slice_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_10 [Constant]\n",
      "[X] llama_model/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/Constant_10 [Constant] outputs: [llama_model/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Squeeze [Squeeze]\n",
      "[X] Searching for input: llama_model/Slice_output_0\n",
      "[X] Searching for input: llama_model/Constant_10_output_0\n",
      "[X] llama_model/Squeeze [Squeeze] inputs: [llama_model/Slice_output_0 -> (1)[INT32]], [llama_model/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1,), squeezing to: ()\n",
      "[X] Registering layer: llama_model/Squeeze for ONNX node: llama_model/Squeeze\n",
      "[X] Registering tensor: llama_model/Squeeze_output_0 for ONNX tensor: llama_model/Squeeze_output_0\n",
      "[X] llama_model/Squeeze [Squeeze] outputs: [llama_model/Squeeze_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/Squeeze_output_0\n",
      "[X] llama_model/Cast_1 [Cast] inputs: [llama_model/Squeeze_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/Cast_1 for ONNX node: llama_model/Cast_1\n",
      "[X] Registering tensor: llama_model/Cast_1_output_0 for ONNX tensor: llama_model/Cast_1_output_0\n",
      "[X] llama_model/Cast_1 [Cast] outputs: [llama_model/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_11 [Constant]\n",
      "[X] llama_model/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/Constant_11 [Constant] outputs: [llama_model/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_12 [Constant]\n",
      "[X] llama_model/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/Constant_12 [Constant] outputs: [llama_model/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Range_1 [Range]\n",
      "[X] Searching for input: llama_model/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_12_output_0\n",
      "[X] llama_model/Range_1 [Range] inputs: [llama_model/Constant_11_output_0 -> ()[INT32]], [llama_model/Cast_1_output_0 -> ()[INT32]], [llama_model/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Range_1 for ONNX node: llama_model/Range_1\n",
      "[X] Registering tensor: llama_model/Range_1_output_0 for ONNX tensor: llama_model/Range_1_output_0\n",
      "[X] llama_model/Range_1 [Range] outputs: [llama_model/Range_1_output_0 -> (-1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_13 [Constant]\n",
      "[X] llama_model/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/Constant_13 [Constant] outputs: [llama_model/Constant_13_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Add [Add]\n",
      "[X] Searching for input: llama_model/Range_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_13_output_0\n",
      "[X] llama_model/Add [Add] inputs: [llama_model/Range_1_output_0 -> (-1)[INT32]], [llama_model/Constant_13_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_13_output_0 for ONNX node: llama_model/Constant_13_output_0\n",
      "[X] Registering layer: llama_model/Add for ONNX node: llama_model/Add\n",
      "[X] Registering tensor: llama_model/Add_output_0 for ONNX tensor: llama_model/Add_output_0\n",
      "[X] llama_model/Add [Add] outputs: [llama_model/Add_output_0 -> (-1)[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/Shape_3 [Shape] inputs: [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Shape_3 for ONNX node: llama_model/Shape_3\n",
      "[X] Registering tensor: llama_model/Shape_3_output_0 for ONNX tensor: llama_model/Shape_3_output_0\n",
      "[X] llama_model/Shape_3 [Shape] outputs: [llama_model/Shape_3_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_14 [Constant]\n",
      "[X] llama_model/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/Constant_14 [Constant] outputs: [llama_model/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_15 [Constant]\n",
      "[X] llama_model/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/Constant_15 [Constant] outputs: [llama_model/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_16 [Constant]\n",
      "[X] llama_model/Constant_16 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/Constant_16 [Constant] outputs: [llama_model/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_15_output_0\n",
      "[X] Searching for input: llama_model/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/Constant_14_output_0\n",
      "[X] llama_model/Slice_1 [Slice] inputs: [llama_model/Shape_3_output_0 -> (2)[INT32]], [llama_model/Constant_15_output_0 -> (1)[INT32]], [llama_model/Constant_16_output_0 -> (1)[INT32]], [llama_model/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Slice_1 for ONNX node: llama_model/Slice_1\n",
      "[X] Registering tensor: llama_model/Slice_1_output_0 for ONNX tensor: llama_model/Slice_1_output_0\n",
      "[X] llama_model/Slice_1 [Slice] outputs: [llama_model/Slice_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_17 [Constant]\n",
      "[X] llama_model/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/Constant_17 [Constant] outputs: [llama_model/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Squeeze_1 [Squeeze]\n",
      "[X] Searching for input: llama_model/Slice_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_17_output_0\n",
      "[X] llama_model/Squeeze_1 [Squeeze] inputs: [llama_model/Slice_1_output_0 -> (1)[INT32]], [llama_model/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1,), squeezing to: ()\n",
      "[X] Registering layer: llama_model/Squeeze_1 for ONNX node: llama_model/Squeeze_1\n",
      "[X] Registering tensor: llama_model/Squeeze_1_output_0 for ONNX tensor: llama_model/Squeeze_1_output_0\n",
      "[X] llama_model/Squeeze_1 [Squeeze] outputs: [llama_model/Squeeze_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_59 [Constant]\n",
      "[X] Constant_59 [Constant] inputs: \n",
      "[X] Constant_59 [Constant] outputs: [onnx::Unsqueeze_95 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Squeeze_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_95\n",
      "[X] llama_model/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/Squeeze_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_95 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_6 for ONNX node: llama_model/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/Unsqueeze_6_output_0 for ONNX tensor: llama_model/Unsqueeze_6_output_0\n",
      "[X] llama_model/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_18 [Constant]\n",
      "[X] llama_model/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/Constant_18 [Constant] outputs: [llama_model/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/Constant_18_output_0\n",
      "[X] llama_model/Concat_3 [Concat] inputs: [llama_model/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_18_output_0 for ONNX node: llama_model/Constant_18_output_0\n",
      "[X] Registering layer: llama_model/Concat_3 for ONNX node: llama_model/Concat_3\n",
      "[X] Registering tensor: llama_model/Concat_3_output_0 for ONNX tensor: llama_model/Concat_3_output_0\n",
      "[X] llama_model/Concat_3 [Concat] outputs: [llama_model/Concat_3_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/Add_output_0\n",
      "[X] Searching for input: llama_model/Concat_3_output_0\n",
      "[X] llama_model/Reshape_1 [Reshape] inputs: [llama_model/Add_output_0 -> (-1)[INT32]], [llama_model/Concat_3_output_0 -> (2)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape_1 for ONNX node: llama_model/Reshape_1\n",
      "[X] Registering tensor: llama_model/Reshape_1_output_0 for ONNX tensor: llama_model/Reshape_1_output_0\n",
      "[X] llama_model/Reshape_1 [Reshape] outputs: [llama_model/Reshape_1_output_0 -> (-1, 1)[INT32]], \n",
      "[X] Parsing node: llama_model/Less [Less]\n",
      "[X] Searching for input: llama_model/Range_1_output_0\n",
      "[X] Searching for input: llama_model/Reshape_1_output_0\n",
      "[X] llama_model/Less [Less] inputs: [llama_model/Range_1_output_0 -> (-1)[INT32]], [llama_model/Reshape_1_output_0 -> (-1, 1)[INT32]], \n",
      "[X] Registering layer: llama_model/Less for ONNX node: llama_model/Less\n",
      "[X] Registering tensor: llama_model/Less_output_0 for ONNX tensor: llama_model/Less_output_0\n",
      "[X] llama_model/Less [Less] outputs: [llama_model/Less_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/Less_output_0\n",
      "[X] llama_model/Cast_2 [Cast] inputs: [llama_model/Less_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Casting to type: bool\n",
      "[X] Registering layer: llama_model/Cast_2 for ONNX node: llama_model/Cast_2\n",
      "[X] Registering tensor: llama_model/Cast_2_output_0 for ONNX tensor: llama_model/Cast_2_output_0\n",
      "[X] llama_model/Cast_2 [Cast] outputs: [llama_model/Cast_2_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_19 [Constant]\n",
      "[X] llama_model/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/Constant_19 [Constant] outputs: [llama_model/Constant_19_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/Where [Where]\n",
      "[X] Searching for input: llama_model/Cast_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/Where [Where] inputs: [llama_model/Cast_2_output_0 -> (-1, -1)[BOOL]], [llama_model/Constant_19_output_0 -> ()[FLOAT]], [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Constant_19_output_0 for ONNX node: llama_model/Constant_19_output_0\n",
      "[X] Registering layer: llama_model/Where for ONNX node: llama_model/Where\n",
      "[X] Registering tensor: llama_model/Where_output_0 for ONNX tensor: llama_model/Where_output_0\n",
      "[X] llama_model/Where [Where] outputs: [llama_model/Where_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/Where_output_0\n",
      "[X] llama_model/Cast_3 [Cast] inputs: [llama_model/Where_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/Cast_3 for ONNX node: llama_model/Cast_3\n",
      "[X] Registering tensor: llama_model/Cast_3_output_0 for ONNX tensor: llama_model/Cast_3_output_0\n",
      "[X] llama_model/Cast_3 [Cast] outputs: [llama_model/Cast_3_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_20 [Constant]\n",
      "[X] llama_model/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/Constant_20 [Constant] outputs: [llama_model/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_20_output_0\n",
      "[X] llama_model/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/Cast_3_output_0 -> (-1, -1)[FLOAT]], [llama_model/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_, _), unsqueezing to: (_, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_7 for ONNX node: llama_model/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/Unsqueeze_7_output_0 for ONNX tensor: llama_model/Unsqueeze_7_output_0\n",
      "[X] llama_model/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/Unsqueeze_7_output_0 -> (1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_21 [Constant]\n",
      "[X] llama_model/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/Constant_21 [Constant] outputs: [llama_model/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/Constant_21_output_0\n",
      "[X] llama_model/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/Unsqueeze_7_output_0 -> (1, -1, -1)[FLOAT]], [llama_model/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, _), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_8 for ONNX node: llama_model/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/Unsqueeze_8_output_0 for ONNX tensor: llama_model/Unsqueeze_8_output_0\n",
      "[X] llama_model/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/Unsqueeze_8_output_0 -> (1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_22 [Constant]\n",
      "[X] llama_model/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/Constant_22 [Constant] outputs: [llama_model/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_output_0\n",
      "[X] Searching for input: llama_model/Constant_22_output_0\n",
      "[X] llama_model/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/Gather_output_0 -> ()[INT32]], [llama_model/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_9 for ONNX node: llama_model/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/Unsqueeze_9_output_0 for ONNX tensor: llama_model/Unsqueeze_9_output_0\n",
      "[X] llama_model/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_23 [Constant]\n",
      "[X] llama_model/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/Constant_23 [Constant] outputs: [llama_model/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_24 [Constant]\n",
      "[X] llama_model/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/Constant_24 [Constant] outputs: [llama_model/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_24_output_0\n",
      "[X] llama_model/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [llama_model/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_10 for ONNX node: llama_model/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/Unsqueeze_10_output_0 for ONNX tensor: llama_model/Unsqueeze_10_output_0\n",
      "[X] llama_model/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_25 [Constant]\n",
      "[X] llama_model/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/Constant_25 [Constant] outputs: [llama_model/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_25_output_0\n",
      "[X] llama_model/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [llama_model/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_11 for ONNX node: llama_model/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/Unsqueeze_11_output_0 for ONNX tensor: llama_model/Unsqueeze_11_output_0\n",
      "[X] llama_model/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_11_output_0\n",
      "[X] llama_model/Concat_4 [Concat] inputs: [llama_model/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/Constant_23_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_23_output_0 for ONNX node: llama_model/Constant_23_output_0\n",
      "[X] Registering layer: llama_model/Concat_4 for ONNX node: llama_model/Concat_4\n",
      "[X] Registering tensor: llama_model/Concat_4_output_0 for ONNX tensor: llama_model/Concat_4_output_0\n",
      "[X] llama_model/Concat_4 [Concat] outputs: [llama_model/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_26 [Constant]\n",
      "[X] llama_model/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/Constant_26 [Constant] outputs: [llama_model/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/Constant_26_output_0\n",
      "[X] llama_model/Reshape_2 [Reshape] inputs: [llama_model/Concat_4_output_0 -> (4)[INT32]], [llama_model/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape_2 for ONNX node: llama_model/Reshape_2\n",
      "[X] Registering tensor: llama_model/Reshape_2_output_0 for ONNX tensor: llama_model/Reshape_2_output_0\n",
      "[X] llama_model/Reshape_2 [Reshape] outputs: [llama_model/Reshape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/Reshape_2_output_0\n",
      "[X] llama_model/Shape_4 [Shape] inputs: [llama_model/Reshape_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape_4 for ONNX node: llama_model/Shape_4\n",
      "[X] Registering tensor: llama_model/Shape_4_output_0 for ONNX tensor: llama_model/Shape_4_output_0\n",
      "[X] llama_model/Shape_4 [Shape] outputs: [llama_model/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape_2 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Shape_4_output_0\n",
      "[X] llama_model/ConstantOfShape_2 [ConstantOfShape] inputs: [llama_model/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_2_output_0 for ONNX tensor: llama_model/ConstantOfShape_2_output_0\n",
      "[X] llama_model/ConstantOfShape_2 [ConstantOfShape] outputs: [llama_model/ConstantOfShape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_27 [Constant]\n",
      "[X] llama_model/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/Constant_27 [Constant] outputs: [llama_model/Constant_27_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Mul [Mul]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_27_output_0\n",
      "[X] llama_model/Mul [Mul] inputs: [llama_model/ConstantOfShape_2_output_0 -> (4)[INT32]], [llama_model/Constant_27_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_27_output_0 for ONNX node: llama_model/Constant_27_output_0\n",
      "[X] Registering layer: llama_model/Mul for ONNX node: llama_model/Mul\n",
      "[X] Registering tensor: llama_model/Mul_output_0 for ONNX tensor: llama_model/Mul_output_0\n",
      "[X] llama_model/Mul [Mul] outputs: [llama_model/Mul_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Equal [Equal]\n",
      "[X] Searching for input: llama_model/Reshape_2_output_0\n",
      "[X] Searching for input: llama_model/Mul_output_0\n",
      "[X] llama_model/Equal [Equal] inputs: [llama_model/Reshape_2_output_0 -> (4)[INT32]], [llama_model/Mul_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Equal for ONNX node: llama_model/Equal\n",
      "[X] Registering tensor: llama_model/Equal_output_0 for ONNX tensor: llama_model/Equal_output_0\n",
      "[X] llama_model/Equal [Equal] outputs: [llama_model/Equal_output_0 -> (4)[BOOL]], \n",
      "[X] Parsing node: llama_model/Where_1 [Where]\n",
      "[X] Searching for input: llama_model/Equal_output_0\n",
      "[X] Searching for input: llama_model/ConstantOfShape_2_output_0\n",
      "[X] Searching for input: llama_model/Reshape_2_output_0\n",
      "[X] llama_model/Where_1 [Where] inputs: [llama_model/Equal_output_0 -> (4)[BOOL]], [llama_model/ConstantOfShape_2_output_0 -> (4)[INT32]], [llama_model/Reshape_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Where_1 for ONNX node: llama_model/Where_1\n",
      "[X] Registering tensor: llama_model/Where_1_output_0 for ONNX tensor: llama_model/Where_1_output_0\n",
      "[X] llama_model/Where_1 [Where] outputs: [llama_model/Where_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Expand [Expand]\n",
      "[X] Searching for input: llama_model/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/Where_1_output_0\n",
      "[X] llama_model/Expand [Expand] inputs: [llama_model/Unsqueeze_8_output_0 -> (1, 1, -1, -1)[FLOAT]], [llama_model/Where_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Expand for ONNX node: llama_model/Expand\n",
      "[X] Registering tensor: llama_model/Expand_output_0 for ONNX tensor: llama_model/Expand_output_0\n",
      "[X] llama_model/Expand [Expand] outputs: [llama_model/Expand_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Shape_5 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_output_0\n",
      "[X] llama_model/Shape_5 [Shape] inputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Registering layer: llama_model/Shape_5 for ONNX node: llama_model/Shape_5\n",
      "[X] Registering tensor: llama_model/Shape_5_output_0 for ONNX tensor: llama_model/Shape_5_output_0\n",
      "[X] llama_model/Shape_5 [Shape] outputs: [llama_model/Shape_5_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_28 [Constant]\n",
      "[X] llama_model/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/Constant_28 [Constant] outputs: [llama_model/Constant_28_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/Shape_5_output_0\n",
      "[X] Searching for input: llama_model/Constant_28_output_0\n",
      "[X] llama_model/Gather_2 [Gather] inputs: [llama_model/Shape_5_output_0 -> (2)[INT32]], [llama_model/Constant_28_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_28_output_0 for ONNX node: llama_model/Constant_28_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather_2 for ONNX node: llama_model/Gather_2\n",
      "[X] Registering tensor: llama_model/Gather_2_output_0 for ONNX tensor: llama_model/Gather_2_output_0\n",
      "[X] llama_model/Gather_2 [Gather] outputs: [llama_model/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_6 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_output_0\n",
      "[X] llama_model/Shape_6 [Shape] inputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Registering layer: llama_model/Shape_6 for ONNX node: llama_model/Shape_6\n",
      "[X] Registering tensor: llama_model/Shape_6_output_0 for ONNX tensor: llama_model/Shape_6_output_0\n",
      "[X] llama_model/Shape_6 [Shape] outputs: [llama_model/Shape_6_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_29 [Constant]\n",
      "[X] llama_model/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/Constant_29 [Constant] outputs: [llama_model/Constant_29_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/Shape_6_output_0\n",
      "[X] Searching for input: llama_model/Constant_29_output_0\n",
      "[X] llama_model/Gather_3 [Gather] inputs: [llama_model/Shape_6_output_0 -> (2)[INT32]], [llama_model/Constant_29_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_29_output_0 for ONNX node: llama_model/Constant_29_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather_3 for ONNX node: llama_model/Gather_3\n",
      "[X] Registering tensor: llama_model/Gather_3_output_0 for ONNX tensor: llama_model/Gather_3_output_0\n",
      "[X] llama_model/Gather_3 [Gather] outputs: [llama_model/Gather_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_30 [Constant]\n",
      "[X] llama_model/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/Constant_30 [Constant] outputs: [llama_model/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_output_0\n",
      "[X] Searching for input: llama_model/Constant_30_output_0\n",
      "[X] llama_model/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], [llama_model/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_, _), unsqueezing to: (_, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_12 for ONNX node: llama_model/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/Unsqueeze_12_output_0 for ONNX tensor: llama_model/Unsqueeze_12_output_0\n",
      "[X] llama_model/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/Unsqueeze_12_output_0 -> (-1, 1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_31 [Constant]\n",
      "[X] llama_model/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/Constant_31 [Constant] outputs: [llama_model/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/Constant_31_output_0\n",
      "[X] llama_model/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/Unsqueeze_12_output_0 -> (-1, 1, -1)[BOOL]], [llama_model/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_, 1, _), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_13 for ONNX node: llama_model/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/Unsqueeze_13_output_0 for ONNX tensor: llama_model/Unsqueeze_13_output_0\n",
      "[X] llama_model/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/Unsqueeze_13_output_0 -> (-1, 1, 1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_32 [Constant]\n",
      "[X] llama_model/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/Constant_32 [Constant] outputs: [llama_model/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_14 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_32_output_0\n",
      "[X] llama_model/Unsqueeze_14 [Unsqueeze] inputs: [llama_model/Gather_2_output_0 -> ()[INT32]], [llama_model/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_14 for ONNX node: llama_model/Unsqueeze_14\n",
      "[X] Registering tensor: llama_model/Unsqueeze_14_output_0 for ONNX tensor: llama_model/Unsqueeze_14_output_0\n",
      "[X] llama_model/Unsqueeze_14 [Unsqueeze] outputs: [llama_model/Unsqueeze_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_33 [Constant]\n",
      "[X] llama_model/Constant_33 [Constant] inputs: \n",
      "[X] llama_model/Constant_33 [Constant] outputs: [llama_model/Constant_33_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_34 [Constant]\n",
      "[X] llama_model/Constant_34 [Constant] inputs: \n",
      "[X] llama_model/Constant_34 [Constant] outputs: [llama_model/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_15 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_34_output_0\n",
      "[X] llama_model/Unsqueeze_15 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [llama_model/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_15 for ONNX node: llama_model/Unsqueeze_15\n",
      "[X] Registering tensor: llama_model/Unsqueeze_15_output_0 for ONNX tensor: llama_model/Unsqueeze_15_output_0\n",
      "[X] llama_model/Unsqueeze_15 [Unsqueeze] outputs: [llama_model/Unsqueeze_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_35 [Constant]\n",
      "[X] llama_model/Constant_35 [Constant] inputs: \n",
      "[X] llama_model/Constant_35 [Constant] outputs: [llama_model/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_16 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_35_output_0\n",
      "[X] llama_model/Unsqueeze_16 [Unsqueeze] inputs: [llama_model/Gather_3_output_0 -> ()[INT32]], [llama_model/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_16 for ONNX node: llama_model/Unsqueeze_16\n",
      "[X] Registering tensor: llama_model/Unsqueeze_16_output_0 for ONNX tensor: llama_model/Unsqueeze_16_output_0\n",
      "[X] llama_model/Unsqueeze_16 [Unsqueeze] outputs: [llama_model/Unsqueeze_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_14_output_0\n",
      "[X] Searching for input: llama_model/Constant_33_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_15_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_16_output_0\n",
      "[X] llama_model/Concat_5 [Concat] inputs: [llama_model/Unsqueeze_14_output_0 -> (1)[INT32]], [llama_model/Constant_33_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_15_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_16_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_33_output_0 for ONNX node: llama_model/Constant_33_output_0\n",
      "[X] Registering layer: llama_model/Concat_5 for ONNX node: llama_model/Concat_5\n",
      "[X] Registering tensor: llama_model/Concat_5_output_0 for ONNX tensor: llama_model/Concat_5_output_0\n",
      "[X] llama_model/Concat_5 [Concat] outputs: [llama_model/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_36 [Constant]\n",
      "[X] llama_model/Constant_36 [Constant] inputs: \n",
      "[X] llama_model/Constant_36 [Constant] outputs: [llama_model/Constant_36_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/Concat_5_output_0\n",
      "[X] Searching for input: llama_model/Constant_36_output_0\n",
      "[X] llama_model/Reshape_3 [Reshape] inputs: [llama_model/Concat_5_output_0 -> (4)[INT32]], [llama_model/Constant_36_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape_3 for ONNX node: llama_model/Reshape_3\n",
      "[X] Registering tensor: llama_model/Reshape_3_output_0 for ONNX tensor: llama_model/Reshape_3_output_0\n",
      "[X] llama_model/Reshape_3 [Reshape] outputs: [llama_model/Reshape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_7 [Shape]\n",
      "[X] Searching for input: llama_model/Reshape_3_output_0\n",
      "[X] llama_model/Shape_7 [Shape] inputs: [llama_model/Reshape_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape_7 for ONNX node: llama_model/Shape_7\n",
      "[X] Registering tensor: llama_model/Shape_7_output_0 for ONNX tensor: llama_model/Shape_7_output_0\n",
      "[X] llama_model/Shape_7 [Shape] outputs: [llama_model/Shape_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape_3 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Shape_7_output_0\n",
      "[X] llama_model/ConstantOfShape_3 [ConstantOfShape] inputs: [llama_model/Shape_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_3_output_0 for ONNX tensor: llama_model/ConstantOfShape_3_output_0\n",
      "[X] llama_model/ConstantOfShape_3 [ConstantOfShape] outputs: [llama_model/ConstantOfShape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_37 [Constant]\n",
      "[X] llama_model/Constant_37 [Constant] inputs: \n",
      "[X] llama_model/Constant_37 [Constant] outputs: [llama_model/Constant_37_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_37_output_0\n",
      "[X] llama_model/Mul_1 [Mul] inputs: [llama_model/ConstantOfShape_3_output_0 -> (4)[INT32]], [llama_model/Constant_37_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_37_output_0 for ONNX node: llama_model/Constant_37_output_0\n",
      "[X] Registering layer: llama_model/Mul_1 for ONNX node: llama_model/Mul_1\n",
      "[X] Registering tensor: llama_model/Mul_1_output_0 for ONNX tensor: llama_model/Mul_1_output_0\n",
      "[X] llama_model/Mul_1 [Mul] outputs: [llama_model/Mul_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Equal_1 [Equal]\n",
      "[X] Searching for input: llama_model/Reshape_3_output_0\n",
      "[X] Searching for input: llama_model/Mul_1_output_0\n",
      "[X] llama_model/Equal_1 [Equal] inputs: [llama_model/Reshape_3_output_0 -> (4)[INT32]], [llama_model/Mul_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Equal_1 for ONNX node: llama_model/Equal_1\n",
      "[X] Registering tensor: llama_model/Equal_1_output_0 for ONNX tensor: llama_model/Equal_1_output_0\n",
      "[X] llama_model/Equal_1 [Equal] outputs: [llama_model/Equal_1_output_0 -> (4)[BOOL]], \n",
      "[X] Parsing node: llama_model/Where_2 [Where]\n",
      "[X] Searching for input: llama_model/Equal_1_output_0\n",
      "[X] Searching for input: llama_model/ConstantOfShape_3_output_0\n",
      "[X] Searching for input: llama_model/Reshape_3_output_0\n",
      "[X] llama_model/Where_2 [Where] inputs: [llama_model/Equal_1_output_0 -> (4)[BOOL]], [llama_model/ConstantOfShape_3_output_0 -> (4)[INT32]], [llama_model/Reshape_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Where_2 for ONNX node: llama_model/Where_2\n",
      "[X] Registering tensor: llama_model/Where_2_output_0 for ONNX tensor: llama_model/Where_2_output_0\n",
      "[X] llama_model/Where_2 [Where] outputs: [llama_model/Where_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Expand_1 [Expand]\n",
      "[X] Searching for input: llama_model/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/Where_2_output_0\n",
      "[X] llama_model/Expand_1 [Expand] inputs: [llama_model/Unsqueeze_13_output_0 -> (-1, 1, 1, -1)[BOOL]], [llama_model/Where_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Expand_1 for ONNX node: llama_model/Expand_1\n",
      "[X] Registering tensor: llama_model/Expand_1_output_0 for ONNX tensor: llama_model/Expand_1_output_0\n",
      "[X] llama_model/Expand_1 [Expand] outputs: [llama_model/Expand_1_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/Expand_1_output_0\n",
      "[X] llama_model/Cast_4 [Cast] inputs: [llama_model/Expand_1_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/Cast_4 for ONNX node: llama_model/Cast_4\n",
      "[X] Registering tensor: llama_model/Cast_4_output_0 for ONNX tensor: llama_model/Cast_4_output_0\n",
      "[X] llama_model/Cast_4 [Cast] outputs: [llama_model/Cast_4_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_38 [Constant]\n",
      "[X] llama_model/Constant_38 [Constant] inputs: \n",
      "[X] llama_model/Constant_38 [Constant] outputs: [llama_model/Constant_38_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/Sub [Sub]\n",
      "[X] Searching for input: llama_model/Constant_38_output_0\n",
      "[X] Searching for input: llama_model/Cast_4_output_0\n",
      "[X] llama_model/Sub [Sub] inputs: [llama_model/Constant_38_output_0 -> ()[FLOAT]], [llama_model/Cast_4_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Constant_38_output_0 for ONNX node: llama_model/Constant_38_output_0\n",
      "[X] Registering layer: llama_model/Sub for ONNX node: llama_model/Sub\n",
      "[X] Registering tensor: llama_model/Sub_output_0 for ONNX tensor: llama_model/Sub_output_0\n",
      "[X] llama_model/Sub [Sub] outputs: [llama_model/Sub_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/Sub_output_0\n",
      "[X] llama_model/Cast_5 [Cast] inputs: [llama_model/Sub_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: bool\n",
      "[X] Registering layer: llama_model/Cast_5 for ONNX node: llama_model/Cast_5\n",
      "[X] Registering tensor: llama_model/Cast_5_output_0 for ONNX tensor: llama_model/Cast_5_output_0\n",
      "[X] llama_model/Cast_5 [Cast] outputs: [llama_model/Cast_5_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Cast_6 [Cast]\n",
      "[X] Searching for input: llama_model/Cast_5_output_0\n",
      "[X] llama_model/Cast_6 [Cast] inputs: [llama_model/Cast_5_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Casting to type: bool\n",
      "[X] Registering layer: llama_model/Cast_6 for ONNX node: llama_model/Cast_6\n",
      "[X] Registering tensor: llama_model/Cast_6_output_0 for ONNX tensor: llama_model/Cast_6_output_0\n",
      "[X] llama_model/Cast_6 [Cast] outputs: [llama_model/Cast_6_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_39 [Constant]\n",
      "[X] llama_model/Constant_39 [Constant] inputs: \n",
      "[X] llama_model/Constant_39 [Constant] outputs: [llama_model/Constant_39_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/Where_3 [Where]\n",
      "[X] Searching for input: llama_model/Cast_6_output_0\n",
      "[X] Searching for input: llama_model/Constant_39_output_0\n",
      "[X] Searching for input: llama_model/Sub_output_0\n",
      "[X] llama_model/Where_3 [Where] inputs: [llama_model/Cast_6_output_0 -> (-1, 1, -1, -1)[BOOL]], [llama_model/Constant_39_output_0 -> ()[FLOAT]], [llama_model/Sub_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Constant_39_output_0 for ONNX node: llama_model/Constant_39_output_0\n",
      "[X] Registering layer: llama_model/Where_3 for ONNX node: llama_model/Where_3\n",
      "[X] Registering tensor: llama_model/Where_3_output_0 for ONNX tensor: llama_model/Where_3_output_0\n",
      "[X] llama_model/Where_3 [Where] outputs: [llama_model/Where_3_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Cast_7 [Cast]\n",
      "[X] Searching for input: llama_model/Where_3_output_0\n",
      "[X] llama_model/Cast_7 [Cast] inputs: [llama_model/Where_3_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/Cast_7 for ONNX node: llama_model/Cast_7\n",
      "[X] Registering tensor: llama_model/Cast_7_output_0 for ONNX tensor: llama_model/Cast_7_output_0\n",
      "[X] llama_model/Cast_7 [Cast] outputs: [llama_model/Cast_7_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/Cast_7_output_0\n",
      "[X] Searching for input: llama_model/Expand_output_0\n",
      "[X] llama_model/Add_1 [Add] inputs: [llama_model/Cast_7_output_0 -> (-1, 1, -1, -1)[FLOAT]], [llama_model/Expand_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Add_1 for ONNX node: llama_model/Add_1\n",
      "[X] Registering tensor: llama_model/Add_1_output_0 for ONNX tensor: llama_model/Add_1_output_0\n",
      "[X] llama_model/Add_1 [Add] outputs: [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/embed_tokens/Gather_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast [Cast] inputs: [llama_model/embed_tokens/Gather_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Cast for ONNX node: llama_model/layers.0/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast [Cast] outputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.0/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/input_layernorm/Constant [Constant] outputs: [llama_model/layers.0/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Pow [Pow] inputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.0/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.0/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Pow for ONNX node: llama_model/layers.0/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Pow [Pow] outputs: [llama_model/layers.0/input_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.0/input_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/ReduceMean for ONNX node: llama_model/layers.0/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.0/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.0/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Add [Add] inputs: [llama_model/layers.0/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.0/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.0/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Add for ONNX node: llama_model/layers.0/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Add [Add] outputs: [llama_model/layers.0/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.0/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Sqrt for ONNX node: llama_model/layers.0/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.0/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.0/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Div [Div] inputs: [llama_model/layers.0/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.0/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.0/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Div for ONNX node: llama_model/layers.0/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Div [Div] outputs: [llama_model/layers.0/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul [Mul] inputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.0/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Mul for ONNX node: llama_model/layers.0/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul [Mul] outputs: [llama_model/layers.0/input_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.0/input_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Cast_1 for ONNX node: llama_model/layers.0/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.0/input_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.0.input_layernorm.weight -> (16)[FLOAT]], [llama_model/layers.0/input_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Mul_1 for ONNX node: llama_model/layers.0/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape [Shape] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape for ONNX node: llama_model/layers.0/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape [Shape] outputs: [llama_model/layers.0/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant [Constant] outputs: [llama_model/layers.0/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather [Gather] inputs: [llama_model/layers.0/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.0/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather for ONNX node: llama_model/layers.0/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather [Gather] outputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_1 for ONNX node: llama_model/layers.0/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.0/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_1 for ONNX node: llama_model/layers.0/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_907\n",
      "[X] llama_model/layers.0/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_907 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_907 for ONNX node: onnx::MatMul_907\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_908\n",
      "[X] llama_model/layers.0/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_908 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_908 for ONNX node: onnx::MatMul_908\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_909\n",
      "[X] llama_model/layers.0/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_909 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_909 for ONNX node: onnx::MatMul_909\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: Constant_147 [Constant]\n",
      "[X] Constant_147 [Constant] inputs: \n",
      "[X] Constant_147 [Constant] outputs: [onnx::Unsqueeze_192 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_192\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_192 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze for ONNX node: llama_model/layers.0/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_149 [Constant]\n",
      "[X] Constant_149 [Constant] inputs: \n",
      "[X] Constant_149 [Constant] outputs: [onnx::Unsqueeze_194 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_194\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_194 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat for ONNX node: llama_model/layers.0/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat [Concat] outputs: [llama_model/layers.0/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_154 [Constant]\n",
      "[X] Constant_154 [Constant] inputs: \n",
      "[X] Constant_154 [Constant] outputs: [onnx::Unsqueeze_201 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_201\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_201 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_156 [Constant]\n",
      "[X] Constant_156 [Constant] inputs: \n",
      "[X] Constant_156 [Constant] outputs: [onnx::Unsqueeze_203 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_203\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_203 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_1 for ONNX node: llama_model/layers.0/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_161 [Constant]\n",
      "[X] Constant_161 [Constant] inputs: \n",
      "[X] Constant_161 [Constant] outputs: [onnx::Unsqueeze_210 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_210\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_210 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_163 [Constant]\n",
      "[X] Constant_163 [Constant] inputs: \n",
      "[X] Constant_163 [Constant] outputs: [onnx::Unsqueeze_212 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_212\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_212 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_2 for ONNX node: llama_model/layers.0/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape [Reshape] inputs: [llama_model/layers.0/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.0/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape for ONNX node: llama_model/layers.0/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose [Transpose] inputs: [llama_model/layers.0/self_attn/Reshape_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose for ONNX node: llama_model/layers.0/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.0/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.0/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape_1 for ONNX node: llama_model/layers.0/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_1_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.0/self_attn/Reshape_1_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_1 for ONNX node: llama_model/layers.0/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.0/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.0/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape_2 for ONNX node: llama_model/layers.0/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_2_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.0/self_attn/Reshape_2_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_2 for ONNX node: llama_model/layers.0/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_2 for ONNX node: llama_model/layers.0/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.0/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_2 for ONNX node: llama_model/layers.0/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_177 [Constant]\n",
      "[X] Constant_177 [Constant] inputs: \n",
      "[X] Constant_177 [Constant] outputs: [onnx::Slice_228 -> (1, 1, 128, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_228\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_228 -> (1, 1, 128, 4)[FLOAT]], [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: onnx::Slice_228 for ONNX node: onnx::Slice_228\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: Constant_185 [Constant]\n",
      "[X] Constant_185 [Constant] inputs: \n",
      "[X] Constant_185 [Constant] outputs: [onnx::Slice_238 -> (1, 1, 128, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_238\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_238 -> (1, 1, 128, 4)[FLOAT]], [llama_model/layers.0/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: onnx::Slice_238 for ONNX node: onnx::Slice_238\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] Searching for input: llama_model/Constant_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_3 [Gather] inputs: [llama_model/Reshape_output_0 -> (1, -1)[INT32]], [llama_model/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_output_0 for ONNX node: llama_model/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_3 for ONNX node: llama_model/layers.0/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_3_output_0 -> (-1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_3_output_0 -> (-1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_4 for ONNX node: llama_model/layers.0/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_3_output_0 -> (-1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_5 for ONNX node: llama_model/layers.0/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_5_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul [Mul] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul for ONNX node: llama_model/layers.0/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul [Mul] outputs: [llama_model/layers.0/self_attn/Mul_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_3 for ONNX node: llama_model/layers.0/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.0/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_6 for ONNX node: llama_model/layers.0/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_10_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div [Div] inputs: [llama_model/layers.0/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_10_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Div for ONNX node: llama_model/layers.0/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Div_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div [Div] outputs: [llama_model/layers.0/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Div_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast [Cast] inputs: [llama_model/layers.0/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast for ONNX node: llama_model/layers.0/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast [Cast] outputs: [llama_model/layers.0/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.0/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_1 for ONNX node: llama_model/layers.0/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice for ONNX node: llama_model/layers.0/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice [Slice] outputs: [llama_model/layers.0/self_attn/Slice_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.0/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice_1 for ONNX node: llama_model/layers.0/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.0/self_attn/Slice_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg [Neg] inputs: [llama_model/layers.0/self_attn/Slice_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Neg for ONNX node: llama_model/layers.0/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg [Neg] outputs: [llama_model/layers.0/self_attn/Neg_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.0/self_attn/Neg_output_0 -> (-1, 4, -1, 2)[FLOAT]], [llama_model/layers.0/self_attn/Slice_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_3 for ONNX node: llama_model/layers.0/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.0/self_attn/Concat_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_5_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul_1 for ONNX node: llama_model/layers.0/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.0/self_attn/Mul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add [Add] inputs: [llama_model/layers.0/self_attn/Mul_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Mul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Add for ONNX node: llama_model/layers.0/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Add_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add [Add] outputs: [llama_model/layers.0/self_attn/Add_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul_2 for ONNX node: llama_model/layers.0/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.0/self_attn/Mul_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_4 for ONNX node: llama_model/layers.0/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_19_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_7 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_19_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_7 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.0/self_attn/Constant_19_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_19_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_7 for ONNX node: llama_model/layers.0/self_attn/Gather_7\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_7_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_7 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_7_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_20_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_7_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_1 [Div] inputs: [llama_model/layers.0/self_attn/Gather_7_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_20_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Div_1 for ONNX node: llama_model/layers.0/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_1 [Div] outputs: [llama_model/layers.0/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.0/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_2 for ONNX node: llama_model/layers.0/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.0/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_3 for ONNX node: llama_model/layers.0/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_21_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Constant_21_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice_2 for ONNX node: llama_model/layers.0/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.0/self_attn/Slice_2_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.0/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_27_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_28_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_27_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice_3 for ONNX node: llama_model/layers.0/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.0/self_attn/Slice_3_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.0/self_attn/Slice_3_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Neg_1 for ONNX node: llama_model/layers.0/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.0/self_attn/Neg_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.0/self_attn/Neg_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], [llama_model/layers.0/self_attn/Slice_2_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_4 for ONNX node: llama_model/layers.0/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_4_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.0/self_attn/Concat_4_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_5_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul_3 for ONNX node: llama_model/layers.0/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.0/self_attn/Mul_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_1 [Add] inputs: [llama_model/layers.0/self_attn/Mul_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Mul_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Add_1 for ONNX node: llama_model/layers.0/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_1 [Add] outputs: [llama_model/layers.0/self_attn/Add_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.0/self_attn/Add_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_3 for ONNX node: llama_model/layers.0/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_3_output_0 -> (-1, 4, 4, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul [MatMul] inputs: [llama_model/layers.0/self_attn/Add_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Transpose_3_output_0 -> (-1, 4, 4, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/MatMul for ONNX node: llama_model/layers.0/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/MatMul_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_29_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_29_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_2 [Div] inputs: [llama_model/layers.0/self_attn/MatMul_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/layers.0/self_attn/Constant_29_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_29_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_29_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Div_2 for ONNX node: llama_model/layers.0/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_2 [Div] outputs: [llama_model/layers.0/self_attn/Div_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_2 [Add] inputs: [llama_model/layers.0/self_attn/Div_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Add_2 for ONNX node: llama_model/layers.0/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_2 [Add] outputs: [llama_model/layers.0/self_attn/Add_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Softmax [Softmax] inputs: [llama_model/layers.0/self_attn/Add_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Softmax for ONNX node: llama_model/layers.0/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.0/self_attn/Softmax [Softmax] outputs: [llama_model/layers.0/self_attn/Softmax_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.0/self_attn/Softmax_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_4 for ONNX node: llama_model/layers.0/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_4_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.0/self_attn/Cast_4_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_5 for ONNX node: llama_model/layers.0/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_5_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.0/self_attn/Cast_5_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/layers.0/self_attn/Transpose_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/MatMul_1 for ONNX node: llama_model/layers.0/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.0/self_attn/MatMul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.0/self_attn/MatMul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_4 for ONNX node: llama_model/layers.0/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_4_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: Constant_254 [Constant]\n",
      "[X] Constant_254 [Constant] inputs: \n",
      "[X] Constant_254 [Constant] outputs: [onnx::Unsqueeze_318 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_318\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_318 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_256 [Constant]\n",
      "[X] Constant_256 [Constant] inputs: \n",
      "[X] Constant_256 [Constant] outputs: [onnx::Unsqueeze_320 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_320\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_320 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_30_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_30_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_5 for ONNX node: llama_model/layers.0/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.0/self_attn/Transpose_4_output_0 -> (-1, -1, 4, 4)[FLOAT]], [llama_model/layers.0/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape_3 for ONNX node: llama_model/layers.0/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_3_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_929\n",
      "[X] llama_model/layers.0/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.0/self_attn/Reshape_3_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_929 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_929 for ONNX node: onnx::MatMul_929\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/Add [Add] inputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.0/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/Add for ONNX node: llama_model/layers.0/Add\n",
      "[X] Registering tensor: llama_model/layers.0/Add_output_0 for ONNX tensor: llama_model/layers.0/Add_output_0\n",
      "[X] llama_model/layers.0/Add [Add] outputs: [llama_model/layers.0/Add_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/Add_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.0/Add_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Cast for ONNX node: llama_model/layers.0/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.0/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.0/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Pow for ONNX node: llama_model/layers.0/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.0/post_attention_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.0/post_attention_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.0/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.0/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Add for ONNX node: llama_model/layers.0/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.0/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.0/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.0/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.0/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.0/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Div for ONNX node: llama_model/layers.0/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.0/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Mul for ONNX node: llama_model/layers.0/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.0/post_attention_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.0/post_attention_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.0/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.0/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.0.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.0.post_attention_layernorm.weight -> (16)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.0/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_930\n",
      "[X] llama_model/layers.0/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_930 -> (16, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_930 for ONNX node: onnx::MatMul_930\n",
      "[X] Registering layer: llama_model/layers.0/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.0/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.0/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.0/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.0/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.0/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.0/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.0/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.0/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/mlp/act_fn/Mul for ONNX node: llama_model/layers.0/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.0/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.0/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_931\n",
      "[X] llama_model/layers.0/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_931 -> (16, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_931 for ONNX node: onnx::MatMul_931\n",
      "[X] Registering layer: llama_model/layers.0/mlp/up_proj/MatMul for ONNX node: llama_model/layers.0/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.0/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/Mul [Mul] inputs: [llama_model/layers.0/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.0/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/mlp/Mul for ONNX node: llama_model/layers.0/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.0/mlp/Mul_output_0\n",
      "[X] llama_model/layers.0/mlp/Mul [Mul] outputs: [llama_model/layers.0/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_932\n",
      "[X] llama_model/layers.0/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.0/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_932 -> (128, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_932 for ONNX node: onnx::MatMul_932\n",
      "[X] Registering layer: llama_model/layers.0/mlp/down_proj/MatMul for ONNX node: llama_model/layers.0/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.0/mlp/down_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/Add_1 [Add] inputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.0/mlp/down_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/Add_1 for ONNX node: llama_model/layers.0/Add_1\n",
      "[X] Registering tensor: llama_model/layers.0/Add_1_output_0 for ONNX tensor: llama_model/layers.0/Add_1_output_0\n",
      "[X] llama_model/layers.0/Add_1 [Add] outputs: [llama_model/layers.0/Add_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/Add_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast [Cast] inputs: [llama_model/layers.0/Add_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Cast for ONNX node: llama_model/layers.1/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast [Cast] outputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.1/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/input_layernorm/Constant [Constant] outputs: [llama_model/layers.1/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Pow [Pow] inputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.1/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.1/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Pow for ONNX node: llama_model/layers.1/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Pow [Pow] outputs: [llama_model/layers.1/input_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.1/input_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/ReduceMean for ONNX node: llama_model/layers.1/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.1/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.1/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Add [Add] inputs: [llama_model/layers.1/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.1/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.1/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Add for ONNX node: llama_model/layers.1/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Add [Add] outputs: [llama_model/layers.1/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.1/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Sqrt for ONNX node: llama_model/layers.1/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.1/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.1/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Div [Div] inputs: [llama_model/layers.1/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.1/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.1/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Div for ONNX node: llama_model/layers.1/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Div [Div] outputs: [llama_model/layers.1/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul [Mul] inputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.1/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Mul for ONNX node: llama_model/layers.1/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul [Mul] outputs: [llama_model/layers.1/input_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.1/input_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Cast_1 for ONNX node: llama_model/layers.1/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.1/input_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.1.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.1.input_layernorm.weight -> (16)[FLOAT]], [llama_model/layers.1/input_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Mul_1 for ONNX node: llama_model/layers.1/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape [Shape] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape for ONNX node: llama_model/layers.1/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape [Shape] outputs: [llama_model/layers.1/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant [Constant] outputs: [llama_model/layers.1/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather [Gather] inputs: [llama_model/layers.1/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.1/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather for ONNX node: llama_model/layers.1/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather [Gather] outputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_1 for ONNX node: llama_model/layers.1/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.1/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_1 for ONNX node: llama_model/layers.1/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_933\n",
      "[X] llama_model/layers.1/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_933 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_933 for ONNX node: onnx::MatMul_933\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_934\n",
      "[X] llama_model/layers.1/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_934 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_934 for ONNX node: onnx::MatMul_934\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_935\n",
      "[X] llama_model/layers.1/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_935 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_935 for ONNX node: onnx::MatMul_935\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: Constant_303 [Constant]\n",
      "[X] Constant_303 [Constant] inputs: \n",
      "[X] Constant_303 [Constant] outputs: [onnx::Unsqueeze_375 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_375\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_375 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze for ONNX node: llama_model/layers.1/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_305 [Constant]\n",
      "[X] Constant_305 [Constant] inputs: \n",
      "[X] Constant_305 [Constant] outputs: [onnx::Unsqueeze_377 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_377\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_377 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat for ONNX node: llama_model/layers.1/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat [Concat] outputs: [llama_model/layers.1/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_310 [Constant]\n",
      "[X] Constant_310 [Constant] inputs: \n",
      "[X] Constant_310 [Constant] outputs: [onnx::Unsqueeze_384 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_384\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_384 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_312 [Constant]\n",
      "[X] Constant_312 [Constant] inputs: \n",
      "[X] Constant_312 [Constant] outputs: [onnx::Unsqueeze_386 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_386\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_386 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_1 for ONNX node: llama_model/layers.1/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_317 [Constant]\n",
      "[X] Constant_317 [Constant] inputs: \n",
      "[X] Constant_317 [Constant] outputs: [onnx::Unsqueeze_393 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_393\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_393 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_319 [Constant]\n",
      "[X] Constant_319 [Constant] inputs: \n",
      "[X] Constant_319 [Constant] outputs: [onnx::Unsqueeze_395 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_395\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_395 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_2 for ONNX node: llama_model/layers.1/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape [Reshape] inputs: [llama_model/layers.1/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.1/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape for ONNX node: llama_model/layers.1/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose [Transpose] inputs: [llama_model/layers.1/self_attn/Reshape_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose for ONNX node: llama_model/layers.1/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.1/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.1/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape_1 for ONNX node: llama_model/layers.1/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_1_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.1/self_attn/Reshape_1_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_1 for ONNX node: llama_model/layers.1/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.1/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.1/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape_2 for ONNX node: llama_model/layers.1/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_2_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.1/self_attn/Reshape_2_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_2 for ONNX node: llama_model/layers.1/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_2 for ONNX node: llama_model/layers.1/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.1/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_2 for ONNX node: llama_model/layers.1/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_228\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_228 -> (1, 1, 128, 4)[FLOAT]], [llama_model/layers.1/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_238\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_238 -> (1, 1, 128, 4)[FLOAT]], [llama_model/layers.1/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_3_output_0 -> (-1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_3 for ONNX node: llama_model/layers.1/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_3_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_3_output_0 -> (-1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_4 for ONNX node: llama_model/layers.1/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul [Mul] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Gather_3_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul for ONNX node: llama_model/layers.1/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul [Mul] outputs: [llama_model/layers.1/self_attn/Mul_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_3 for ONNX node: llama_model/layers.1/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.1/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_5 for ONNX node: llama_model/layers.1/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_10_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div [Div] inputs: [llama_model/layers.1/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_10_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Div for ONNX node: llama_model/layers.1/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Div_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div [Div] outputs: [llama_model/layers.1/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Div_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast [Cast] inputs: [llama_model/layers.1/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast for ONNX node: llama_model/layers.1/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast [Cast] outputs: [llama_model/layers.1/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.1/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_1 for ONNX node: llama_model/layers.1/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice for ONNX node: llama_model/layers.1/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice [Slice] outputs: [llama_model/layers.1/self_attn/Slice_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.1/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice_1 for ONNX node: llama_model/layers.1/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.1/self_attn/Slice_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg [Neg] inputs: [llama_model/layers.1/self_attn/Slice_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Neg for ONNX node: llama_model/layers.1/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg [Neg] outputs: [llama_model/layers.1/self_attn/Neg_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.1/self_attn/Neg_output_0 -> (-1, 4, -1, 2)[FLOAT]], [llama_model/layers.1/self_attn/Slice_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_3 for ONNX node: llama_model/layers.1/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.1/self_attn/Concat_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul_1 for ONNX node: llama_model/layers.1/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.1/self_attn/Mul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add [Add] inputs: [llama_model/layers.1/self_attn/Mul_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Mul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Add for ONNX node: llama_model/layers.1/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Add_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add [Add] outputs: [llama_model/layers.1/self_attn/Add_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Gather_3_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul_2 for ONNX node: llama_model/layers.1/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.1/self_attn/Mul_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_4 for ONNX node: llama_model/layers.1/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_19_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_19_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.1/self_attn/Constant_19_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_19_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_6 for ONNX node: llama_model/layers.1/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_20_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_1 [Div] inputs: [llama_model/layers.1/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_20_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Div_1 for ONNX node: llama_model/layers.1/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_1 [Div] outputs: [llama_model/layers.1/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.1/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_2 for ONNX node: llama_model/layers.1/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.1/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_3 for ONNX node: llama_model/layers.1/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_21_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Constant_21_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice_2 for ONNX node: llama_model/layers.1/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.1/self_attn/Slice_2_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.1/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_27_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_28_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_27_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice_3 for ONNX node: llama_model/layers.1/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.1/self_attn/Slice_3_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.1/self_attn/Slice_3_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Neg_1 for ONNX node: llama_model/layers.1/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.1/self_attn/Neg_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.1/self_attn/Neg_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], [llama_model/layers.1/self_attn/Slice_2_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_4 for ONNX node: llama_model/layers.1/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_4_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.1/self_attn/Concat_4_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul_3 for ONNX node: llama_model/layers.1/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.1/self_attn/Mul_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_1 [Add] inputs: [llama_model/layers.1/self_attn/Mul_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Mul_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Add_1 for ONNX node: llama_model/layers.1/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_1 [Add] outputs: [llama_model/layers.1/self_attn/Add_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.1/self_attn/Add_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_3 for ONNX node: llama_model/layers.1/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_3_output_0 -> (-1, 4, 4, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul [MatMul] inputs: [llama_model/layers.1/self_attn/Add_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Transpose_3_output_0 -> (-1, 4, 4, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/MatMul for ONNX node: llama_model/layers.1/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/MatMul_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_29_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_29_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_2 [Div] inputs: [llama_model/layers.1/self_attn/MatMul_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/layers.1/self_attn/Constant_29_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_29_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_29_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Div_2 for ONNX node: llama_model/layers.1/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_2 [Div] outputs: [llama_model/layers.1/self_attn/Div_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_2 [Add] inputs: [llama_model/layers.1/self_attn/Div_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Add_2 for ONNX node: llama_model/layers.1/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_2 [Add] outputs: [llama_model/layers.1/self_attn/Add_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Softmax [Softmax] inputs: [llama_model/layers.1/self_attn/Add_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Softmax for ONNX node: llama_model/layers.1/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.1/self_attn/Softmax [Softmax] outputs: [llama_model/layers.1/self_attn/Softmax_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.1/self_attn/Softmax_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_4 for ONNX node: llama_model/layers.1/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_4_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.1/self_attn/Cast_4_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_5 for ONNX node: llama_model/layers.1/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_5_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.1/self_attn/Cast_5_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/layers.1/self_attn/Transpose_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/MatMul_1 for ONNX node: llama_model/layers.1/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.1/self_attn/MatMul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.1/self_attn/MatMul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_4 for ONNX node: llama_model/layers.1/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_4_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: Constant_407 [Constant]\n",
      "[X] Constant_407 [Constant] inputs: \n",
      "[X] Constant_407 [Constant] outputs: [onnx::Unsqueeze_497 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_497\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_497 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_409 [Constant]\n",
      "[X] Constant_409 [Constant] inputs: \n",
      "[X] Constant_409 [Constant] outputs: [onnx::Unsqueeze_499 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_499\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_499 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_30_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_30_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_5 for ONNX node: llama_model/layers.1/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.1/self_attn/Transpose_4_output_0 -> (-1, -1, 4, 4)[FLOAT]], [llama_model/layers.1/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape_3 for ONNX node: llama_model/layers.1/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_3_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_955\n",
      "[X] llama_model/layers.1/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.1/self_attn/Reshape_3_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_955 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_955 for ONNX node: onnx::MatMul_955\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/Add [Add] inputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.1/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/Add for ONNX node: llama_model/layers.1/Add\n",
      "[X] Registering tensor: llama_model/layers.1/Add_output_0 for ONNX tensor: llama_model/layers.1/Add_output_0\n",
      "[X] llama_model/layers.1/Add [Add] outputs: [llama_model/layers.1/Add_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/Add_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.1/Add_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Cast for ONNX node: llama_model/layers.1/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.1/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.1/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Pow for ONNX node: llama_model/layers.1/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.1/post_attention_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.1/post_attention_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.1/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.1/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Add for ONNX node: llama_model/layers.1/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.1/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.1/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.1/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.1/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.1/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Div for ONNX node: llama_model/layers.1/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.1/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Mul for ONNX node: llama_model/layers.1/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.1/post_attention_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.1/post_attention_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.1/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.1/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.1.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.1.post_attention_layernorm.weight -> (16)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.1/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_956\n",
      "[X] llama_model/layers.1/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_956 -> (16, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_956 for ONNX node: onnx::MatMul_956\n",
      "[X] Registering layer: llama_model/layers.1/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.1/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.1/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.1/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.1/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.1/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.1/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.1/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.1/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/mlp/act_fn/Mul for ONNX node: llama_model/layers.1/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.1/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.1/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_957\n",
      "[X] llama_model/layers.1/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_957 -> (16, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_957 for ONNX node: onnx::MatMul_957\n",
      "[X] Registering layer: llama_model/layers.1/mlp/up_proj/MatMul for ONNX node: llama_model/layers.1/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.1/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/Mul [Mul] inputs: [llama_model/layers.1/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.1/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/mlp/Mul for ONNX node: llama_model/layers.1/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.1/mlp/Mul_output_0\n",
      "[X] llama_model/layers.1/mlp/Mul [Mul] outputs: [llama_model/layers.1/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_958\n",
      "[X] llama_model/layers.1/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.1/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_958 -> (128, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_958 for ONNX node: onnx::MatMul_958\n",
      "[X] Registering layer: llama_model/layers.1/mlp/down_proj/MatMul for ONNX node: llama_model/layers.1/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.1/mlp/down_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/Add_1 [Add] inputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.1/mlp/down_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/Add_1 for ONNX node: llama_model/layers.1/Add_1\n",
      "[X] Registering tensor: llama_model/layers.1/Add_1_output_0 for ONNX tensor: llama_model/layers.1/Add_1_output_0\n",
      "[X] llama_model/layers.1/Add_1 [Add] outputs: [llama_model/layers.1/Add_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/Add_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast [Cast] inputs: [llama_model/layers.1/Add_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Cast for ONNX node: llama_model/layers.2/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast [Cast] outputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.2/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/input_layernorm/Constant [Constant] outputs: [llama_model/layers.2/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Pow [Pow] inputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.2/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.2/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Pow for ONNX node: llama_model/layers.2/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Pow [Pow] outputs: [llama_model/layers.2/input_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.2/input_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/ReduceMean for ONNX node: llama_model/layers.2/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.2/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.2/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Add [Add] inputs: [llama_model/layers.2/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.2/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.2/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Add for ONNX node: llama_model/layers.2/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Add [Add] outputs: [llama_model/layers.2/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.2/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Sqrt for ONNX node: llama_model/layers.2/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.2/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.2/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Div [Div] inputs: [llama_model/layers.2/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.2/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.2/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Div for ONNX node: llama_model/layers.2/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Div [Div] outputs: [llama_model/layers.2/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul [Mul] inputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.2/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Mul for ONNX node: llama_model/layers.2/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul [Mul] outputs: [llama_model/layers.2/input_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.2/input_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Cast_1 for ONNX node: llama_model/layers.2/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.2/input_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.2.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.2.input_layernorm.weight -> (16)[FLOAT]], [llama_model/layers.2/input_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Mul_1 for ONNX node: llama_model/layers.2/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape [Shape] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape for ONNX node: llama_model/layers.2/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape [Shape] outputs: [llama_model/layers.2/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant [Constant] outputs: [llama_model/layers.2/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather [Gather] inputs: [llama_model/layers.2/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.2/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather for ONNX node: llama_model/layers.2/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather [Gather] outputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_1 for ONNX node: llama_model/layers.2/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.2/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_1 for ONNX node: llama_model/layers.2/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_959\n",
      "[X] llama_model/layers.2/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_959 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_959 for ONNX node: onnx::MatMul_959\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_960\n",
      "[X] llama_model/layers.2/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_960 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_960 for ONNX node: onnx::MatMul_960\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_961\n",
      "[X] llama_model/layers.2/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_961 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_961 for ONNX node: onnx::MatMul_961\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: Constant_456 [Constant]\n",
      "[X] Constant_456 [Constant] inputs: \n",
      "[X] Constant_456 [Constant] outputs: [onnx::Unsqueeze_554 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_554\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_554 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze for ONNX node: llama_model/layers.2/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_458 [Constant]\n",
      "[X] Constant_458 [Constant] inputs: \n",
      "[X] Constant_458 [Constant] outputs: [onnx::Unsqueeze_556 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_556\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_556 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat for ONNX node: llama_model/layers.2/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat [Concat] outputs: [llama_model/layers.2/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_463 [Constant]\n",
      "[X] Constant_463 [Constant] inputs: \n",
      "[X] Constant_463 [Constant] outputs: [onnx::Unsqueeze_563 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_563\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_563 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_465 [Constant]\n",
      "[X] Constant_465 [Constant] inputs: \n",
      "[X] Constant_465 [Constant] outputs: [onnx::Unsqueeze_565 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_565\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_565 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_1 for ONNX node: llama_model/layers.2/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_470 [Constant]\n",
      "[X] Constant_470 [Constant] inputs: \n",
      "[X] Constant_470 [Constant] outputs: [onnx::Unsqueeze_572 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_572\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_572 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_472 [Constant]\n",
      "[X] Constant_472 [Constant] inputs: \n",
      "[X] Constant_472 [Constant] outputs: [onnx::Unsqueeze_574 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_574\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_574 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_2 for ONNX node: llama_model/layers.2/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape [Reshape] inputs: [llama_model/layers.2/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.2/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape for ONNX node: llama_model/layers.2/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose [Transpose] inputs: [llama_model/layers.2/self_attn/Reshape_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose for ONNX node: llama_model/layers.2/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.2/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.2/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape_1 for ONNX node: llama_model/layers.2/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_1_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.2/self_attn/Reshape_1_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_1 for ONNX node: llama_model/layers.2/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.2/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.2/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape_2 for ONNX node: llama_model/layers.2/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_2_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.2/self_attn/Reshape_2_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_2 for ONNX node: llama_model/layers.2/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_2 for ONNX node: llama_model/layers.2/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.2/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_2 for ONNX node: llama_model/layers.2/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_228\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_228 -> (1, 1, 128, 4)[FLOAT]], [llama_model/layers.2/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_238\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_238 -> (1, 1, 128, 4)[FLOAT]], [llama_model/layers.2/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_3_output_0 -> (-1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_3 for ONNX node: llama_model/layers.2/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_3_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_3_output_0 -> (-1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_4 for ONNX node: llama_model/layers.2/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul [Mul] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Gather_3_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul for ONNX node: llama_model/layers.2/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul [Mul] outputs: [llama_model/layers.2/self_attn/Mul_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_3 for ONNX node: llama_model/layers.2/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.2/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_5 for ONNX node: llama_model/layers.2/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_10_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div [Div] inputs: [llama_model/layers.2/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_10_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Div for ONNX node: llama_model/layers.2/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Div_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div [Div] outputs: [llama_model/layers.2/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Div_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast [Cast] inputs: [llama_model/layers.2/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast for ONNX node: llama_model/layers.2/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast [Cast] outputs: [llama_model/layers.2/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.2/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_1 for ONNX node: llama_model/layers.2/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice for ONNX node: llama_model/layers.2/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice [Slice] outputs: [llama_model/layers.2/self_attn/Slice_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.2/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice_1 for ONNX node: llama_model/layers.2/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.2/self_attn/Slice_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg [Neg] inputs: [llama_model/layers.2/self_attn/Slice_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Neg for ONNX node: llama_model/layers.2/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg [Neg] outputs: [llama_model/layers.2/self_attn/Neg_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.2/self_attn/Neg_output_0 -> (-1, 4, -1, 2)[FLOAT]], [llama_model/layers.2/self_attn/Slice_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_3 for ONNX node: llama_model/layers.2/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.2/self_attn/Concat_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul_1 for ONNX node: llama_model/layers.2/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.2/self_attn/Mul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add [Add] inputs: [llama_model/layers.2/self_attn/Mul_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Mul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Add for ONNX node: llama_model/layers.2/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Add_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add [Add] outputs: [llama_model/layers.2/self_attn/Add_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Gather_3_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul_2 for ONNX node: llama_model/layers.2/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.2/self_attn/Mul_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_4 for ONNX node: llama_model/layers.2/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_19_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_19_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.2/self_attn/Constant_19_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_19_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_6 for ONNX node: llama_model/layers.2/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_20_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_1 [Div] inputs: [llama_model/layers.2/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_20_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Div_1 for ONNX node: llama_model/layers.2/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_1 [Div] outputs: [llama_model/layers.2/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.2/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_2 for ONNX node: llama_model/layers.2/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.2/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_3 for ONNX node: llama_model/layers.2/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_21_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Constant_21_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice_2 for ONNX node: llama_model/layers.2/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.2/self_attn/Slice_2_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.2/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_27_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_28_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_27_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice_3 for ONNX node: llama_model/layers.2/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.2/self_attn/Slice_3_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.2/self_attn/Slice_3_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Neg_1 for ONNX node: llama_model/layers.2/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.2/self_attn/Neg_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.2/self_attn/Neg_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], [llama_model/layers.2/self_attn/Slice_2_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_4 for ONNX node: llama_model/layers.2/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_4_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.2/self_attn/Concat_4_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul_3 for ONNX node: llama_model/layers.2/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.2/self_attn/Mul_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_1 [Add] inputs: [llama_model/layers.2/self_attn/Mul_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Mul_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Add_1 for ONNX node: llama_model/layers.2/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_1 [Add] outputs: [llama_model/layers.2/self_attn/Add_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.2/self_attn/Add_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_3 for ONNX node: llama_model/layers.2/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_3_output_0 -> (-1, 4, 4, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul [MatMul] inputs: [llama_model/layers.2/self_attn/Add_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Transpose_3_output_0 -> (-1, 4, 4, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/MatMul for ONNX node: llama_model/layers.2/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/MatMul_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_29_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_29_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_2 [Div] inputs: [llama_model/layers.2/self_attn/MatMul_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/layers.2/self_attn/Constant_29_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_29_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_29_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Div_2 for ONNX node: llama_model/layers.2/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_2 [Div] outputs: [llama_model/layers.2/self_attn/Div_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_2 [Add] inputs: [llama_model/layers.2/self_attn/Div_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Add_2 for ONNX node: llama_model/layers.2/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_2 [Add] outputs: [llama_model/layers.2/self_attn/Add_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Softmax [Softmax] inputs: [llama_model/layers.2/self_attn/Add_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Softmax for ONNX node: llama_model/layers.2/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.2/self_attn/Softmax [Softmax] outputs: [llama_model/layers.2/self_attn/Softmax_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.2/self_attn/Softmax_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_4 for ONNX node: llama_model/layers.2/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_4_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.2/self_attn/Cast_4_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_5 for ONNX node: llama_model/layers.2/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_5_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.2/self_attn/Cast_5_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/layers.2/self_attn/Transpose_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/MatMul_1 for ONNX node: llama_model/layers.2/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.2/self_attn/MatMul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.2/self_attn/MatMul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_4 for ONNX node: llama_model/layers.2/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_4_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: Constant_560 [Constant]\n",
      "[X] Constant_560 [Constant] inputs: \n",
      "[X] Constant_560 [Constant] outputs: [onnx::Unsqueeze_676 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_676\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_676 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_562 [Constant]\n",
      "[X] Constant_562 [Constant] inputs: \n",
      "[X] Constant_562 [Constant] outputs: [onnx::Unsqueeze_678 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_678\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_678 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_30_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_30_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_5 for ONNX node: llama_model/layers.2/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.2/self_attn/Transpose_4_output_0 -> (-1, -1, 4, 4)[FLOAT]], [llama_model/layers.2/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape_3 for ONNX node: llama_model/layers.2/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_3_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_981\n",
      "[X] llama_model/layers.2/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.2/self_attn/Reshape_3_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_981 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_981 for ONNX node: onnx::MatMul_981\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/Add [Add] inputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.2/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/Add for ONNX node: llama_model/layers.2/Add\n",
      "[X] Registering tensor: llama_model/layers.2/Add_output_0 for ONNX tensor: llama_model/layers.2/Add_output_0\n",
      "[X] llama_model/layers.2/Add [Add] outputs: [llama_model/layers.2/Add_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/Add_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.2/Add_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Cast for ONNX node: llama_model/layers.2/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.2/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.2/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Pow for ONNX node: llama_model/layers.2/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.2/post_attention_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.2/post_attention_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.2/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.2/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Add for ONNX node: llama_model/layers.2/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.2/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.2/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.2/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.2/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.2/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Div for ONNX node: llama_model/layers.2/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.2/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Mul for ONNX node: llama_model/layers.2/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.2/post_attention_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.2/post_attention_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.2/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.2/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.2.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.2.post_attention_layernorm.weight -> (16)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.2/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_982\n",
      "[X] llama_model/layers.2/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_982 -> (16, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_982 for ONNX node: onnx::MatMul_982\n",
      "[X] Registering layer: llama_model/layers.2/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.2/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.2/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.2/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.2/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.2/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.2/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.2/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.2/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/mlp/act_fn/Mul for ONNX node: llama_model/layers.2/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.2/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.2/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_983\n",
      "[X] llama_model/layers.2/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_983 -> (16, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_983 for ONNX node: onnx::MatMul_983\n",
      "[X] Registering layer: llama_model/layers.2/mlp/up_proj/MatMul for ONNX node: llama_model/layers.2/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.2/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/Mul [Mul] inputs: [llama_model/layers.2/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.2/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/mlp/Mul for ONNX node: llama_model/layers.2/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.2/mlp/Mul_output_0\n",
      "[X] llama_model/layers.2/mlp/Mul [Mul] outputs: [llama_model/layers.2/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_984\n",
      "[X] llama_model/layers.2/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.2/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_984 -> (128, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_984 for ONNX node: onnx::MatMul_984\n",
      "[X] Registering layer: llama_model/layers.2/mlp/down_proj/MatMul for ONNX node: llama_model/layers.2/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.2/mlp/down_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/Add_1 [Add] inputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.2/mlp/down_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/Add_1 for ONNX node: llama_model/layers.2/Add_1\n",
      "[X] Registering tensor: llama_model/layers.2/Add_1_output_0 for ONNX tensor: llama_model/layers.2/Add_1_output_0\n",
      "[X] llama_model/layers.2/Add_1 [Add] outputs: [llama_model/layers.2/Add_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/Add_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast [Cast] inputs: [llama_model/layers.2/Add_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Cast for ONNX node: llama_model/layers.3/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast [Cast] outputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.3/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/input_layernorm/Constant [Constant] outputs: [llama_model/layers.3/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Pow [Pow] inputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.3/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.3/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Pow for ONNX node: llama_model/layers.3/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Pow [Pow] outputs: [llama_model/layers.3/input_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.3/input_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/ReduceMean for ONNX node: llama_model/layers.3/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.3/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.3/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Add [Add] inputs: [llama_model/layers.3/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.3/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.3/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Add for ONNX node: llama_model/layers.3/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Add [Add] outputs: [llama_model/layers.3/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.3/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Sqrt for ONNX node: llama_model/layers.3/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.3/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.3/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Div [Div] inputs: [llama_model/layers.3/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.3/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.3/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Div for ONNX node: llama_model/layers.3/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Div [Div] outputs: [llama_model/layers.3/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul [Mul] inputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.3/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Mul for ONNX node: llama_model/layers.3/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul [Mul] outputs: [llama_model/layers.3/input_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.3/input_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Cast_1 for ONNX node: llama_model/layers.3/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.3/input_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.3.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.3.input_layernorm.weight -> (16)[FLOAT]], [llama_model/layers.3/input_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Mul_1 for ONNX node: llama_model/layers.3/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape [Shape] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape for ONNX node: llama_model/layers.3/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape [Shape] outputs: [llama_model/layers.3/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant [Constant] outputs: [llama_model/layers.3/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather [Gather] inputs: [llama_model/layers.3/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.3/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather for ONNX node: llama_model/layers.3/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather [Gather] outputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_1 for ONNX node: llama_model/layers.3/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.3/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_1 for ONNX node: llama_model/layers.3/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_985\n",
      "[X] llama_model/layers.3/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_985 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_985 for ONNX node: onnx::MatMul_985\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_986\n",
      "[X] llama_model/layers.3/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_986 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_986 for ONNX node: onnx::MatMul_986\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_987\n",
      "[X] llama_model/layers.3/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_987 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_987 for ONNX node: onnx::MatMul_987\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: Constant_609 [Constant]\n",
      "[X] Constant_609 [Constant] inputs: \n",
      "[X] Constant_609 [Constant] outputs: [onnx::Unsqueeze_733 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_733\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_733 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze for ONNX node: llama_model/layers.3/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_611 [Constant]\n",
      "[X] Constant_611 [Constant] inputs: \n",
      "[X] Constant_611 [Constant] outputs: [onnx::Unsqueeze_735 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_735\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_735 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat for ONNX node: llama_model/layers.3/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat [Concat] outputs: [llama_model/layers.3/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_616 [Constant]\n",
      "[X] Constant_616 [Constant] inputs: \n",
      "[X] Constant_616 [Constant] outputs: [onnx::Unsqueeze_742 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_742\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_742 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_618 [Constant]\n",
      "[X] Constant_618 [Constant] inputs: \n",
      "[X] Constant_618 [Constant] outputs: [onnx::Unsqueeze_744 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_744\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_744 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_1 for ONNX node: llama_model/layers.3/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_623 [Constant]\n",
      "[X] Constant_623 [Constant] inputs: \n",
      "[X] Constant_623 [Constant] outputs: [onnx::Unsqueeze_751 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_751\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_751 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_625 [Constant]\n",
      "[X] Constant_625 [Constant] inputs: \n",
      "[X] Constant_625 [Constant] outputs: [onnx::Unsqueeze_753 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_753\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_753 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_2 for ONNX node: llama_model/layers.3/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape [Reshape] inputs: [llama_model/layers.3/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.3/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape for ONNX node: llama_model/layers.3/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose [Transpose] inputs: [llama_model/layers.3/self_attn/Reshape_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose for ONNX node: llama_model/layers.3/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.3/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.3/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape_1 for ONNX node: llama_model/layers.3/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_1_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.3/self_attn/Reshape_1_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_1 for ONNX node: llama_model/layers.3/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.3/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.3/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape_2 for ONNX node: llama_model/layers.3/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_2_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.3/self_attn/Reshape_2_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_2 for ONNX node: llama_model/layers.3/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_2 for ONNX node: llama_model/layers.3/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.3/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_2 for ONNX node: llama_model/layers.3/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_228\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_228 -> (1, 1, 128, 4)[FLOAT]], [llama_model/layers.3/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_238\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_238 -> (1, 1, 128, 4)[FLOAT]], [llama_model/layers.3/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_3_output_0 -> (-1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_3 for ONNX node: llama_model/layers.3/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_3_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Gather_3_output_0 -> (-1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_4 for ONNX node: llama_model/layers.3/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul [Mul] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Gather_3_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul for ONNX node: llama_model/layers.3/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul [Mul] outputs: [llama_model/layers.3/self_attn/Mul_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_3 for ONNX node: llama_model/layers.3/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.3/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_5 for ONNX node: llama_model/layers.3/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_10_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div [Div] inputs: [llama_model/layers.3/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_10_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Div for ONNX node: llama_model/layers.3/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Div_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div [Div] outputs: [llama_model/layers.3/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Div_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast [Cast] inputs: [llama_model/layers.3/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast for ONNX node: llama_model/layers.3/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast [Cast] outputs: [llama_model/layers.3/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.3/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_1 for ONNX node: llama_model/layers.3/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice for ONNX node: llama_model/layers.3/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice [Slice] outputs: [llama_model/layers.3/self_attn/Slice_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.3/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice_1 for ONNX node: llama_model/layers.3/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.3/self_attn/Slice_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg [Neg] inputs: [llama_model/layers.3/self_attn/Slice_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Neg for ONNX node: llama_model/layers.3/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg [Neg] outputs: [llama_model/layers.3/self_attn/Neg_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.3/self_attn/Neg_output_0 -> (-1, 4, -1, 2)[FLOAT]], [llama_model/layers.3/self_attn/Slice_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_3 for ONNX node: llama_model/layers.3/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.3/self_attn/Concat_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul_1 for ONNX node: llama_model/layers.3/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.3/self_attn/Mul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add [Add] inputs: [llama_model/layers.3/self_attn/Mul_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Mul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Add for ONNX node: llama_model/layers.3/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Add_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add [Add] outputs: [llama_model/layers.3/self_attn/Add_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Gather_3_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul_2 for ONNX node: llama_model/layers.3/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.3/self_attn/Mul_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_4 for ONNX node: llama_model/layers.3/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_19_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_19_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.3/self_attn/Constant_19_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_19_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_6 for ONNX node: llama_model/layers.3/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_20_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_1 [Div] inputs: [llama_model/layers.3/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_20_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Div_1 for ONNX node: llama_model/layers.3/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_1 [Div] outputs: [llama_model/layers.3/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.3/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_2 for ONNX node: llama_model/layers.3/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.3/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_3 for ONNX node: llama_model/layers.3/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_21_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Constant_21_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice_2 for ONNX node: llama_model/layers.3/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.3/self_attn/Slice_2_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.3/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_27_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_28_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_27_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice_3 for ONNX node: llama_model/layers.3/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.3/self_attn/Slice_3_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.3/self_attn/Slice_3_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Neg_1 for ONNX node: llama_model/layers.3/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.3/self_attn/Neg_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.3/self_attn/Neg_1_output_0 -> (-1, 4, -1, 2)[FLOAT]], [llama_model/layers.3/self_attn/Slice_2_output_0 -> (-1, 4, -1, 2)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_4 for ONNX node: llama_model/layers.3/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_4_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.3/self_attn/Concat_4_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Gather_4_output_0 -> (1, 1, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul_3 for ONNX node: llama_model/layers.3/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.3/self_attn/Mul_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_1 [Add] inputs: [llama_model/layers.3/self_attn/Mul_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Mul_3_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Add_1 for ONNX node: llama_model/layers.3/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_1 [Add] outputs: [llama_model/layers.3/self_attn/Add_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.3/self_attn/Add_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_3 for ONNX node: llama_model/layers.3/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_3_output_0 -> (-1, 4, 4, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul [MatMul] inputs: [llama_model/layers.3/self_attn/Add_output_0 -> (-1, 4, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Transpose_3_output_0 -> (-1, 4, 4, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/MatMul for ONNX node: llama_model/layers.3/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/MatMul_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_29_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_29_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_2 [Div] inputs: [llama_model/layers.3/self_attn/MatMul_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/layers.3/self_attn/Constant_29_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_29_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_29_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Div_2 for ONNX node: llama_model/layers.3/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_2 [Div] outputs: [llama_model/layers.3/self_attn/Div_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_2 [Add] inputs: [llama_model/layers.3/self_attn/Div_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Add_2 for ONNX node: llama_model/layers.3/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_2 [Add] outputs: [llama_model/layers.3/self_attn/Add_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Softmax [Softmax] inputs: [llama_model/layers.3/self_attn/Add_2_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Softmax for ONNX node: llama_model/layers.3/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.3/self_attn/Softmax [Softmax] outputs: [llama_model/layers.3/self_attn/Softmax_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.3/self_attn/Softmax_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_4 for ONNX node: llama_model/layers.3/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_4_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.3/self_attn/Cast_4_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_5 for ONNX node: llama_model/layers.3/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_5_output_0 -> (-1, 4, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.3/self_attn/Cast_5_output_0 -> (-1, 4, -1, -1)[FLOAT]], [llama_model/layers.3/self_attn/Transpose_2_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/MatMul_1 for ONNX node: llama_model/layers.3/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.3/self_attn/MatMul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.3/self_attn/MatMul_1_output_0 -> (-1, 4, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_4 for ONNX node: llama_model/layers.3/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_4_output_0 -> (-1, -1, 4, 4)[FLOAT]], \n",
      "[X] Parsing node: Constant_713 [Constant]\n",
      "[X] Constant_713 [Constant] inputs: \n",
      "[X] Constant_713 [Constant] outputs: [onnx::Unsqueeze_855 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_855\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_855 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_715 [Constant]\n",
      "[X] Constant_715 [Constant] inputs: \n",
      "[X] Constant_715 [Constant] outputs: [onnx::Unsqueeze_857 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_857\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_857 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_30_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_30_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_5 for ONNX node: llama_model/layers.3/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.3/self_attn/Transpose_4_output_0 -> (-1, -1, 4, 4)[FLOAT]], [llama_model/layers.3/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape_3 for ONNX node: llama_model/layers.3/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_3_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_1007\n",
      "[X] llama_model/layers.3/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.3/self_attn/Reshape_3_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_1007 -> (16, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1007 for ONNX node: onnx::MatMul_1007\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/Add [Add] inputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.3/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/Add for ONNX node: llama_model/layers.3/Add\n",
      "[X] Registering tensor: llama_model/layers.3/Add_output_0 for ONNX tensor: llama_model/layers.3/Add_output_0\n",
      "[X] llama_model/layers.3/Add [Add] outputs: [llama_model/layers.3/Add_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/Add_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.3/Add_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Cast for ONNX node: llama_model/layers.3/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.3/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.3/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Pow for ONNX node: llama_model/layers.3/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.3/post_attention_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.3/post_attention_layernorm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.3/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.3/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Add for ONNX node: llama_model/layers.3/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.3/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.3/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.3/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.3/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.3/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Div for ONNX node: llama_model/layers.3/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.3/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Mul for ONNX node: llama_model/layers.3/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.3/post_attention_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.3/post_attention_layernorm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.3/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.3/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.3.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.3.post_attention_layernorm.weight -> (16)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.3/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1008\n",
      "[X] llama_model/layers.3/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_1008 -> (16, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1008 for ONNX node: onnx::MatMul_1008\n",
      "[X] Registering layer: llama_model/layers.3/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.3/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.3/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.3/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.3/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.3/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.3/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.3/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.3/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/mlp/act_fn/Mul for ONNX node: llama_model/layers.3/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.3/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.3/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1009\n",
      "[X] llama_model/layers.3/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_1009 -> (16, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1009 for ONNX node: onnx::MatMul_1009\n",
      "[X] Registering layer: llama_model/layers.3/mlp/up_proj/MatMul for ONNX node: llama_model/layers.3/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.3/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/Mul [Mul] inputs: [llama_model/layers.3/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.3/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/mlp/Mul for ONNX node: llama_model/layers.3/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.3/mlp/Mul_output_0\n",
      "[X] llama_model/layers.3/mlp/Mul [Mul] outputs: [llama_model/layers.3/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_1010\n",
      "[X] llama_model/layers.3/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.3/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_1010 -> (128, 16)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1010 for ONNX node: onnx::MatMul_1010\n",
      "[X] Registering layer: llama_model/layers.3/mlp/down_proj/MatMul for ONNX node: llama_model/layers.3/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.3/mlp/down_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/Add_1 [Add] inputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/layers.3/mlp/down_proj/MatMul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/Add_1 for ONNX node: llama_model/layers.3/Add_1\n",
      "[X] Registering tensor: llama_model/layers.3/Add_1_output_0 for ONNX tensor: llama_model/layers.3/Add_1_output_0\n",
      "[X] llama_model/layers.3/Add_1 [Add] outputs: [llama_model/layers.3/Add_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/Add_1_output_0\n",
      "[X] llama_model/norm/Cast [Cast] inputs: [llama_model/layers.3/Add_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/norm/Cast for ONNX node: llama_model/norm/Cast\n",
      "[X] Registering tensor: llama_model/norm/Cast_output_0 for ONNX tensor: llama_model/norm/Cast_output_0\n",
      "[X] llama_model/norm/Cast [Cast] outputs: [llama_model/norm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Constant [Constant]\n",
      "[X] llama_model/norm/Constant [Constant] inputs: \n",
      "[X] llama_model/norm/Constant [Constant] outputs: [llama_model/norm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/norm/Cast_output_0\n",
      "[X] Searching for input: llama_model/norm/Constant_output_0\n",
      "[X] llama_model/norm/Pow [Pow] inputs: [llama_model/norm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/norm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Constant_output_0 for ONNX node: llama_model/norm/Constant_output_0\n",
      "[X] Registering layer: llama_model/norm/Pow for ONNX node: llama_model/norm/Pow\n",
      "[X] Registering tensor: llama_model/norm/Pow_output_0 for ONNX tensor: llama_model/norm/Pow_output_0\n",
      "[X] llama_model/norm/Pow [Pow] outputs: [llama_model/norm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/norm/Pow_output_0\n",
      "[X] llama_model/norm/ReduceMean [ReduceMean] inputs: [llama_model/norm/Pow_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/ReduceMean for ONNX node: llama_model/norm/ReduceMean\n",
      "[X] Registering tensor: llama_model/norm/ReduceMean_output_0 for ONNX tensor: llama_model/norm/ReduceMean_output_0\n",
      "[X] llama_model/norm/ReduceMean [ReduceMean] outputs: [llama_model/norm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Constant_1 [Constant]\n",
      "[X] llama_model/norm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/norm/Constant_1 [Constant] outputs: [llama_model/norm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Add [Add]\n",
      "[X] Searching for input: llama_model/norm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/norm/Constant_1_output_0\n",
      "[X] llama_model/norm/Add [Add] inputs: [llama_model/norm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/norm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Constant_1_output_0 for ONNX node: llama_model/norm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/norm/Add for ONNX node: llama_model/norm/Add\n",
      "[X] Registering tensor: llama_model/norm/Add_output_0 for ONNX tensor: llama_model/norm/Add_output_0\n",
      "[X] llama_model/norm/Add [Add] outputs: [llama_model/norm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/norm/Add_output_0\n",
      "[X] llama_model/norm/Sqrt [Sqrt] inputs: [llama_model/norm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Sqrt for ONNX node: llama_model/norm/Sqrt\n",
      "[X] Registering tensor: llama_model/norm/Sqrt_output_0 for ONNX tensor: llama_model/norm/Sqrt_output_0\n",
      "[X] llama_model/norm/Sqrt [Sqrt] outputs: [llama_model/norm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Constant_2 [Constant]\n",
      "[X] llama_model/norm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/norm/Constant_2 [Constant] outputs: [llama_model/norm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Div [Div]\n",
      "[X] Searching for input: llama_model/norm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/norm/Sqrt_output_0\n",
      "[X] llama_model/norm/Div [Div] inputs: [llama_model/norm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/norm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Constant_2_output_0 for ONNX node: llama_model/norm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/norm/Div for ONNX node: llama_model/norm/Div\n",
      "[X] Registering tensor: llama_model/norm/Div_output_0 for ONNX tensor: llama_model/norm/Div_output_0\n",
      "[X] llama_model/norm/Div [Div] outputs: [llama_model/norm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/norm/Cast_output_0\n",
      "[X] Searching for input: llama_model/norm/Div_output_0\n",
      "[X] llama_model/norm/Mul [Mul] inputs: [llama_model/norm/Cast_output_0 -> (-1, -1, 16)[FLOAT]], [llama_model/norm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Mul for ONNX node: llama_model/norm/Mul\n",
      "[X] Registering tensor: llama_model/norm/Mul_output_0 for ONNX tensor: llama_model/norm/Mul_output_0\n",
      "[X] llama_model/norm/Mul [Mul] outputs: [llama_model/norm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/norm/Mul_output_0\n",
      "[X] llama_model/norm/Cast_1 [Cast] inputs: [llama_model/norm/Mul_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/norm/Cast_1 for ONNX node: llama_model/norm/Cast_1\n",
      "[X] Registering tensor: llama_model/norm/Cast_1_output_0 for ONNX tensor: llama_model/norm/Cast_1_output_0\n",
      "[X] llama_model/norm/Cast_1 [Cast] outputs: [llama_model/norm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.norm.weight\n",
      "[X] Searching for input: llama_model/norm/Cast_1_output_0\n",
      "[X] llama_model/norm/Mul_1 [Mul] inputs: [llama_model.norm.weight -> (16)[FLOAT]], [llama_model/norm/Cast_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Mul_1 for ONNX node: llama_model/norm/Mul_1\n",
      "[X] Registering tensor: llama_model/norm/Mul_1_output_0 for ONNX tensor: llama_model/norm/Mul_1_output_0\n",
      "[X] llama_model/norm/Mul_1 [Mul] outputs: [llama_model/norm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], \n",
      "[X] Parsing node: lm_head/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/norm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1011\n",
      "[X] lm_head/MatMul [MatMul] inputs: [llama_model/norm/Mul_1_output_0 -> (-1, -1, 16)[FLOAT]], [onnx::MatMul_1011 -> (16, 32000)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1011 for ONNX node: onnx::MatMul_1011\n",
      "[X] Registering layer: lm_head/MatMul for ONNX node: lm_head/MatMul\n",
      "[X] Registering tensor: logits_443 for ONNX tensor: logits\n",
      "[X] lm_head/MatMul [MatMul] outputs: [logits -> (-1, -1, 32000)[FLOAT]], \n",
      "[X] Marking logits_443 as output: logits\n",
      "[V] Builder, Network, and Parser were provided directly instead of via a Callable. This loader will not assume ownership. Please ensure that they are freed.\n",
      "[V]     Setting TensorRT Optimization Profiles\n",
      "[V]     Input tensor: input_ids (dtype=DataType.INT32, shape=(-1, -1)) | Setting input tensor shapes to: (min=(1, 1), opt=(1, 10), max=(1, 20))\n",
      "[I]     Configuring with profiles: [Profile().add('input_ids', min=(1, 1), opt=(1, 10), max=(1, 20))]\n",
      "[I] Building engine with configuration:\n",
      "    Flags                  | [FP16, TF32, OBEY_PRECISION_CONSTRAINTS]\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 3072.00 MiB, TACTIC_DRAM: 15102.06 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
      "[V] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[X] Original: 459 layers\n",
      "[X] After dead-layer removal: 459 layers\n",
      "[X] Graph construction completed in 0.0251152 seconds.\n",
      "[X] Running: IdentityToCastTransform on Identity_0\n",
      "[X] Swap the layer type of Identity_0 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_1\n",
      "[X] Swap the layer type of Identity_1 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_2\n",
      "[X] Swap the layer type of Identity_2 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_3\n",
      "[X] Swap the layer type of Identity_3 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_4\n",
      "[X] Swap the layer type of Identity_4 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_5\n",
      "[X] Swap the layer type of Identity_5 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_6\n",
      "[X] Swap the layer type of Identity_6 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_7\n",
      "[X] Swap the layer type of Identity_7 from IDENTITY to CAST\n",
      "[X] Running: ConstShuffleFusion on (Unnamed Layer* 38) [Constant]\n",
      "[X] ConstShuffleFusion: Fusing (Unnamed Layer* 38) [Constant] with (Unnamed Layer* 39) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on (Unnamed Layer* 44) [Constant]\n",
      "[X] ConstShuffleFusion: Fusing (Unnamed Layer* 44) [Constant] with (Unnamed Layer* 45) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/Constant_19_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/Constant_19_output_0 with (Unnamed Layer* 75) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/Constant_38_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/Constant_38_output_0 with (Unnamed Layer* 158) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/Constant_39_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/Constant_39_output_0 with (Unnamed Layer* 163) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/input_layernorm/Constant_output_0 with (Unnamed Layer* 169) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 173) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 177) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_907\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_907 with (Unnamed Layer* 190) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_908\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_908 with (Unnamed Layer* 193) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_909\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_909 with (Unnamed Layer* 196) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/self_attn/Constant_29_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/self_attn/Constant_29_output_0 with (Unnamed Layer* 403) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_929\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_929 with (Unnamed Layer* 419) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 424) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 428) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 432) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_930\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_930 with (Unnamed Layer* 439) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_931\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_931 with (Unnamed Layer* 444) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_932\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_932 with (Unnamed Layer* 448) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/input_layernorm/Constant_output_0 with (Unnamed Layer* 453) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 457) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 461) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_933\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_933 with (Unnamed Layer* 474) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_934\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_934 with (Unnamed Layer* 477) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_935\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_935 with (Unnamed Layer* 480) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/self_attn/Constant_29_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/self_attn/Constant_29_output_0 with (Unnamed Layer* 683) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_955\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_955 with (Unnamed Layer* 699) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 704) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 708) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 712) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_956\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_956 with (Unnamed Layer* 719) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_957\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_957 with (Unnamed Layer* 724) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_958\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_958 with (Unnamed Layer* 728) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/input_layernorm/Constant_output_0 with (Unnamed Layer* 733) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 737) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 741) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_959\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_959 with (Unnamed Layer* 754) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_960\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_960 with (Unnamed Layer* 757) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_961\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_961 with (Unnamed Layer* 760) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/self_attn/Constant_29_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/self_attn/Constant_29_output_0 with (Unnamed Layer* 963) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_981\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_981 with (Unnamed Layer* 979) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 984) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 988) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 992) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_982\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_982 with (Unnamed Layer* 999) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_983\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_983 with (Unnamed Layer* 1004) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_984\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_984 with (Unnamed Layer* 1008) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/input_layernorm/Constant_output_0 with (Unnamed Layer* 1013) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 1017) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 1021) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_985\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_985 with (Unnamed Layer* 1034) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_986\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_986 with (Unnamed Layer* 1037) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_987\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_987 with (Unnamed Layer* 1040) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/self_attn/Constant_29_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/self_attn/Constant_29_output_0 with (Unnamed Layer* 1243) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1007\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1007 with (Unnamed Layer* 1259) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 1264) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 1268) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 1272) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1008\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1008 with (Unnamed Layer* 1279) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1009\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1009 with (Unnamed Layer* 1284) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1010\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1010 with (Unnamed Layer* 1288) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/norm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/norm/Constant_output_0 with (Unnamed Layer* 1293) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/norm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/norm/Constant_1_output_0 with (Unnamed Layer* 1297) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/norm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/norm/Constant_2_output_0 with (Unnamed Layer* 1301) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1011\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1011 with (Unnamed Layer* 1308) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/Unsqueeze_12\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/Unsqueeze_12 with llama_model/Unsqueeze_13\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/Unsqueeze\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/Unsqueeze with llama_model/Reshape\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Reshape with llama_model/layers.0/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Reshape_1 with llama_model/layers.0/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Reshape_2 with llama_model/layers.0/self_attn/Transpose_2\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/Unsqueeze_7\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/Unsqueeze_7 with llama_model/Unsqueeze_8\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 407) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 407) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Transpose_4 with llama_model/layers.0/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Reshape with llama_model/layers.1/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Reshape_1 with llama_model/layers.1/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Reshape_2 with llama_model/layers.1/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 687) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 687) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Transpose_4 with llama_model/layers.1/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Reshape with llama_model/layers.2/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Reshape_1 with llama_model/layers.2/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Reshape_2 with llama_model/layers.2/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 967) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 967) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Transpose_4 with llama_model/layers.2/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Reshape with llama_model/layers.3/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Reshape_1 with llama_model/layers.3/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Reshape_2 with llama_model/layers.3/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 1247) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 1247) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Transpose_4 with llama_model/layers.3/self_attn/Reshape_3\n",
      "[X] Found llama_model/layers.0/self_attn/Softmax to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.0/self_attn/q_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.0/self_attn/v_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.0/self_attn/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.0/self_attn/k_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.0/self_attn/MatMul_1 to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.1/self_attn/q_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.1/self_attn/k_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.1/self_attn/v_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.1/self_attn/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.1/self_attn/Softmax to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.1/self_attn/MatMul_1 to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.2/self_attn/q_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.2/self_attn/k_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.2/self_attn/v_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.2/self_attn/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.2/self_attn/Softmax to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.2/self_attn/MatMul_1 to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.3/self_attn/q_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.3/self_attn/k_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.3/self_attn/v_proj/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.3/self_attn/MatMul to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.3/self_attn/Softmax to be part of self-attention pattern.\n",
      "[X] Found llama_model/layers.3/self_attn/MatMul_1 to be part of self-attention pattern.\n",
      "[X] Found and reassigned Myelin backends for Self-Attention nodes\n",
      "[X] After Myelin optimization: 1 layers\n",
      "[X] Applying ScaleNodes fusions.\n",
      "[X] After scale fusion: 1 layers\n",
      "[X] After dupe layer removal: 1 layers\n",
      "[X] After final dead-layer removal: 1 layers\n",
      "[X] After tensor merging: 1 layers\n",
      "[X] After vertical fusions: 1 layers\n",
      "[X] After dupe layer removal: 1 layers\n",
      "[X] After final dead-layer removal: 1 layers\n",
      "[X] After tensor merging: 1 layers\n",
      "[X] After slice removal: 1 layers\n",
      "[X] After concat removal: 1 layers\n",
      "[X] Trying to split Reshape and strided tensor\n",
      "[V] Graph optimization time: 0.052588 seconds.\n",
      "[V] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[X] Building graph using backend strategy 2\n",
      "[V] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[X] Constructing optimization profile number 0 [1/1].\n",
      "[X] Applying generic optimizations to the graph for inference.\n",
      "[X] 0 requirements combinations were removed due to type constraints.\n",
      "[X] Reserving memory for host IO tensors. Host: 0 bytes\n",
      "[X] =============== Computing costs for {ForeignNode[(Unnamed Layer* 38) [Constant] + (Unnamed Layer* 39) [Shuffle]...lm_head/MatMul]}\n",
      "[X] *************** Autotuning format combination: Int32(sequence,1) -> Float((* 32000 sequence),32000,1) ***************\n",
      "[X] --------------- Timing Runner: {ForeignNode[(Unnamed Layer* 38) [Constant] + (Unnamed Layer* 39) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023])\n",
      "[X]  (foreignNode) Set user's cuda kernel library\n",
      "[X] Tactic: 0x0000000000000000 Time: 0.185573\n",
      "[X] {ForeignNode[(Unnamed Layer* 38) [Constant] + (Unnamed Layer* 39) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023]) profiling completed in 4.48528 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.185573\n",
      "[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000\n",
      "[X] *************** Autotuning format combination: Int32(sequence,1) -> Half((* 32000 sequence),32000,1) ***************\n",
      "[X] --------------- Timing Runner: {ForeignNode[(Unnamed Layer* 38) [Constant] + (Unnamed Layer* 39) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023])\n",
      "[X]  (foreignNode) Set user's cuda kernel library\n",
      "[X] Tactic: 0x0000000000000000 Time: 0.164043\n",
      "[X] {ForeignNode[(Unnamed Layer* 38) [Constant] + (Unnamed Layer* 39) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023]) profiling completed in 4.90023 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.164043\n",
      "[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000\n",
      "[X] =============== Computing reformatting costs\n",
      "[X] =============== Computing reformatting costs\n",
      "[X] =============== Computing reformatting costs: \n",
      "[X] *************** Autotuning Reformat: Float((* 32000 sequence),32000,1) -> Half((* 32000 sequence),32000,1) ***************\n",
      "[X] --------------- Timing Runner: Optimizer Reformat(<in> -> logits) (Reformat[0x80000006])\n",
      "[X] Tactic: 0x00000000000003e8 Time: 0.00406361\n",
      "[X] Tactic: 0x00000000000003ea Time: 0.0119505\n",
      "[X] Tactic: 0x0000000000000000 Time: 0.00940504\n",
      "[X] Optimizer Reformat(<in> -> logits) (Reformat[0x80000006]) profiling completed in 0.0143995 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00406361\n",
      "[X] Formats and tactics selection completed in 9.41689 seconds.\n",
      "[X] After reformat layers: 1 layers\n",
      "[X] Total number of blocks in pre-optimized block assignment: 1\n",
      "[V] Detected 1 inputs and 1 output network tensors.\n",
      "[X] Layer: {ForeignNode[(Unnamed Layer* 38) [Constant] + (Unnamed Layer* 39) [Shuffle]...lm_head/MatMul]} Host Persistent: 32 Device Persistent: 0 Scratch Memory: 24064\n",
      "[X] Skipped printing memory information for 0 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.\n",
      "[V] Total Host Persistent Memory: 32\n",
      "[V] Total Device Persistent Memory: 0\n",
      "[V] Total Scratch Memory: 24064\n",
      "[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2 MiB, GPU 8 MiB\n",
      "[V] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.\n",
      "[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.022907ms to assign 1 blocks to 1 nodes requiring 24064 bytes.\n",
      "[X] Total number of blocks in optimized block assignment: 1\n",
      "[V] Total Activation Memory: 24064\n",
      "[X] Total number of generated kernels selected for the engine: 0\n",
      "[X] Disabling unused tactic source: EDGE_MASK_CONVOLUTIONS\n",
      "[X] Disabling unused tactic source: JIT_CONVOLUTIONS\n",
      "[X] Engine generation completed in 9.55548 seconds.\n",
      "[W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[W] Check verbose logs for the list of affected weights.\n",
      "[W] - 30 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[X]   List of affected weights: llama_model/layers.0/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 173) [Shuffle], llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 428) [Shuffle], llama_model/layers.1/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 457) [Shuffle], llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 708) [Shuffle], llama_model/layers.2/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 737) [Shuffle], llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 988) [Shuffle], llama_model/layers.3/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 1017) [Shuffle], llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 1268) [Shuffle], llama_model/norm/Constant_1_output_0 + (Unnamed Layer* 1297) [Shuffle], llama_model_embed_tokens_weight_constant, onnx__MatMul_1007 _ (Unnamed Layer_ 1259) [Shuffle]_constant, onnx__MatMul_1009 _ (Unnamed Layer_ 1284) [Shuffle]_constant, onnx__MatMul_1010 _ (Unnamed Layer_ 1288) [Shuffle]_constant, onnx__MatMul_1011 _ (Unnamed Layer_ 1308) [Shuffle]_constant, onnx__MatMul_929 _ (Unnamed Layer_ 419) [Shuffle]_constant, onnx__MatMul_930 _ (Unnamed Layer_ 439) [Shuffle]_constant, onnx__MatMul_931 _ (Unnamed Layer_ 444) [Shuffle]_constant, onnx__MatMul_932 _ (Unnamed Layer_ 448) [Shuffle]_constant, onnx__MatMul_933 _ (Unnamed Layer_ 474) [Shuffle]_constant, onnx__MatMul_955 _ (Unnamed Layer_ 699) [Shuffle]_constant, onnx__MatMul_956 _ (Unnamed Layer_ 719) [Shuffle]_constant, onnx__MatMul_957 _ (Unnamed Layer_ 724) [Shuffle]_constant, onnx__MatMul_958 _ (Unnamed Layer_ 728) [Shuffle]_constant, onnx__MatMul_960 _ (Unnamed Layer_ 757) [Shuffle]_constant, onnx__MatMul_981 _ (Unnamed Layer_ 979) [Shuffle]_constant, onnx__MatMul_982 _ (Unnamed Layer_ 999) [Shuffle]_constant, onnx__MatMul_983 _ (Unnamed Layer_ 1004) [Shuffle]_constant, onnx__MatMul_984 _ (Unnamed Layer_ 1008) [Shuffle]_constant, onnx__MatMul_985 _ (Unnamed Layer_ 1034) [Shuffle]_constant, onnx__MatMul_987 _ (Unnamed Layer_ 1040) [Shuffle]_constant\n",
      "[W] - 2 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "[X]   List of affected weights: llama_model_embed_tokens_weight_constant, onnx__MatMul_1011 _ (Unnamed Layer_ 1308) [Shuffle]_constant\n",
      "[W] - 2 weights are affected by this issue: Detected finite FP32 values which would overflow in FP16 and converted them to the closest finite FP16 value.\n",
      "[X]   List of affected weights: (Unnamed Layer* 44) [Constant] + (Unnamed Layer* 45) [Shuffle], llama_model/Constant_39_output_0 + (Unnamed Layer* 163) [Shuffle]\n",
      "[X] Engine Layer Information:\n",
      "    Layer(Myelin): {ForeignNode[(Unnamed Layer* 38) [Constant] + (Unnamed Layer* 39) [Shuffle]...lm_head/MatMul]}, Tactic: 0x0000000000000000, input_ids (Int32[1,-1]) -> logits (Half[1,-1,32000])\n",
      "[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)\n",
      "[X] Adding 1 engine(s) to plan file.\n",
      "[I] Finished engine building in 9.647 seconds\n",
      "[X] Deleting timing cache: 1 entries, served 0 hits since creation.\n",
      "[X] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1\n",
      "[X] Plugin creator already registered - ::BatchedNMS_TRT version 1\n",
      "[X] Plugin creator already registered - ::BatchTilePlugin_TRT version 1\n",
      "[X] Plugin creator already registered - ::Clip_TRT version 1\n",
      "[X] Plugin creator already registered - ::CoordConvAC version 1\n",
      "[X] Plugin creator already registered - ::CropAndResizeDynamic version 1\n",
      "[X] Plugin creator already registered - ::CropAndResize version 1\n",
      "[X] Plugin creator already registered - ::DecodeBbox3DPlugin version 1\n",
      "[X] Plugin creator already registered - ::DetectionLayer_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_TRT version 1\n",
      "[X] Plugin creator already registered - ::FlattenConcat_TRT version 1\n",
      "[X] Plugin creator already registered - ::GenerateDetection_TRT version 1\n",
      "[X] Plugin creator already registered - ::GridAnchor_TRT version 1\n",
      "[X] Plugin creator already registered - ::GridAnchorRect_TRT version 1\n",
      "[X] Plugin creator already registered - ::InstanceNormalization_TRT version 1\n",
      "[X] Plugin creator already registered - ::InstanceNormalization_TRT version 2\n",
      "[X] Plugin creator already registered - ::LReLU_TRT version 1\n",
      "[X] Plugin creator already registered - ::ModulatedDeformConv2d version 1\n",
      "[X] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1\n",
      "[X] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1\n",
      "[X] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[X] Plugin creator already registered - ::NMSDynamic_TRT version 1\n",
      "[X] Plugin creator already registered - ::NMS_TRT version 1\n",
      "[X] Plugin creator already registered - ::Normalize_TRT version 1\n",
      "[X] Plugin creator already registered - ::PillarScatterPlugin version 1\n",
      "[X] Plugin creator already registered - ::PriorBox_TRT version 1\n",
      "[X] Plugin creator already registered - ::ProposalDynamic version 1\n",
      "[X] Plugin creator already registered - ::ProposalLayer_TRT version 1\n",
      "[X] Plugin creator already registered - ::Proposal version 1\n",
      "[X] Plugin creator already registered - ::PyramidROIAlign_TRT version 1\n",
      "[X] Plugin creator already registered - ::Region_TRT version 1\n",
      "[X] Plugin creator already registered - ::Reorg_TRT version 1\n",
      "[X] Plugin creator already registered - ::ResizeNearest_TRT version 1\n",
      "[X] Plugin creator already registered - ::ROIAlign_TRT version 1\n",
      "[X] Plugin creator already registered - ::RPROI_TRT version 1\n",
      "[X] Plugin creator already registered - ::ScatterND version 1\n",
      "[X] Plugin creator already registered - ::SpecialSlice_TRT version 1\n",
      "[X] Plugin creator already registered - ::Split version 1\n",
      "[X] Plugin creator already registered - ::VoxelGeneratorPlugin version 1\n",
      "[V] Loaded engine size: 3 MiB\n",
      "[X] Deserialization required 2354 microseconds.\n",
      "[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +2, now: CPU 0, GPU 2 (MiB)\n",
      "[X] Adding 1 engine(s) to plan file.\n",
      "[I] Saving engine to ./models/llama-xs/trt-engine/llama-xs-bs1.engine\n"
     ]
    }
   ],
   "source": [
    "engine_path = os.path.join(trt_engine_folder, f\"{model_name}-{engine_tag}.engine\")\n",
    "\n",
    "if os.path.exists(engine_path):\n",
    "    os.remove(engine_path)\n",
    "\n",
    "if not os.path.exists(engine_path):\n",
    "    gpt2_engine = GPT2ONNXFile(onnx_path, metadata).as_trt_engine(output_fpath=engine_path, profiles=profiles, preview_features=preview_features)\n",
    "else:\n",
    "    gpt2_engine = GPT2TRTEngine(engine_path, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82d909c9-6783-4a92-8ffd-7b0d598d8daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GPT2.export.GPT2TRTEngine at 0x7fbf109a5630>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9ef7153-808d-4bd0-90c9-ebc44b1eb6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2077"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(engine_path).stat().st_size / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3378c4dc-ad06-41ae-81ed-29cf7cfc533e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {},
   "source": [
    "### Inference with TensorRT engine\n",
    "\n",
    "Great, if you have reached this stage, it means we now have an optimized TensorRT engine for the GPT-2 model, ready for us to carry out inference. \n",
    "\n",
    "The GPT-2 model with TensorRT backend can now be employed in place of the original HuggingFace GPT-2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae13aa-bf6f-4eb7-a453-389865562ae4",
   "metadata": {},
   "source": [
    "#### Single batch inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b58f1-3d9f-4844-85c9-73058bd36a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from GPT2.trt import GPT2TRTDecoder\n",
    "config = GPT2Config.from_pretrained(GPT2_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfda583-b684-48b1-9046-15ab022ef982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2_trt = GPT2TRTDecoder(gpt2_engine, metadata, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc60ad-73a7-46df-85d7-a292a8abbd80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Benchmarking TensorRT performance on single batch\n",
    "_, decoder_e2e_median_time = gpt2_inference(\n",
    "            gpt2_trt, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    "        )\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d86d29-1c7b-4020-9ef2-b77ea5e52764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = gpt2_trt(input_ids=input_ids)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e0162-c9eb-473d-ace6-c4c61ff578b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22122064-5a17-4990-bd6b-073fca5a3e9b",
   "metadata": {},
   "source": [
    "#### Open-end text generation\n",
    "Let's generate the same task again. Since GPT-2 is an open-ended model, a small turbulent in the model might have a very different result. Since we have done some format changes and input/output restriction while exporting the model, you might see a different result compared to raw HuggingFace model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bffb8-a7a4-4fcb-91c9-f4e9f7263e6c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sample_output = gpt2_trt.generate(input_ids.cuda(), max_length=max_length)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8bc4c-bf3e-4cb5-afc6-c0bd7d8655cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    gpt2_trt, input_ids.cuda(), tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68a915-2c32-49e5-b1f6-e93d7618f637",
   "metadata": {},
   "source": [
    "You can now compare the output of the original PyTorch model and the TensorRT engine. Notice the speed difference. On an NVIDIA V100 32GB GPU, this results in about ~5x performance improvement for the GPT-2 model (from an average of 0.704s to 0.134s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2562388-d97b-45dd-8569-3f6c053f4e98",
   "metadata": {},
   "source": [
    "Now you have known how to convert a model to onnx, build TRT engine and optimize it. As you might have recalled, using kv cache and beam search are two important ways to improve the performance of the decoder models. We have recently added thse support to our HuggingFace demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4132e54-aba7-42ec-8324-c68d82c17296",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. Advanced Topic: KV Cache\n",
    "\n",
    "As you have seen above, we put `use_cache = False` in some code blocks. This is because in the simplified model, we only take `input_ids` as input and `logits` as output. `input_ids` is growing as the sequence goes longer. In reality, we sometimes cache the self-attentions for each layer and reuse them in the later computations. This allows us to only take the last generated `input_ids`. This is a trade-off between space and time. When the model is small or the sequence is small, the D2D data copy time usually outweights the performance improvement of the model. However, performance improvements have been found in larger models with larger sequence length like 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d1dcb-250f-4d86-9726-b114d4962fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "kv_config = GPT2Config.from_pretrained(GPT2_VARIANT, use_cache = use_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fdf0f-2da0-46c0-a948-e4e6e16b898a",
   "metadata": {},
   "source": [
    "#### Raw HuggingFace\n",
    "\n",
    "The model that we download from `GPT2LMHeadModel.from_pretrained` is dynamic in its inputs. It can take both kv and non-kv configurations. Changing `use_cache` will do it. You can see that changing this configuration, the output is changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3c51a-07ee-4936-b620-50766a45b945",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    model, input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, use_cache = use_cache\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14607bf-f449-4151-9076-d099ae1a3ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_output = model.generate(input_ids, max_length=max_length, use_cache = use_cache)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057ef83-0cdc-4631-9958-66d04fc7fc22",
   "metadata": {},
   "source": [
    "#### TensorRT\n",
    "\n",
    "For the 1st decoding step, we take `input_ids` and generate both `logits` and the kv cache. In other steps, we take the new `input_ids` with `past` kv-cache and the outputs are `logits` and the updated `present` kv-cache. Taking dynamic number of inputs for trt is not currently supported in our demo, so we need to output 2 onnx files and build 2 engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbcfad-9c9c-47e2-894a-731c7a3a04df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_metadata = NetworkMetadata(variant=GPT2_VARIANT, precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=use_cache))\n",
    "kv_gpt2 = GPT2TorchFile(model.to('cpu'), kv_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe680c5-d9ff-466f-87fe-a7bb0cbee944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_onnx_path = ('./models/{}/ONNX/{}-kv_cache.onnx'.format(GPT2_VARIANT, GPT2_VARIANT))\n",
    "kv_gpt2.as_onnx_model(kv_onnx_path, force_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f6824-286d-4afa-926b-7eed4cafafc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_onnx_model = onnx.load(kv_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f0012-7a2d-41be-a8d8-c818dcb7c244",
   "metadata": {},
   "source": [
    "We could see that the kv model has #inputs = #outputs = num_layers * 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579aeec-2c7a-43de-b8f7-beff8d3d7784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(kv_onnx_model.graph.input), len(kv_onnx_model.graph.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add1139-aab0-4531-b2ac-c3aca90e5d49",
   "metadata": {},
   "source": [
    "The next blocks will set up the profile and build the engine. The only difference is that we now have the profile for kv cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae055cb-41b7-4523-86bc-490bc9edf204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "disable_preview_dynamic_shapes = False\n",
    "\n",
    "engine_tag = \"bs{}\".format(batch_size)\n",
    "\n",
    "preview_features = [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-disableFasterDynamicShapes\"\n",
    "    preview_features = []\n",
    "\n",
    "use_input_length = False\n",
    "num_heads = kv_config.n_head\n",
    "embedding_size_per_head = kv_config.n_embd // num_heads\n",
    "num_layers = kv_config.n_layer\n",
    "\n",
    "max_sequence_length = max_length\n",
    "max_output_length = max_length\n",
    "if not use_input_length:\n",
    "    opt_input_seq_len = max_sequence_length // 2\n",
    "else:\n",
    "    opt_input_seq_len = input_ids.shape[1]\n",
    "\n",
    "opt_output_seq_len = max_output_length // 2\n",
    "\n",
    "# context phase uses the provided input_ids to generate hidden states and self attention kv cache\n",
    "# It is only used in the 1st decoder run.\n",
    "dec_profiles_context = Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, opt_output_seq_len),\n",
    "    max=(batch_size, max_output_length),\n",
    ")\n",
    "self_attention_profile_context = {\n",
    "    \"min\": (batch_size, num_heads, 0, embedding_size_per_head),\n",
    "    \"opt\": (batch_size, num_heads, 0, embedding_size_per_head),\n",
    "    \"max\": (batch_size, num_heads, 0, embedding_size_per_head),\n",
    "}\n",
    "\n",
    "# generation phase uses previous self attention kv cache with the last input_ids token to generate the next hidden states and self attention kv cache\n",
    "# This optimization profile is used after the 1st decoder run.\n",
    "dec_profiles_generation = Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, 1),\n",
    "    max=(batch_size, 1),\n",
    ")\n",
    "\n",
    "self_attention_profile_generation = {\n",
    "    \"min\": (batch_size, num_heads, 1, embedding_size_per_head),\n",
    "    \"opt\": (batch_size, num_heads, opt_output_seq_len - 1, embedding_size_per_head),\n",
    "    \"max\": (batch_size, num_heads, max_output_length - 1, embedding_size_per_head),\n",
    "}\n",
    "\n",
    "for i in range(num_layers):\n",
    "    dec_profiles_context = dec_profiles_context.add(\n",
    "        f\"past_key_values.{i}.decoder.key\",\n",
    "        **self_attention_profile_context\n",
    "    ).add(\n",
    "        f\"past_key_values.{i}.decoder.value\",\n",
    "        **self_attention_profile_context\n",
    "    )\n",
    "\n",
    "    dec_profiles_generation = dec_profiles_generation.add(\n",
    "        f\"past_key_values.{i}.decoder.key\",\n",
    "        **self_attention_profile_generation\n",
    "    ).add(\n",
    "        f\"past_key_values.{i}.decoder.value\",\n",
    "        **self_attention_profile_generation\n",
    "    )\n",
    "\n",
    "# TensorRT accepts multiple optimization engines for the same model.\n",
    "# Profile 1 is only used in the first decoder iterations.\n",
    "decoder_profiles = [dec_profiles_generation, dec_profiles_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadf843-9f60-41c7-90a9-098b33ce3603",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_engine_path = os.path.join(trt_engine_folder, f\"{GPT2_VARIANT}-kv_cache_{engine_tag}.engine\")\n",
    "\n",
    "# Set up the trt engine with both kv input/output augmented\n",
    "if not os.path.exists(kv_engine_path):\n",
    "    kv_gpt2_engine = GPT2ONNXFile(kv_onnx_path, kv_metadata).as_trt_engine(kv_engine_path,profiles=decoder_profiles, preview_features=preview_features)\n",
    "else:\n",
    "    kv_gpt2_engine = GPT2TRTEngine(kv_engine_path, kv_metadata)\n",
    "\n",
    "    \n",
    "kv_gpt2_trt = GPT2TRTDecoder(\n",
    "    kv_gpt2_engine, kv_metadata, kv_config, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090007db-9a09-4b6d-95ed-8a688ea05798",
   "metadata": {},
   "source": [
    "Since we have 2 profiles, benchmarking single-run runtime does not make sense. We instead use `full_inference` to measure the time for the entire inference cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b93d88-21bb-4f87-9ff6-709d0babdf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    kv_gpt2_trt, input_ids.cuda(), tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, use_cache = use_cache\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ab217-9ee4-435c-b689-69d98cef1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_gpt2_trt.reset()\n",
    "kv_sample_output = kv_gpt2_trt.generate(input_ids.cuda(), max_length=max_length)\n",
    "tokenizer.decode(kv_sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b614fb8-63d6-4711-84cf-c69ca8b3f141",
   "metadata": {},
   "source": [
    "In this short example, kv cache performance does not improve the performance, and may even be slightly worse than non kv cache mode. However, when we have larger input sequences for the model, it will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764049f-0578-4305-b010-4e7a3156a377",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "\n",
    "## 5. Advanced Topic: Beam Search\n",
    "\n",
    "Beam search is a way to increase the model quality. It looks for the top `num_beams` number of possible words and pick the one that conditions the best to the current position. Similarly, the original HuggingFace PyTorch model supports beam search natively, while we need to build separate trt engine for different `num_beams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5808db-2cc0-4d88-aebe-1b6e17a023e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_config = GPT2Config.from_pretrained(GPT2_VARIANT, use_cache = False)\n",
    "beam_metadata = NetworkMetadata(variant=GPT2_VARIANT, precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=False))\n",
    "num_beams = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1403609-b24d-4e10-a8eb-852d3eab6fa0",
   "metadata": {},
   "source": [
    "#### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd992c8-1eeb-427c-ae32-2c63766c6a69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    model, input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, num_beams = num_beams\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09418760-84bd-4308-b06b-8540945a6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(input_ids, max_length=max_length, num_beams = num_beams)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8d9fa-d74a-40dd-94ce-d98551d24608",
   "metadata": {},
   "source": [
    "You could see that the output is very different from the original one. If you change `num_beams`, the result will also change significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba01e0ec-68ad-4682-8ca4-2ecde7d70f7f",
   "metadata": {},
   "source": [
    "#### TensorRT\n",
    "It uses the same onnx file as the original configuration, but the engine set up is differently, because it expands the inputs by `num_beams` for the first dimension of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055fb314-8e0f-4edd-bf78-16890d196de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimization profile for dynamic shape input. Can modify batch_size / max_sequence_length to build engines for different shapes\n",
    "batch_size = 1\n",
    "disable_preview_dynamic_shapes = False # preview_dynamic_shapes optimizes the trt engine building time\n",
    "# We can either use input length as the optimal length, or use max_length // 2. \n",
    "# In T5 or BART, input_length is better, but in GPT-2, max_length // 2 is better because we need to generate max_length number of tokens\n",
    "\n",
    "use_input_length = False\n",
    "opt_length = input_id.shape[1] if use_input_length else max_length // 2 \n",
    "# Create different engine tags for different configurations\n",
    "engine_tag = f\"bs{batch_size}-beam{num_beams}\"\n",
    "\n",
    "preview_features = [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-disableFasterDynamicShapes\"\n",
    "    preview_features = []\n",
    "    \n",
    "\n",
    "beam_profiles = [Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, opt_length), # Optimized based on the inputs. \n",
    "    max=(batch_size * num_beams, max_length),\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18986d0f-9509-463f-a489-a76dd4d28a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd04a7b-8aa6-4c97-8d85-96f14b06abbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beam_engine_path = os.path.join(trt_engine_folder, f\"{GPT2_VARIANT}-{engine_tag}.engine\")\n",
    "if not os.path.exists(beam_engine_path):\n",
    "    beam_gpt2_engine = GPT2ONNXFile(onnx_path, beam_metadata).as_trt_engine(output_fpath=beam_engine_path, profiles=beam_profiles, preview_features=preview_features)\n",
    "else:\n",
    "    beam_gpt2_engine = GPT2TRTEngine(beam_engine_path, beam_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe1dba-4e84-478e-9ea7-07c21856e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_gpt2_trt = GPT2TRTDecoder(beam_gpt2_engine, beam_metadata, beam_config, num_beams = num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42614e14-c962-4c31-a469-7e0343efbdbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    beam_gpt2_trt, input_ids.cuda(), tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, num_beams=num_beams\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ab05a-fe0d-42c3-9591-605ddab389ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_sample_output = beam_gpt2_trt.generate(input_ids.cuda(), max_length=max_length, num_beams=num_beams)\n",
    "tokenizer.decode(beam_sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543dbfd-4650-46f5-8f77-587dcb05785a",
   "metadata": {},
   "source": [
    "We could see that because of larger batch size, beam search will take slightly longer, but for most sequences, it will generate more meaningful outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc6c04-ca47-4fc6-9a12-ed500722bb4a",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch GPT-2 model to an optimized TensorRT engine for inference in 3 easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace GPT-2 model while providing significant speed up. \n",
    "\n",
    "If you are interested in further details of the conversion process, check out [GPT2/trt.py](../GPT2/trt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14079b8f-738e-4137-9ca3-6a4254e8f006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
