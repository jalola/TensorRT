{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66562c2d-1586-4830-be91-e0d7a1ccd10c",
   "metadata": {},
   "source": [
    "# Test Llama 7b model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28559219-9845-4c04-9fac-5e0e71d199d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# import transformers\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f0b9c1-0c66-44e8-9c74-fe3cabc1cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf21a80-97df-4df7-8ea1-41b1d652b217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"codellama/CodeLlama-7b-hf\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176895eb-538e-4586-9d29-377cb70609c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# sequences = pipeline(\n",
    "#     'def fibonacci(',\n",
    "#     do_sample=True,\n",
    "#     temperature=0.2,\n",
    "#     top_p=0.9,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     max_length=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abaa0553-b2ed-46ab-a636-beb36a062854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37c46b-5b0c-48d6-a2ff-7ed85dc080fb",
   "metadata": {},
   "source": [
    "# Test small xs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce23e94-2de5-43f7-9520-c0a28600a375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e75a007a-9811-47e6-8053-e8c554fb6fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 749/749 [00:00<00:00, 3.49MB/s]\n",
      "Downloading tokenizer.model: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 1.82MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 25.8MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 411/411 [00:00<00:00, 2.29MB/s]\n",
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00d1972-61c0-45c2-9d0e-860081d732cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# huggingface\n",
    "# from transformers import (\n",
    "#     GPT2LMHeadModel,\n",
    "#     GPT2Tokenizer,\n",
    "#     GPT2Config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "510deb98-a498-45e4-9b32-8e5d92782f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaModel, LlamaConfig, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3b4a5a5-f937-439e-b223-a8e0e78cff15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.34.0.dev0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e814cc8c-e40b-45f5-93f0-a9dee3cc6aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"attention_bias\": false,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 64,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 128,\n",
       "  \"max_position_embeddings\": 128,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.34.0.dev0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"llama-xs\"\n",
    "max_length = 20\n",
    "\n",
    "xs_config = LlamaConfig(\n",
    "    hidden_size=16*4,\n",
    "    intermediate_size=128,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=None,\n",
    "    hidden_act='silu',\n",
    "    max_position_embeddings=128,\n",
    "    initializer_range=0.02,\n",
    "    rms_norm_eps=1e-06,\n",
    "    use_cache=False,\n",
    "    pad_token_id=None,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pretraining_tp=1,\n",
    "    tie_word_embeddings=False,\n",
    "    rope_theta=10000.0,\n",
    ")\n",
    "xs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9afc5a0f-6bec-4ae9-b830-f76668592279",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = LlamaModel(xs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f40299b3-baa6-4661-9ff0-a741fdd8987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_causalllm = LlamaForCausalLM(xs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d05f3252-c62e-4468-9454-056494bb8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Hello, my dog is a dog\"\n",
    "prompt2 = \"Hey, are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7e97feb-dc8e-4fcf-896b-13d75b001d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([prompt1, prompt2], padding=True, return_tensors=\"pt\")\n",
    "model_inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f42fc4dc-978a-42b3-aa04-d6cd90634cbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is a dog`).`).`).`).`).`).`).`).`).`).`).`).`). leng check知 стату Verm Verm Verm Verm Verm'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "# generate_ids = llama_causalllm.generate(inputs.input_ids, max_length=30)\n",
    "generate_ids = llama_causalllm.generate(model_inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/llama-xs/pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(model_name)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "llama_causalllm.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4af5ada0-9358-46f0-a682-f9722cd5f027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/llama-xs/pytorch'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c5766-97ed-4d04-bab5-7fa18e89dee8",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e43067c2-ecd9-4bd6-9047-a3f74621931b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# carry out inference with a single sample\n",
    "input_str = \"Hello, my dog is a dog\"\n",
    "inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d347ddf-4504-4ab7-b15b-29d218bdd7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1, 15043, 29892,   590, 11203,   338,   263, 11203]]),\n",
       " torch.Size([1, 8]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf83454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu\n"
     ]
    }
   ],
   "source": [
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    print(\"Using cpu\")\n",
    "    llama_causalllm = llama_causalllm.cpu()\n",
    "    input_ids = input_ids.cpu()\n",
    "    inputs = inputs.to('cpu')\n",
    "else:\n",
    "    print(\"Using gpu\")\n",
    "    llama_causalllm = llama_causalllm.cuda()\n",
    "    input_ids = input_ids.cuda()\n",
    "    inputs = inputs.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c65e83e6-8b89-4c3c-a2fb-19615557cb7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.4 ms ± 2.38 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "generate_ids = llama_causalllm.generate(input_ids, max_length=20)\n",
    "tokenizer.decode(generate_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e47d40a-1ca3-4f31-98a2-04a968827c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is a dog`).`).`).`).`).`).`).`).`).`).`).`).'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ids = llama_causalllm.generate(input_ids, max_length=20)\n",
    "tokenizer.decode(generate_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6c2ea-3450-4b8b-9cc8-09943d967ece",
   "metadata": {},
   "source": [
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b844f057-e768-467d-9185-68fb4c74b5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_causalllm.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = llama_causalllm(**inputs, labels=inputs['input_ids'], use_cache = False)\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "717b2f68-9d92-474e-9937-8b42a1c60d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0386,  0.1334, -0.3535,  ...,  0.0198,  0.1487, -0.1088],\n",
       "          [-0.0242,  0.0524, -0.3753,  ...,  0.1425,  0.1202, -0.2102],\n",
       "          [-0.0914,  0.1016, -0.4090,  ...,  0.0519,  0.0864, -0.1593],\n",
       "          ...,\n",
       "          [ 0.0956, -0.0386, -0.3524,  ...,  0.1093,  0.1433, -0.2476],\n",
       "          [-0.0195,  0.0074, -0.3320,  ...,  0.1180, -0.0042, -0.2238],\n",
       "          [ 0.1060,  0.0197, -0.3005,  ...,  0.1625,  0.1257, -0.2346]]],\n",
       "        device='cuda:0'),\n",
       " torch.Size([1, 8, 32000]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX format\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format: ONNX.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "At a high level, the steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU with the TensorRT engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b79f67ad-080a-4d85-926f-d2812e672794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkMetadata(variant='llama-xs', precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=False))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "from GPT2.GPT2ModelConfig import GPT2Metadata\n",
    "metadata = NetworkMetadata(variant=model_name, precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=False))\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58be125b-7d33-49ad-af84-1558f88251f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/working/repos/transformers/src/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Module\n",
    "from transformers.generation_utils import GenerationMixin\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "class TorchModule(Module, GenerationMixin):\n",
    "    \"\"\"\n",
    "    A simplied definition of Llama.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llama_model, lm_head, config):\n",
    "        super().__init__()\n",
    "        self.llama_model = llama_model\n",
    "        self.lm_head = lm_head\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda') # WAR to avoid beam search in framework\n",
    "        self.main_input_name = \"input_ids\" # For better HuggingFace version compatibility\n",
    "\n",
    "    # def prepare_inputs_for_generation(self, input_ids, past = None, use_cache=None, **kwargs):\n",
    "    #     # Todo (@pchadha): add position_ids, token_type_ids support\n",
    "    #     # cut decoder_input_ids if past is used\n",
    "    #     if past is not None:\n",
    "    #         input_ids = input_ids[:, -1:]\n",
    "\n",
    "    #     return {\n",
    "    #         \"input_ids\": input_ids,\n",
    "    #         \"use_cache\": use_cache,\n",
    "    #         \"past_key_values\": past\n",
    "    #     }\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        outputs = self.llama_model(input_ids, **kwargs)\n",
    "        hidden_states = outputs[0]\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            logits=lm_logits, \n",
    "            past_key_values=outputs.past_key_values\n",
    "        )\n",
    "\n",
    "    # def _reorder_cache(self, past, beam_idx):\n",
    "    #     \"\"\"\n",
    "    #     This function is used to re-order the :obj:`past_key_values` cache if\n",
    "    #     :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n",
    "    #     called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n",
    "    #     \"\"\"\n",
    "    #     return tuple(\n",
    "    #         tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "    #         for layer_past in past\n",
    "    #     )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "238d93a9-e511-4bcf-a2ca-6ca0eeccd567",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = ('input_ids',)\n",
    "output_names = ('logits',)\n",
    "input_dynamic_axis = {'input_ids': {0: 'batch', 1: 'sequence'}}\n",
    "output_dynamic_axis = {'logits': {0: 'batch', 1: 'sequence'}}\n",
    "\n",
    "opt_args = {}\n",
    "\n",
    "output_fpath = ('./models/{}/ONNX/{}.onnx'.format(model_name, model_name))\n",
    "output_fpath_sim = ('./models/{}/ONNX/{}_sim.onnx'.format(model_name, model_name))\n",
    "Path(output_fpath).parent.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68c4ba2c-8450-40d6-80b3-a8eeebd5cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchModule(llama_causalllm.model, llama_causalllm.lm_head, xs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71e01fc7-3670-4db1-9798-7bded28fca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/working/repos/transformers/src/transformers/models/llama/modeling_llama.py:815: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/ec2-user/working/repos/transformers/src/transformers/models/llama/modeling_llama.py:146: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/home/ec2-user/working/repos/transformers/src/transformers/models/llama/modeling_llama.py:386: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "/home/ec2-user/working/repos/transformers/src/transformers/models/llama/modeling_llama.py:393: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "/home/ec2-user/working/repos/transformers/src/transformers/models/llama/modeling_llama.py:403: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_forward = torch_model.forward\n",
    "def _export_forward(input_ids, **kwargs):\n",
    "    kwargs[\"use_cache\"] = False\n",
    "    result = old_forward(input_ids, **kwargs)\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "torch_model.forward = _export_forward\n",
    "\n",
    "torch.onnx.export(\n",
    "    torch_model,\n",
    "    input_ids,\n",
    "    output_fpath,\n",
    "    opset_version=13,\n",
    "    do_constant_folding=True,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes={\n",
    "        **input_dynamic_axis,\n",
    "        **output_dynamic_axis,\n",
    "    },\n",
    "    training=torch.onnx.TrainingMode.EVAL,\n",
    "    **opt_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba8a16bc-165f-4d58-87af-e847c9a179dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/llama-xs/ONNX/llama-xs.onnx'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eea268ad-bb84-4131-8c7c-e74be6ed5964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 18M Oct  1 11:15 ./models/llama-xs/ONNX/llama-xs.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $output_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed8bc4ef-c2e7-4bfc-8c12-81e0e2d26c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !onnxsim $output_fpath $output_fpath_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93a14724-5283-44fe-840c-ff9b77438894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 17M\n",
      "-rw-r--r-- 1 root root 607 Oct  1 11:15 config.json\n",
      "-rw-r--r-- 1 root root 138 Oct  1 11:15 generation_config.json\n",
      "-rw-r--r-- 1 root root 17M Oct  1 11:15 pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $pytorch_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1eb21049-5722-4e78-b073-45ee53ea4fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 18M Oct  1 11:15 ./models/llama-xs/ONNX/llama-xs.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $output_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cab5652c-3e18-4fe9-bc9b-2bc1a9d5dc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.949639"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(output_fpath).stat().st_size / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b04de1-e887-445c-9bc8-e2a7e0fca7ea",
   "metadata": {},
   "source": [
    "Let's take a look at the onnx file and investigate its input and output. You should see that \"input_ids\" as the input, and \"logits\" as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29c82042-b6d7-4739-beec-2f1093f69440",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = output_fpath\n",
    "onnx_path_sim = output_fpath_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e4fff25-97da-4f9f-ae98-e918745faebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03c409e6-d312-4cc7-b13f-4621609d5633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2314caaf-836d-4140-93e4-4b3f4c931347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input_ids\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fe7a8d4-2bc3-49fc-863a-0e7f4be6565e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"logits\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 32000\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.graph.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "567e7779-b0cc-4c9b-887c-bf8384918595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT engine\n",
    "\n",
    "Now we are ready to parse the ONNX model and convert it to an optimized TensorRT model.\n",
    "\n",
    "Since the model contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile.\n",
    "\n",
    "Note: As TensorRT carries out many optimization, this conversion process for the larger model might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from polygraphy.backend.trt import Profile\n",
    "from tensorrt import PreviewFeature\n",
    "from GPT2.export import GPT2ONNXFile, GPT2TRTEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./models/$model_name/trt-engine\n",
    "trt_engine_folder = './models/{}/trt-engine'.format(model_name)\n",
    "\n",
    "# Create optimization profile for dynamic shape input. Can modify batch_size / max_sequence_length to build engines for different shapes\n",
    "batch_size = 1\n",
    "disable_preview_dynamic_shapes = False # preview_dynamic_shapes optimizes the trt engine building time\n",
    "# We can either use input length as the optimal length, or use max_length // 2. \n",
    "# In T5 or BART, input_length is better, but in GPT-2, max_length // 2 is better because we need to generate max_length number of tokens\n",
    "\n",
    "use_input_length = False\n",
    "opt_length = input_id.shape[1] if use_input_length else max_length // 2 \n",
    "# Create different engine tags for different configurations\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noPreviewFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features += [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "\n",
    "profiles = [Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, opt_length), # Optimized based on the inputs. \n",
    "    max=(batch_size, max_length),\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5538106b-3ae4-4d5f-b0ee-1f76174dcecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Profile().add('input_ids', min=(1, 1), opt=(1, 10), max=(1, 20))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7852a4d8-ebcd-46be-b725-3ed77fc621a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805: 1>,\n",
       " <PreviewFeature.FASTER_DYNAMIC_SHAPES_0805: 0>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a5934f0-46d3-45d7-8dd5-6cf81de61e66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] 'colored' module is not installed, will not use colors when logging. To enable colors, please install the 'colored' module: python3 -m pip install colored\n",
      "[V] Loaded Module: tensorrt | Version: 8.6.1 | Path: ['/usr/local/lib/python3.10/dist-packages/tensorrt']\n",
      "[V] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 1889, GPU 197 (MiB)\n",
      "[X] Trying to load shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[X] Loaded shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[V] [MemUsageChange] Init builder kernel library: CPU +888, GPU +172, now: CPU 2854, GPU 369 (MiB)\n",
      "[X] CUDA lazy loading is enabled.\n",
      "[V] ----------------------------------------------------------------\n",
      "[V] Input filename:   ./models/llama-xs/ONNX/llama-xs.onnx\n",
      "[V] ONNX IR version:  0.0.7\n",
      "[V] Opset version:    13\n",
      "[V] Producer name:    pytorch\n",
      "[V] Producer version: 2.0.1\n",
      "[V] Domain:           \n",
      "[V] Model version:    0\n",
      "[V] Doc string:       \n",
      "[V] ----------------------------------------------------------------\n",
      "[X] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1\n",
      "[X] Registered plugin creator - ::BatchedNMS_TRT version 1\n",
      "[X] Registered plugin creator - ::BatchTilePlugin_TRT version 1\n",
      "[X] Registered plugin creator - ::Clip_TRT version 1\n",
      "[X] Registered plugin creator - ::CoordConvAC version 1\n",
      "[X] Registered plugin creator - ::CropAndResizeDynamic version 1\n",
      "[X] Registered plugin creator - ::CropAndResize version 1\n",
      "[X] Registered plugin creator - ::DecodeBbox3DPlugin version 1\n",
      "[X] Registered plugin creator - ::DetectionLayer_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_TRT version 1\n",
      "[X] Registered plugin creator - ::FlattenConcat_TRT version 1\n",
      "[X] Registered plugin creator - ::GenerateDetection_TRT version 1\n",
      "[X] Registered plugin creator - ::GridAnchor_TRT version 1\n",
      "[X] Registered plugin creator - ::GridAnchorRect_TRT version 1\n",
      "[X] Registered plugin creator - ::InstanceNormalization_TRT version 1\n",
      "[X] Registered plugin creator - ::InstanceNormalization_TRT version 2\n",
      "[X] Registered plugin creator - ::LReLU_TRT version 1\n",
      "[X] Registered plugin creator - ::ModulatedDeformConv2d version 1\n",
      "[X] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1\n",
      "[X] Registered plugin creator - ::MultilevelProposeROI_TRT version 1\n",
      "[X] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[X] Registered plugin creator - ::NMSDynamic_TRT version 1\n",
      "[X] Registered plugin creator - ::NMS_TRT version 1\n",
      "[X] Registered plugin creator - ::Normalize_TRT version 1\n",
      "[X] Registered plugin creator - ::PillarScatterPlugin version 1\n",
      "[X] Registered plugin creator - ::PriorBox_TRT version 1\n",
      "[X] Registered plugin creator - ::ProposalDynamic version 1\n",
      "[X] Registered plugin creator - ::ProposalLayer_TRT version 1\n",
      "[X] Registered plugin creator - ::Proposal version 1\n",
      "[X] Registered plugin creator - ::PyramidROIAlign_TRT version 1\n",
      "[X] Registered plugin creator - ::Region_TRT version 1\n",
      "[X] Registered plugin creator - ::Reorg_TRT version 1\n",
      "[X] Registered plugin creator - ::ResizeNearest_TRT version 1\n",
      "[X] Registered plugin creator - ::ROIAlign_TRT version 1\n",
      "[X] Registered plugin creator - ::RPROI_TRT version 1\n",
      "[X] Registered plugin creator - ::ScatterND version 1\n",
      "[X] Registered plugin creator - ::SpecialSlice_TRT version 1\n",
      "[X] Registered plugin creator - ::Split version 1\n",
      "[X] Registered plugin creator - ::VoxelGeneratorPlugin version 1\n",
      "[X] Adding network input: input_ids with dtype: int32, dimensions: (-1, -1)\n",
      "[X] Registering tensor: input_ids for ONNX tensor: input_ids\n",
      "[X] Importing initializer: llama_model.embed_tokens.weight\n",
      "[X] Importing initializer: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Importing initializer: onnx::MatMul_1689\n",
      "[X] Importing initializer: onnx::MatMul_1690\n",
      "[X] Importing initializer: onnx::MatMul_1691\n",
      "[X] Importing initializer: onnx::MatMul_1711\n",
      "[X] Importing initializer: onnx::MatMul_1712\n",
      "[X] Importing initializer: onnx::MatMul_1713\n",
      "[X] Importing initializer: onnx::MatMul_1714\n",
      "[X] Importing initializer: onnx::MatMul_1715\n",
      "[X] Importing initializer: onnx::MatMul_1716\n",
      "[X] Importing initializer: onnx::MatMul_1717\n",
      "[X] Importing initializer: onnx::MatMul_1737\n",
      "[X] Importing initializer: onnx::MatMul_1738\n",
      "[X] Importing initializer: onnx::MatMul_1739\n",
      "[X] Importing initializer: onnx::MatMul_1740\n",
      "[X] Importing initializer: onnx::MatMul_1741\n",
      "[X] Importing initializer: onnx::MatMul_1742\n",
      "[X] Importing initializer: onnx::MatMul_1743\n",
      "[X] Importing initializer: onnx::MatMul_1763\n",
      "[X] Importing initializer: onnx::MatMul_1764\n",
      "[X] Importing initializer: onnx::MatMul_1765\n",
      "[X] Importing initializer: onnx::MatMul_1766\n",
      "[X] Importing initializer: onnx::MatMul_1767\n",
      "[X] Importing initializer: onnx::MatMul_1768\n",
      "[X] Importing initializer: onnx::MatMul_1769\n",
      "[X] Importing initializer: onnx::MatMul_1789\n",
      "[X] Importing initializer: onnx::MatMul_1790\n",
      "[X] Importing initializer: onnx::MatMul_1791\n",
      "[X] Importing initializer: onnx::MatMul_1792\n",
      "[X] Importing initializer: onnx::MatMul_1793\n",
      "[X] Importing initializer: onnx::MatMul_1794\n",
      "[X] Importing initializer: onnx::MatMul_1795\n",
      "[X] Importing initializer: onnx::MatMul_1815\n",
      "[X] Importing initializer: onnx::MatMul_1816\n",
      "[X] Importing initializer: onnx::MatMul_1817\n",
      "[X] Importing initializer: onnx::MatMul_1818\n",
      "[X] Importing initializer: onnx::MatMul_1819\n",
      "[X] Importing initializer: onnx::MatMul_1820\n",
      "[X] Importing initializer: onnx::MatMul_1821\n",
      "[X] Importing initializer: onnx::MatMul_1841\n",
      "[X] Importing initializer: onnx::MatMul_1842\n",
      "[X] Importing initializer: onnx::MatMul_1843\n",
      "[X] Importing initializer: onnx::MatMul_1844\n",
      "[X] Importing initializer: onnx::MatMul_1845\n",
      "[X] Importing initializer: onnx::MatMul_1846\n",
      "[X] Importing initializer: onnx::MatMul_1847\n",
      "[X] Importing initializer: onnx::MatMul_1867\n",
      "[X] Importing initializer: onnx::MatMul_1868\n",
      "[X] Importing initializer: onnx::MatMul_1869\n",
      "[X] Importing initializer: onnx::MatMul_1870\n",
      "[X] Importing initializer: onnx::MatMul_1871\n",
      "[X] Importing initializer: onnx::MatMul_1872\n",
      "[X] Importing initializer: onnx::MatMul_1873\n",
      "[X] Importing initializer: onnx::MatMul_1893\n",
      "[X] Importing initializer: onnx::MatMul_1894\n",
      "[X] Importing initializer: onnx::MatMul_1895\n",
      "[X] Importing initializer: onnx::MatMul_1896\n",
      "[X] Importing initializer: onnx::MatMul_1897\n",
      "[X] Parsing node: Identity_0 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_0 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: llama_model.layers.0.input_layernorm.weight for ONNX node: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Registering layer: Identity_0 for ONNX node: Identity_0\n",
      "[X] Registering tensor: llama_model.norm.weight for ONNX tensor: llama_model.norm.weight\n",
      "[X] Identity_0 [Identity] outputs: [llama_model.norm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_1 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_1 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_1 for ONNX node: Identity_1\n",
      "[X] Registering tensor: llama_model.layers.7.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.7.post_attention_layernorm.weight\n",
      "[X] Identity_1 [Identity] outputs: [llama_model.layers.7.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_2 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_2 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_2 for ONNX node: Identity_2\n",
      "[X] Registering tensor: llama_model.layers.7.input_layernorm.weight for ONNX tensor: llama_model.layers.7.input_layernorm.weight\n",
      "[X] Identity_2 [Identity] outputs: [llama_model.layers.7.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_3 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_3 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_3 for ONNX node: Identity_3\n",
      "[X] Registering tensor: llama_model.layers.6.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.6.post_attention_layernorm.weight\n",
      "[X] Identity_3 [Identity] outputs: [llama_model.layers.6.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_4 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_4 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_4 for ONNX node: Identity_4\n",
      "[X] Registering tensor: llama_model.layers.6.input_layernorm.weight for ONNX tensor: llama_model.layers.6.input_layernorm.weight\n",
      "[X] Identity_4 [Identity] outputs: [llama_model.layers.6.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_5 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_5 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_5 for ONNX node: Identity_5\n",
      "[X] Registering tensor: llama_model.layers.5.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.5.post_attention_layernorm.weight\n",
      "[X] Identity_5 [Identity] outputs: [llama_model.layers.5.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_6 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_6 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_6 for ONNX node: Identity_6\n",
      "[X] Registering tensor: llama_model.layers.5.input_layernorm.weight for ONNX tensor: llama_model.layers.5.input_layernorm.weight\n",
      "[X] Identity_6 [Identity] outputs: [llama_model.layers.5.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_7 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_7 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_7 for ONNX node: Identity_7\n",
      "[X] Registering tensor: llama_model.layers.4.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.4.post_attention_layernorm.weight\n",
      "[X] Identity_7 [Identity] outputs: [llama_model.layers.4.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_8 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_8 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_8 for ONNX node: Identity_8\n",
      "[X] Registering tensor: llama_model.layers.4.input_layernorm.weight for ONNX tensor: llama_model.layers.4.input_layernorm.weight\n",
      "[X] Identity_8 [Identity] outputs: [llama_model.layers.4.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_9 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_9 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_9 for ONNX node: Identity_9\n",
      "[X] Registering tensor: llama_model.layers.3.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.3.post_attention_layernorm.weight\n",
      "[X] Identity_9 [Identity] outputs: [llama_model.layers.3.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_10 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_10 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_10 for ONNX node: Identity_10\n",
      "[X] Registering tensor: llama_model.layers.3.input_layernorm.weight for ONNX tensor: llama_model.layers.3.input_layernorm.weight\n",
      "[X] Identity_10 [Identity] outputs: [llama_model.layers.3.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_11 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_11 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_11 for ONNX node: Identity_11\n",
      "[X] Registering tensor: llama_model.layers.2.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.2.post_attention_layernorm.weight\n",
      "[X] Identity_11 [Identity] outputs: [llama_model.layers.2.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_12 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_12 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_12 for ONNX node: Identity_12\n",
      "[X] Registering tensor: llama_model.layers.2.input_layernorm.weight for ONNX tensor: llama_model.layers.2.input_layernorm.weight\n",
      "[X] Identity_12 [Identity] outputs: [llama_model.layers.2.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_13 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_13 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_13 for ONNX node: Identity_13\n",
      "[X] Registering tensor: llama_model.layers.1.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.1.post_attention_layernorm.weight\n",
      "[X] Identity_13 [Identity] outputs: [llama_model.layers.1.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_14 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_14 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_14 for ONNX node: Identity_14\n",
      "[X] Registering tensor: llama_model.layers.1.input_layernorm.weight for ONNX tensor: llama_model.layers.1.input_layernorm.weight\n",
      "[X] Identity_14 [Identity] outputs: [llama_model.layers.1.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_15 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_15 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_15 for ONNX node: Identity_15\n",
      "[X] Registering tensor: llama_model.layers.0.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.0.post_attention_layernorm.weight\n",
      "[X] Identity_15 [Identity] outputs: [llama_model.layers.0.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Shape [Shape]\n",
      "[X] Searching for input: input_ids\n",
      "[X] llama_model/Shape [Shape] inputs: [input_ids -> (-1, -1)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape for ONNX node: llama_model/Shape\n",
      "[X] Registering tensor: llama_model/Shape_output_0 for ONNX tensor: llama_model/Shape_output_0\n",
      "[X] llama_model/Shape [Shape] outputs: [llama_model/Shape_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant [Constant]\n",
      "[X] llama_model/Constant [Constant] inputs: \n",
      "[W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[X] llama_model/Constant [Constant] outputs: [llama_model/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather [Gather]\n",
      "[X] Searching for input: llama_model/Shape_output_0\n",
      "[X] Searching for input: llama_model/Constant_output_0\n",
      "[X] llama_model/Gather [Gather] inputs: [llama_model/Shape_output_0 -> (2)[INT32]], [llama_model/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_output_0 for ONNX node: llama_model/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather for ONNX node: llama_model/Gather\n",
      "[X] Registering tensor: llama_model/Gather_output_0 for ONNX tensor: llama_model/Gather_output_0\n",
      "[X] llama_model/Gather [Gather] outputs: [llama_model/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_1 [Shape]\n",
      "[X] Searching for input: input_ids\n",
      "[X] llama_model/Shape_1 [Shape] inputs: [input_ids -> (-1, -1)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape_1 for ONNX node: llama_model/Shape_1\n",
      "[X] Registering tensor: llama_model/Shape_1_output_0 for ONNX tensor: llama_model/Shape_1_output_0\n",
      "[X] llama_model/Shape_1 [Shape] outputs: [llama_model/Shape_1_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_1 [Constant]\n",
      "[X] llama_model/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/Constant_1 [Constant] outputs: [llama_model/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_1_output_0\n",
      "[X] llama_model/Gather_1 [Gather] inputs: [llama_model/Shape_1_output_0 -> (2)[INT32]], [llama_model/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_1_output_0 for ONNX node: llama_model/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather_1 for ONNX node: llama_model/Gather_1\n",
      "[X] Registering tensor: llama_model/Gather_1_output_0 for ONNX tensor: llama_model/Gather_1_output_0\n",
      "[X] llama_model/Gather_1 [Gather] outputs: [llama_model/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_2 [Constant]\n",
      "[X] llama_model/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/Constant_2 [Constant] outputs: [llama_model/Constant_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Cast [Cast]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] llama_model/Cast [Cast] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/Cast for ONNX node: llama_model/Cast\n",
      "[X] Registering tensor: llama_model/Cast_output_0 for ONNX tensor: llama_model/Cast_output_0\n",
      "[X] llama_model/Cast [Cast] outputs: [llama_model/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_3 [Constant]\n",
      "[X] llama_model/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/Constant_3 [Constant] outputs: [llama_model/Constant_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Range [Range]\n",
      "[X] Searching for input: llama_model/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/Cast_output_0\n",
      "[X] Searching for input: llama_model/Constant_3_output_0\n",
      "[X] llama_model/Range [Range] inputs: [llama_model/Constant_2_output_0 -> ()[INT32]], [llama_model/Cast_output_0 -> ()[INT32]], [llama_model/Constant_3_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Range for ONNX node: llama_model/Range\n",
      "[X] Registering tensor: llama_model/Range_output_0 for ONNX tensor: llama_model/Range_output_0\n",
      "[X] llama_model/Range [Range] outputs: [llama_model/Range_output_0 -> (-1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_4 [Constant]\n",
      "[X] llama_model/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/Constant_4 [Constant] outputs: [llama_model/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Range_output_0\n",
      "[X] Searching for input: llama_model/Constant_4_output_0\n",
      "[X] llama_model/Unsqueeze [Unsqueeze] inputs: [llama_model/Range_output_0 -> (-1)[INT32]], [llama_model/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_,), unsqueezing to: (_, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze for ONNX node: llama_model/Unsqueeze\n",
      "[X] Registering tensor: llama_model/Unsqueeze_output_0 for ONNX tensor: llama_model/Unsqueeze_output_0\n",
      "[X] llama_model/Unsqueeze [Unsqueeze] outputs: [llama_model/Unsqueeze_output_0 -> (1, -1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_5 [Constant]\n",
      "[X] llama_model/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/Constant_5 [Constant] outputs: [llama_model/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_29 [Constant]\n",
      "[X] Constant_29 [Constant] inputs: \n",
      "[X] Constant_29 [Constant] outputs: [onnx::Unsqueeze_93 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_93\n",
      "[X] llama_model/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_93 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_1 for ONNX node: llama_model/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/Unsqueeze_1_output_0 for ONNX tensor: llama_model/Unsqueeze_1_output_0\n",
      "[X] llama_model/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat [Concat]\n",
      "[X] Searching for input: llama_model/Constant_5_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_1_output_0\n",
      "[X] llama_model/Concat [Concat] inputs: [llama_model/Constant_5_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_5_output_0 for ONNX node: llama_model/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/Concat for ONNX node: llama_model/Concat\n",
      "[X] Registering tensor: llama_model/Concat_output_0 for ONNX tensor: llama_model/Concat_output_0\n",
      "[X] llama_model/Concat [Concat] outputs: [llama_model/Concat_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/Concat_output_0\n",
      "[X] llama_model/Reshape [Reshape] inputs: [llama_model/Unsqueeze_output_0 -> (1, -1)[INT32]], [llama_model/Concat_output_0 -> (2)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape for ONNX node: llama_model/Reshape\n",
      "[X] Registering tensor: llama_model/Reshape_output_0 for ONNX tensor: llama_model/Reshape_output_0\n",
      "[X] llama_model/Reshape [Reshape] outputs: [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Parsing node: llama_model/embed_tokens/Gather [Gather]\n",
      "[X] Searching for input: llama_model.embed_tokens.weight\n",
      "[X] Searching for input: input_ids\n",
      "[X] llama_model/embed_tokens/Gather [Gather] inputs: [llama_model.embed_tokens.weight -> (32000, 64)[FLOAT]], [input_ids -> (-1, -1)[INT32]], \n",
      "[X] Registering layer: llama_model.embed_tokens.weight for ONNX node: llama_model.embed_tokens.weight\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/embed_tokens/Gather for ONNX node: llama_model/embed_tokens/Gather\n",
      "[X] Registering tensor: llama_model/embed_tokens/Gather_output_0 for ONNX tensor: llama_model/embed_tokens/Gather_output_0\n",
      "[X] llama_model/embed_tokens/Gather [Gather] outputs: [llama_model/embed_tokens/Gather_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_34 [Constant]\n",
      "[X] Constant_34 [Constant] inputs: \n",
      "[X] Constant_34 [Constant] outputs: [onnx::Unsqueeze_98 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_98\n",
      "[X] llama_model/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_98 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_2 for ONNX node: llama_model/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/Unsqueeze_2_output_0 for ONNX tensor: llama_model/Unsqueeze_2_output_0\n",
      "[X] llama_model/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_36 [Constant]\n",
      "[X] Constant_36 [Constant] inputs: \n",
      "[X] Constant_36 [Constant] outputs: [onnx::Unsqueeze_100 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_100\n",
      "[X] llama_model/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_100 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_3 for ONNX node: llama_model/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/Unsqueeze_3_output_0 for ONNX tensor: llama_model/Unsqueeze_3_output_0\n",
      "[X] llama_model/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_3_output_0\n",
      "[X] llama_model/Concat_1 [Concat] inputs: [llama_model/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Concat_1 for ONNX node: llama_model/Concat_1\n",
      "[X] Registering tensor: llama_model/Concat_1_output_0 for ONNX tensor: llama_model/Concat_1_output_0\n",
      "[X] llama_model/Concat_1 [Concat] outputs: [llama_model/Concat_1_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Concat_1_output_0\n",
      "[X] llama_model/ConstantOfShape [ConstantOfShape] inputs: [llama_model/Concat_1_output_0 -> (2)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_output_0 for ONNX tensor: llama_model/ConstantOfShape_output_0\n",
      "[X] llama_model/ConstantOfShape [ConstantOfShape] outputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Parsing node: Constant_40 [Constant]\n",
      "[X] Constant_40 [Constant] inputs: \n",
      "[X] Constant_40 [Constant] outputs: [onnx::Unsqueeze_104 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_104\n",
      "[X] llama_model/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_104 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_4 for ONNX node: llama_model/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/Unsqueeze_4_output_0 for ONNX tensor: llama_model/Unsqueeze_4_output_0\n",
      "[X] llama_model/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_42 [Constant]\n",
      "[X] Constant_42 [Constant] inputs: \n",
      "[X] Constant_42 [Constant] outputs: [onnx::Unsqueeze_106 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_106\n",
      "[X] llama_model/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_106 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_5 for ONNX node: llama_model/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/Unsqueeze_5_output_0 for ONNX tensor: llama_model/Unsqueeze_5_output_0\n",
      "[X] llama_model/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_5_output_0\n",
      "[X] llama_model/Concat_2 [Concat] inputs: [llama_model/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Concat_2 for ONNX node: llama_model/Concat_2\n",
      "[X] Registering tensor: llama_model/Concat_2_output_0 for ONNX tensor: llama_model/Concat_2_output_0\n",
      "[X] llama_model/Concat_2 [Concat] outputs: [llama_model/Concat_2_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape_1 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Concat_2_output_0\n",
      "[X] llama_model/ConstantOfShape_1 [ConstantOfShape] inputs: [llama_model/Concat_2_output_0 -> (2)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_1_output_0 for ONNX tensor: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/ConstantOfShape_1 [ConstantOfShape] outputs: [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/Shape_2 [Shape] inputs: [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Shape_2 for ONNX node: llama_model/Shape_2\n",
      "[X] Registering tensor: llama_model/Shape_2_output_0 for ONNX tensor: llama_model/Shape_2_output_0\n",
      "[X] llama_model/Shape_2 [Shape] outputs: [llama_model/Shape_2_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_6 [Constant]\n",
      "[X] llama_model/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/Constant_6 [Constant] outputs: [llama_model/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_7 [Constant]\n",
      "[X] llama_model/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/Constant_7 [Constant] outputs: [llama_model/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_8 [Constant]\n",
      "[X] llama_model/Constant_8 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[W] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n",
      "[X] llama_model/Constant_8 [Constant] outputs: [llama_model/Constant_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Slice [Slice]\n",
      "[X] Searching for input: llama_model/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_7_output_0\n",
      "[X] Searching for input: llama_model/Constant_8_output_0\n",
      "[X] Searching for input: llama_model/Constant_6_output_0\n",
      "[X] llama_model/Slice [Slice] inputs: [llama_model/Shape_2_output_0 -> (2)[INT32]], [llama_model/Constant_7_output_0 -> (1)[INT32]], [llama_model/Constant_8_output_0 -> (1)[INT32]], [llama_model/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Slice for ONNX node: llama_model/Slice\n",
      "[X] Registering tensor: llama_model/Slice_output_0 for ONNX tensor: llama_model/Slice_output_0\n",
      "[X] llama_model/Slice [Slice] outputs: [llama_model/Slice_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_9 [Constant]\n",
      "[X] llama_model/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/Constant_9 [Constant] outputs: [llama_model/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Squeeze [Squeeze]\n",
      "[X] Searching for input: llama_model/Slice_output_0\n",
      "[X] Searching for input: llama_model/Constant_9_output_0\n",
      "[X] llama_model/Squeeze [Squeeze] inputs: [llama_model/Slice_output_0 -> (1)[INT32]], [llama_model/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1,), squeezing to: ()\n",
      "[X] Registering layer: llama_model/Squeeze for ONNX node: llama_model/Squeeze\n",
      "[X] Registering tensor: llama_model/Squeeze_output_0 for ONNX tensor: llama_model/Squeeze_output_0\n",
      "[X] llama_model/Squeeze [Squeeze] outputs: [llama_model/Squeeze_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/Squeeze_output_0\n",
      "[X] llama_model/Cast_1 [Cast] inputs: [llama_model/Squeeze_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/Cast_1 for ONNX node: llama_model/Cast_1\n",
      "[X] Registering tensor: llama_model/Cast_1_output_0 for ONNX tensor: llama_model/Cast_1_output_0\n",
      "[X] llama_model/Cast_1 [Cast] outputs: [llama_model/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_10 [Constant]\n",
      "[X] llama_model/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/Constant_10 [Constant] outputs: [llama_model/Constant_10_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_11 [Constant]\n",
      "[X] llama_model/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/Constant_11 [Constant] outputs: [llama_model/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Range_1 [Range]\n",
      "[X] Searching for input: llama_model/Constant_10_output_0\n",
      "[X] Searching for input: llama_model/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_11_output_0\n",
      "[X] llama_model/Range_1 [Range] inputs: [llama_model/Constant_10_output_0 -> ()[INT32]], [llama_model/Cast_1_output_0 -> ()[INT32]], [llama_model/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Range_1 for ONNX node: llama_model/Range_1\n",
      "[X] Registering tensor: llama_model/Range_1_output_0 for ONNX tensor: llama_model/Range_1_output_0\n",
      "[X] llama_model/Range_1 [Range] outputs: [llama_model/Range_1_output_0 -> (-1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_12 [Constant]\n",
      "[X] llama_model/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/Constant_12 [Constant] outputs: [llama_model/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Add [Add]\n",
      "[X] Searching for input: llama_model/Range_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_12_output_0\n",
      "[X] llama_model/Add [Add] inputs: [llama_model/Range_1_output_0 -> (-1)[INT32]], [llama_model/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_12_output_0 for ONNX node: llama_model/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/Add for ONNX node: llama_model/Add\n",
      "[X] Registering tensor: llama_model/Add_output_0 for ONNX tensor: llama_model/Add_output_0\n",
      "[X] llama_model/Add [Add] outputs: [llama_model/Add_output_0 -> (-1)[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/Shape_3 [Shape] inputs: [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Shape_3 for ONNX node: llama_model/Shape_3\n",
      "[X] Registering tensor: llama_model/Shape_3_output_0 for ONNX tensor: llama_model/Shape_3_output_0\n",
      "[X] llama_model/Shape_3 [Shape] outputs: [llama_model/Shape_3_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_13 [Constant]\n",
      "[X] llama_model/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/Constant_13 [Constant] outputs: [llama_model/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_14 [Constant]\n",
      "[X] llama_model/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/Constant_14 [Constant] outputs: [llama_model/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_15 [Constant]\n",
      "[X] llama_model/Constant_15 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/Constant_15 [Constant] outputs: [llama_model/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_14_output_0\n",
      "[X] Searching for input: llama_model/Constant_15_output_0\n",
      "[X] Searching for input: llama_model/Constant_13_output_0\n",
      "[X] llama_model/Slice_1 [Slice] inputs: [llama_model/Shape_3_output_0 -> (2)[INT32]], [llama_model/Constant_14_output_0 -> (1)[INT32]], [llama_model/Constant_15_output_0 -> (1)[INT32]], [llama_model/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Slice_1 for ONNX node: llama_model/Slice_1\n",
      "[X] Registering tensor: llama_model/Slice_1_output_0 for ONNX tensor: llama_model/Slice_1_output_0\n",
      "[X] llama_model/Slice_1 [Slice] outputs: [llama_model/Slice_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_16 [Constant]\n",
      "[X] llama_model/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/Constant_16 [Constant] outputs: [llama_model/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Squeeze_1 [Squeeze]\n",
      "[X] Searching for input: llama_model/Slice_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_16_output_0\n",
      "[X] llama_model/Squeeze_1 [Squeeze] inputs: [llama_model/Slice_1_output_0 -> (1)[INT32]], [llama_model/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1,), squeezing to: ()\n",
      "[X] Registering layer: llama_model/Squeeze_1 for ONNX node: llama_model/Squeeze_1\n",
      "[X] Registering tensor: llama_model/Squeeze_1_output_0 for ONNX tensor: llama_model/Squeeze_1_output_0\n",
      "[X] llama_model/Squeeze_1 [Squeeze] outputs: [llama_model/Squeeze_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_66 [Constant]\n",
      "[X] Constant_66 [Constant] inputs: \n",
      "[X] Constant_66 [Constant] outputs: [onnx::Unsqueeze_130 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Squeeze_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_130\n",
      "[X] llama_model/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/Squeeze_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_130 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_6 for ONNX node: llama_model/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/Unsqueeze_6_output_0 for ONNX tensor: llama_model/Unsqueeze_6_output_0\n",
      "[X] llama_model/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_17 [Constant]\n",
      "[X] llama_model/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/Constant_17 [Constant] outputs: [llama_model/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/Constant_17_output_0\n",
      "[X] llama_model/Concat_3 [Concat] inputs: [llama_model/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_17_output_0 for ONNX node: llama_model/Constant_17_output_0\n",
      "[X] Registering layer: llama_model/Concat_3 for ONNX node: llama_model/Concat_3\n",
      "[X] Registering tensor: llama_model/Concat_3_output_0 for ONNX tensor: llama_model/Concat_3_output_0\n",
      "[X] llama_model/Concat_3 [Concat] outputs: [llama_model/Concat_3_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/Add_output_0\n",
      "[X] Searching for input: llama_model/Concat_3_output_0\n",
      "[X] llama_model/Reshape_1 [Reshape] inputs: [llama_model/Add_output_0 -> (-1)[INT32]], [llama_model/Concat_3_output_0 -> (2)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape_1 for ONNX node: llama_model/Reshape_1\n",
      "[X] Registering tensor: llama_model/Reshape_1_output_0 for ONNX tensor: llama_model/Reshape_1_output_0\n",
      "[X] llama_model/Reshape_1 [Reshape] outputs: [llama_model/Reshape_1_output_0 -> (-1, 1)[INT32]], \n",
      "[X] Parsing node: llama_model/Less [Less]\n",
      "[X] Searching for input: llama_model/Range_1_output_0\n",
      "[X] Searching for input: llama_model/Reshape_1_output_0\n",
      "[X] llama_model/Less [Less] inputs: [llama_model/Range_1_output_0 -> (-1)[INT32]], [llama_model/Reshape_1_output_0 -> (-1, 1)[INT32]], \n",
      "[X] Registering layer: llama_model/Less for ONNX node: llama_model/Less\n",
      "[X] Registering tensor: llama_model/Less_output_0 for ONNX tensor: llama_model/Less_output_0\n",
      "[X] llama_model/Less [Less] outputs: [llama_model/Less_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/Less_output_0\n",
      "[X] llama_model/Cast_2 [Cast] inputs: [llama_model/Less_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Casting to type: bool\n",
      "[X] Registering layer: llama_model/Cast_2 for ONNX node: llama_model/Cast_2\n",
      "[X] Registering tensor: llama_model/Cast_2_output_0 for ONNX tensor: llama_model/Cast_2_output_0\n",
      "[X] llama_model/Cast_2 [Cast] outputs: [llama_model/Cast_2_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_18 [Constant]\n",
      "[X] llama_model/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/Constant_18 [Constant] outputs: [llama_model/Constant_18_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/Where [Where]\n",
      "[X] Searching for input: llama_model/Cast_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_18_output_0\n",
      "[X] Searching for input: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/Where [Where] inputs: [llama_model/Cast_2_output_0 -> (-1, -1)[BOOL]], [llama_model/Constant_18_output_0 -> ()[FLOAT]], [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Constant_18_output_0 for ONNX node: llama_model/Constant_18_output_0\n",
      "[X] Registering layer: llama_model/Where for ONNX node: llama_model/Where\n",
      "[X] Registering tensor: llama_model/Where_output_0 for ONNX tensor: llama_model/Where_output_0\n",
      "[X] llama_model/Where [Where] outputs: [llama_model/Where_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/Where_output_0\n",
      "[X] llama_model/Cast_3 [Cast] inputs: [llama_model/Where_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/Cast_3 for ONNX node: llama_model/Cast_3\n",
      "[X] Registering tensor: llama_model/Cast_3_output_0 for ONNX tensor: llama_model/Cast_3_output_0\n",
      "[X] llama_model/Cast_3 [Cast] outputs: [llama_model/Cast_3_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_19 [Constant]\n",
      "[X] llama_model/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/Constant_19 [Constant] outputs: [llama_model/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_19_output_0\n",
      "[X] llama_model/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/Cast_3_output_0 -> (-1, -1)[FLOAT]], [llama_model/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_, _), unsqueezing to: (_, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_7 for ONNX node: llama_model/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/Unsqueeze_7_output_0 for ONNX tensor: llama_model/Unsqueeze_7_output_0\n",
      "[X] llama_model/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/Unsqueeze_7_output_0 -> (1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_20 [Constant]\n",
      "[X] llama_model/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/Constant_20 [Constant] outputs: [llama_model/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/Constant_20_output_0\n",
      "[X] llama_model/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/Unsqueeze_7_output_0 -> (1, -1, -1)[FLOAT]], [llama_model/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, _), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_8 for ONNX node: llama_model/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/Unsqueeze_8_output_0 for ONNX tensor: llama_model/Unsqueeze_8_output_0\n",
      "[X] llama_model/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/Unsqueeze_8_output_0 -> (1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_21 [Constant]\n",
      "[X] llama_model/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/Constant_21 [Constant] outputs: [llama_model/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_output_0\n",
      "[X] Searching for input: llama_model/Constant_21_output_0\n",
      "[X] llama_model/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/Gather_output_0 -> ()[INT32]], [llama_model/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_9 for ONNX node: llama_model/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/Unsqueeze_9_output_0 for ONNX tensor: llama_model/Unsqueeze_9_output_0\n",
      "[X] llama_model/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_22 [Constant]\n",
      "[X] llama_model/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/Constant_22 [Constant] outputs: [llama_model/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_23 [Constant]\n",
      "[X] llama_model/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/Constant_23 [Constant] outputs: [llama_model/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_23_output_0\n",
      "[X] llama_model/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [llama_model/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_10 for ONNX node: llama_model/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/Unsqueeze_10_output_0 for ONNX tensor: llama_model/Unsqueeze_10_output_0\n",
      "[X] llama_model/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_24 [Constant]\n",
      "[X] llama_model/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/Constant_24 [Constant] outputs: [llama_model/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_24_output_0\n",
      "[X] llama_model/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [llama_model/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_11 for ONNX node: llama_model/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/Unsqueeze_11_output_0 for ONNX tensor: llama_model/Unsqueeze_11_output_0\n",
      "[X] llama_model/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/Constant_22_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_11_output_0\n",
      "[X] llama_model/Concat_4 [Concat] inputs: [llama_model/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/Constant_22_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_22_output_0 for ONNX node: llama_model/Constant_22_output_0\n",
      "[X] Registering layer: llama_model/Concat_4 for ONNX node: llama_model/Concat_4\n",
      "[X] Registering tensor: llama_model/Concat_4_output_0 for ONNX tensor: llama_model/Concat_4_output_0\n",
      "[X] llama_model/Concat_4 [Concat] outputs: [llama_model/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_25 [Constant]\n",
      "[X] llama_model/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/Constant_25 [Constant] outputs: [llama_model/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/Constant_25_output_0\n",
      "[X] llama_model/Reshape_2 [Reshape] inputs: [llama_model/Concat_4_output_0 -> (4)[INT32]], [llama_model/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape_2 for ONNX node: llama_model/Reshape_2\n",
      "[X] Registering tensor: llama_model/Reshape_2_output_0 for ONNX tensor: llama_model/Reshape_2_output_0\n",
      "[X] llama_model/Reshape_2 [Reshape] outputs: [llama_model/Reshape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/Reshape_2_output_0\n",
      "[X] llama_model/Shape_4 [Shape] inputs: [llama_model/Reshape_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape_4 for ONNX node: llama_model/Shape_4\n",
      "[X] Registering tensor: llama_model/Shape_4_output_0 for ONNX tensor: llama_model/Shape_4_output_0\n",
      "[X] llama_model/Shape_4 [Shape] outputs: [llama_model/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape_2 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Shape_4_output_0\n",
      "[X] llama_model/ConstantOfShape_2 [ConstantOfShape] inputs: [llama_model/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_2_output_0 for ONNX tensor: llama_model/ConstantOfShape_2_output_0\n",
      "[X] llama_model/ConstantOfShape_2 [ConstantOfShape] outputs: [llama_model/ConstantOfShape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_26 [Constant]\n",
      "[X] llama_model/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/Constant_26 [Constant] outputs: [llama_model/Constant_26_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Mul [Mul]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_26_output_0\n",
      "[X] llama_model/Mul [Mul] inputs: [llama_model/ConstantOfShape_2_output_0 -> (4)[INT32]], [llama_model/Constant_26_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_26_output_0 for ONNX node: llama_model/Constant_26_output_0\n",
      "[X] Registering layer: llama_model/Mul for ONNX node: llama_model/Mul\n",
      "[X] Registering tensor: llama_model/Mul_output_0 for ONNX tensor: llama_model/Mul_output_0\n",
      "[X] llama_model/Mul [Mul] outputs: [llama_model/Mul_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Equal [Equal]\n",
      "[X] Searching for input: llama_model/Reshape_2_output_0\n",
      "[X] Searching for input: llama_model/Mul_output_0\n",
      "[X] llama_model/Equal [Equal] inputs: [llama_model/Reshape_2_output_0 -> (4)[INT32]], [llama_model/Mul_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Equal for ONNX node: llama_model/Equal\n",
      "[X] Registering tensor: llama_model/Equal_output_0 for ONNX tensor: llama_model/Equal_output_0\n",
      "[X] llama_model/Equal [Equal] outputs: [llama_model/Equal_output_0 -> (4)[BOOL]], \n",
      "[X] Parsing node: llama_model/Where_1 [Where]\n",
      "[X] Searching for input: llama_model/Equal_output_0\n",
      "[X] Searching for input: llama_model/ConstantOfShape_2_output_0\n",
      "[X] Searching for input: llama_model/Reshape_2_output_0\n",
      "[X] llama_model/Where_1 [Where] inputs: [llama_model/Equal_output_0 -> (4)[BOOL]], [llama_model/ConstantOfShape_2_output_0 -> (4)[INT32]], [llama_model/Reshape_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Where_1 for ONNX node: llama_model/Where_1\n",
      "[X] Registering tensor: llama_model/Where_1_output_0 for ONNX tensor: llama_model/Where_1_output_0\n",
      "[X] llama_model/Where_1 [Where] outputs: [llama_model/Where_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Expand [Expand]\n",
      "[X] Searching for input: llama_model/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/Where_1_output_0\n",
      "[X] llama_model/Expand [Expand] inputs: [llama_model/Unsqueeze_8_output_0 -> (1, 1, -1, -1)[FLOAT]], [llama_model/Where_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Expand for ONNX node: llama_model/Expand\n",
      "[X] Registering tensor: llama_model/Expand_output_0 for ONNX tensor: llama_model/Expand_output_0\n",
      "[X] llama_model/Expand [Expand] outputs: [llama_model/Expand_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Shape_5 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_output_0\n",
      "[X] llama_model/Shape_5 [Shape] inputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Registering layer: llama_model/Shape_5 for ONNX node: llama_model/Shape_5\n",
      "[X] Registering tensor: llama_model/Shape_5_output_0 for ONNX tensor: llama_model/Shape_5_output_0\n",
      "[X] llama_model/Shape_5 [Shape] outputs: [llama_model/Shape_5_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_27 [Constant]\n",
      "[X] llama_model/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/Constant_27 [Constant] outputs: [llama_model/Constant_27_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/Shape_5_output_0\n",
      "[X] Searching for input: llama_model/Constant_27_output_0\n",
      "[X] llama_model/Gather_2 [Gather] inputs: [llama_model/Shape_5_output_0 -> (2)[INT32]], [llama_model/Constant_27_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_27_output_0 for ONNX node: llama_model/Constant_27_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather_2 for ONNX node: llama_model/Gather_2\n",
      "[X] Registering tensor: llama_model/Gather_2_output_0 for ONNX tensor: llama_model/Gather_2_output_0\n",
      "[X] llama_model/Gather_2 [Gather] outputs: [llama_model/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_6 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_output_0\n",
      "[X] llama_model/Shape_6 [Shape] inputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Registering layer: llama_model/Shape_6 for ONNX node: llama_model/Shape_6\n",
      "[X] Registering tensor: llama_model/Shape_6_output_0 for ONNX tensor: llama_model/Shape_6_output_0\n",
      "[X] llama_model/Shape_6 [Shape] outputs: [llama_model/Shape_6_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_28 [Constant]\n",
      "[X] llama_model/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/Constant_28 [Constant] outputs: [llama_model/Constant_28_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/Shape_6_output_0\n",
      "[X] Searching for input: llama_model/Constant_28_output_0\n",
      "[X] llama_model/Gather_3 [Gather] inputs: [llama_model/Shape_6_output_0 -> (2)[INT32]], [llama_model/Constant_28_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_28_output_0 for ONNX node: llama_model/Constant_28_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather_3 for ONNX node: llama_model/Gather_3\n",
      "[X] Registering tensor: llama_model/Gather_3_output_0 for ONNX tensor: llama_model/Gather_3_output_0\n",
      "[X] llama_model/Gather_3 [Gather] outputs: [llama_model/Gather_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_29 [Constant]\n",
      "[X] llama_model/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/Constant_29 [Constant] outputs: [llama_model/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_output_0\n",
      "[X] Searching for input: llama_model/Constant_29_output_0\n",
      "[X] llama_model/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], [llama_model/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_, _), unsqueezing to: (_, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_12 for ONNX node: llama_model/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/Unsqueeze_12_output_0 for ONNX tensor: llama_model/Unsqueeze_12_output_0\n",
      "[X] llama_model/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/Unsqueeze_12_output_0 -> (-1, 1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_30 [Constant]\n",
      "[X] llama_model/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/Constant_30 [Constant] outputs: [llama_model/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/Constant_30_output_0\n",
      "[X] llama_model/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/Unsqueeze_12_output_0 -> (-1, 1, -1)[BOOL]], [llama_model/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_, 1, _), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_13 for ONNX node: llama_model/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/Unsqueeze_13_output_0 for ONNX tensor: llama_model/Unsqueeze_13_output_0\n",
      "[X] llama_model/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/Unsqueeze_13_output_0 -> (-1, 1, 1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_31 [Constant]\n",
      "[X] llama_model/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/Constant_31 [Constant] outputs: [llama_model/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_14 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_31_output_0\n",
      "[X] llama_model/Unsqueeze_14 [Unsqueeze] inputs: [llama_model/Gather_2_output_0 -> ()[INT32]], [llama_model/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_14 for ONNX node: llama_model/Unsqueeze_14\n",
      "[X] Registering tensor: llama_model/Unsqueeze_14_output_0 for ONNX tensor: llama_model/Unsqueeze_14_output_0\n",
      "[X] llama_model/Unsqueeze_14 [Unsqueeze] outputs: [llama_model/Unsqueeze_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_32 [Constant]\n",
      "[X] llama_model/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/Constant_32 [Constant] outputs: [llama_model/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_33 [Constant]\n",
      "[X] llama_model/Constant_33 [Constant] inputs: \n",
      "[X] llama_model/Constant_33 [Constant] outputs: [llama_model/Constant_33_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_15 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_33_output_0\n",
      "[X] llama_model/Unsqueeze_15 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [llama_model/Constant_33_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_15 for ONNX node: llama_model/Unsqueeze_15\n",
      "[X] Registering tensor: llama_model/Unsqueeze_15_output_0 for ONNX tensor: llama_model/Unsqueeze_15_output_0\n",
      "[X] llama_model/Unsqueeze_15 [Unsqueeze] outputs: [llama_model/Unsqueeze_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_34 [Constant]\n",
      "[X] llama_model/Constant_34 [Constant] inputs: \n",
      "[X] llama_model/Constant_34 [Constant] outputs: [llama_model/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_16 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_34_output_0\n",
      "[X] llama_model/Unsqueeze_16 [Unsqueeze] inputs: [llama_model/Gather_3_output_0 -> ()[INT32]], [llama_model/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_16 for ONNX node: llama_model/Unsqueeze_16\n",
      "[X] Registering tensor: llama_model/Unsqueeze_16_output_0 for ONNX tensor: llama_model/Unsqueeze_16_output_0\n",
      "[X] llama_model/Unsqueeze_16 [Unsqueeze] outputs: [llama_model/Unsqueeze_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_14_output_0\n",
      "[X] Searching for input: llama_model/Constant_32_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_15_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_16_output_0\n",
      "[X] llama_model/Concat_5 [Concat] inputs: [llama_model/Unsqueeze_14_output_0 -> (1)[INT32]], [llama_model/Constant_32_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_15_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_16_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_32_output_0 for ONNX node: llama_model/Constant_32_output_0\n",
      "[X] Registering layer: llama_model/Concat_5 for ONNX node: llama_model/Concat_5\n",
      "[X] Registering tensor: llama_model/Concat_5_output_0 for ONNX tensor: llama_model/Concat_5_output_0\n",
      "[X] llama_model/Concat_5 [Concat] outputs: [llama_model/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_35 [Constant]\n",
      "[X] llama_model/Constant_35 [Constant] inputs: \n",
      "[X] llama_model/Constant_35 [Constant] outputs: [llama_model/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/Concat_5_output_0\n",
      "[X] Searching for input: llama_model/Constant_35_output_0\n",
      "[X] llama_model/Reshape_3 [Reshape] inputs: [llama_model/Concat_5_output_0 -> (4)[INT32]], [llama_model/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape_3 for ONNX node: llama_model/Reshape_3\n",
      "[X] Registering tensor: llama_model/Reshape_3_output_0 for ONNX tensor: llama_model/Reshape_3_output_0\n",
      "[X] llama_model/Reshape_3 [Reshape] outputs: [llama_model/Reshape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_7 [Shape]\n",
      "[X] Searching for input: llama_model/Reshape_3_output_0\n",
      "[X] llama_model/Shape_7 [Shape] inputs: [llama_model/Reshape_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape_7 for ONNX node: llama_model/Shape_7\n",
      "[X] Registering tensor: llama_model/Shape_7_output_0 for ONNX tensor: llama_model/Shape_7_output_0\n",
      "[X] llama_model/Shape_7 [Shape] outputs: [llama_model/Shape_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape_3 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Shape_7_output_0\n",
      "[X] llama_model/ConstantOfShape_3 [ConstantOfShape] inputs: [llama_model/Shape_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_3_output_0 for ONNX tensor: llama_model/ConstantOfShape_3_output_0\n",
      "[X] llama_model/ConstantOfShape_3 [ConstantOfShape] outputs: [llama_model/ConstantOfShape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_36 [Constant]\n",
      "[X] llama_model/Constant_36 [Constant] inputs: \n",
      "[X] llama_model/Constant_36 [Constant] outputs: [llama_model/Constant_36_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_36_output_0\n",
      "[X] llama_model/Mul_1 [Mul] inputs: [llama_model/ConstantOfShape_3_output_0 -> (4)[INT32]], [llama_model/Constant_36_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_36_output_0 for ONNX node: llama_model/Constant_36_output_0\n",
      "[X] Registering layer: llama_model/Mul_1 for ONNX node: llama_model/Mul_1\n",
      "[X] Registering tensor: llama_model/Mul_1_output_0 for ONNX tensor: llama_model/Mul_1_output_0\n",
      "[X] llama_model/Mul_1 [Mul] outputs: [llama_model/Mul_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Equal_1 [Equal]\n",
      "[X] Searching for input: llama_model/Reshape_3_output_0\n",
      "[X] Searching for input: llama_model/Mul_1_output_0\n",
      "[X] llama_model/Equal_1 [Equal] inputs: [llama_model/Reshape_3_output_0 -> (4)[INT32]], [llama_model/Mul_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Equal_1 for ONNX node: llama_model/Equal_1\n",
      "[X] Registering tensor: llama_model/Equal_1_output_0 for ONNX tensor: llama_model/Equal_1_output_0\n",
      "[X] llama_model/Equal_1 [Equal] outputs: [llama_model/Equal_1_output_0 -> (4)[BOOL]], \n",
      "[X] Parsing node: llama_model/Where_2 [Where]\n",
      "[X] Searching for input: llama_model/Equal_1_output_0\n",
      "[X] Searching for input: llama_model/ConstantOfShape_3_output_0\n",
      "[X] Searching for input: llama_model/Reshape_3_output_0\n",
      "[X] llama_model/Where_2 [Where] inputs: [llama_model/Equal_1_output_0 -> (4)[BOOL]], [llama_model/ConstantOfShape_3_output_0 -> (4)[INT32]], [llama_model/Reshape_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Where_2 for ONNX node: llama_model/Where_2\n",
      "[X] Registering tensor: llama_model/Where_2_output_0 for ONNX tensor: llama_model/Where_2_output_0\n",
      "[X] llama_model/Where_2 [Where] outputs: [llama_model/Where_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Expand_1 [Expand]\n",
      "[X] Searching for input: llama_model/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/Where_2_output_0\n",
      "[X] llama_model/Expand_1 [Expand] inputs: [llama_model/Unsqueeze_13_output_0 -> (-1, 1, 1, -1)[BOOL]], [llama_model/Where_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Expand_1 for ONNX node: llama_model/Expand_1\n",
      "[X] Registering tensor: llama_model/Expand_1_output_0 for ONNX tensor: llama_model/Expand_1_output_0\n",
      "[X] llama_model/Expand_1 [Expand] outputs: [llama_model/Expand_1_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/Expand_1_output_0\n",
      "[X] llama_model/Cast_4 [Cast] inputs: [llama_model/Expand_1_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/Cast_4 for ONNX node: llama_model/Cast_4\n",
      "[X] Registering tensor: llama_model/Cast_4_output_0 for ONNX tensor: llama_model/Cast_4_output_0\n",
      "[X] llama_model/Cast_4 [Cast] outputs: [llama_model/Cast_4_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_37 [Constant]\n",
      "[X] llama_model/Constant_37 [Constant] inputs: \n",
      "[X] llama_model/Constant_37 [Constant] outputs: [llama_model/Constant_37_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/Sub [Sub]\n",
      "[X] Searching for input: llama_model/Constant_37_output_0\n",
      "[X] Searching for input: llama_model/Cast_4_output_0\n",
      "[X] llama_model/Sub [Sub] inputs: [llama_model/Constant_37_output_0 -> ()[FLOAT]], [llama_model/Cast_4_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Constant_37_output_0 for ONNX node: llama_model/Constant_37_output_0\n",
      "[X] Registering layer: llama_model/Sub for ONNX node: llama_model/Sub\n",
      "[X] Registering tensor: llama_model/Sub_output_0 for ONNX tensor: llama_model/Sub_output_0\n",
      "[X] llama_model/Sub [Sub] outputs: [llama_model/Sub_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/Sub_output_0\n",
      "[X] llama_model/Cast_5 [Cast] inputs: [llama_model/Sub_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: bool\n",
      "[X] Registering layer: llama_model/Cast_5 for ONNX node: llama_model/Cast_5\n",
      "[X] Registering tensor: llama_model/Cast_5_output_0 for ONNX tensor: llama_model/Cast_5_output_0\n",
      "[X] llama_model/Cast_5 [Cast] outputs: [llama_model/Cast_5_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Cast_6 [Cast]\n",
      "[X] Searching for input: llama_model/Cast_5_output_0\n",
      "[X] llama_model/Cast_6 [Cast] inputs: [llama_model/Cast_5_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Casting to type: bool\n",
      "[X] Registering layer: llama_model/Cast_6 for ONNX node: llama_model/Cast_6\n",
      "[X] Registering tensor: llama_model/Cast_6_output_0 for ONNX tensor: llama_model/Cast_6_output_0\n",
      "[X] llama_model/Cast_6 [Cast] outputs: [llama_model/Cast_6_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_38 [Constant]\n",
      "[X] llama_model/Constant_38 [Constant] inputs: \n",
      "[X] llama_model/Constant_38 [Constant] outputs: [llama_model/Constant_38_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/Where_3 [Where]\n",
      "[X] Searching for input: llama_model/Cast_6_output_0\n",
      "[X] Searching for input: llama_model/Constant_38_output_0\n",
      "[X] Searching for input: llama_model/Sub_output_0\n",
      "[X] llama_model/Where_3 [Where] inputs: [llama_model/Cast_6_output_0 -> (-1, 1, -1, -1)[BOOL]], [llama_model/Constant_38_output_0 -> ()[FLOAT]], [llama_model/Sub_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Constant_38_output_0 for ONNX node: llama_model/Constant_38_output_0\n",
      "[X] Registering layer: llama_model/Where_3 for ONNX node: llama_model/Where_3\n",
      "[X] Registering tensor: llama_model/Where_3_output_0 for ONNX tensor: llama_model/Where_3_output_0\n",
      "[X] llama_model/Where_3 [Where] outputs: [llama_model/Where_3_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Cast_7 [Cast]\n",
      "[X] Searching for input: llama_model/Where_3_output_0\n",
      "[X] llama_model/Cast_7 [Cast] inputs: [llama_model/Where_3_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/Cast_7 for ONNX node: llama_model/Cast_7\n",
      "[X] Registering tensor: llama_model/Cast_7_output_0 for ONNX tensor: llama_model/Cast_7_output_0\n",
      "[X] llama_model/Cast_7 [Cast] outputs: [llama_model/Cast_7_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/Cast_7_output_0\n",
      "[X] Searching for input: llama_model/Expand_output_0\n",
      "[X] llama_model/Add_1 [Add] inputs: [llama_model/Cast_7_output_0 -> (-1, 1, -1, -1)[FLOAT]], [llama_model/Expand_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Add_1 for ONNX node: llama_model/Add_1\n",
      "[X] Registering tensor: llama_model/Add_1_output_0 for ONNX tensor: llama_model/Add_1_output_0\n",
      "[X] llama_model/Add_1 [Add] outputs: [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/embed_tokens/Gather_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast [Cast] inputs: [llama_model/embed_tokens/Gather_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Cast for ONNX node: llama_model/layers.0/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast [Cast] outputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.0/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/input_layernorm/Constant [Constant] outputs: [llama_model/layers.0/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Pow [Pow] inputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.0/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Pow for ONNX node: llama_model/layers.0/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Pow [Pow] outputs: [llama_model/layers.0/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.0/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/ReduceMean for ONNX node: llama_model/layers.0/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.0/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.0/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Add [Add] inputs: [llama_model/layers.0/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.0/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.0/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Add for ONNX node: llama_model/layers.0/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Add [Add] outputs: [llama_model/layers.0/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.0/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Sqrt for ONNX node: llama_model/layers.0/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.0/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.0/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Div [Div] inputs: [llama_model/layers.0/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.0/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.0/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Div for ONNX node: llama_model/layers.0/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Div [Div] outputs: [llama_model/layers.0/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul [Mul] inputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Mul for ONNX node: llama_model/layers.0/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul [Mul] outputs: [llama_model/layers.0/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.0/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Cast_1 for ONNX node: llama_model/layers.0/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.0/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.0/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Mul_1 for ONNX node: llama_model/layers.0/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape [Shape] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape for ONNX node: llama_model/layers.0/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape [Shape] outputs: [llama_model/layers.0/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant [Constant] outputs: [llama_model/layers.0/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather [Gather] inputs: [llama_model/layers.0/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.0/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather for ONNX node: llama_model/layers.0/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather [Gather] outputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_1 for ONNX node: llama_model/layers.0/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.0/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_1 for ONNX node: llama_model/layers.0/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1689\n",
      "[X] llama_model/layers.0/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1689 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1689 for ONNX node: onnx::MatMul_1689\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1690\n",
      "[X] llama_model/layers.0/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1690 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1690 for ONNX node: onnx::MatMul_1690\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1691\n",
      "[X] llama_model/layers.0/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1691 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1691 for ONNX node: onnx::MatMul_1691\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_154 [Constant]\n",
      "[X] Constant_154 [Constant] inputs: \n",
      "[X] Constant_154 [Constant] outputs: [onnx::Unsqueeze_227 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_227\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_227 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze for ONNX node: llama_model/layers.0/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_156 [Constant]\n",
      "[X] Constant_156 [Constant] inputs: \n",
      "[X] Constant_156 [Constant] outputs: [onnx::Unsqueeze_229 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_229\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_229 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat for ONNX node: llama_model/layers.0/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat [Concat] outputs: [llama_model/layers.0/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_161 [Constant]\n",
      "[X] Constant_161 [Constant] inputs: \n",
      "[X] Constant_161 [Constant] outputs: [onnx::Unsqueeze_236 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_236\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_236 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_163 [Constant]\n",
      "[X] Constant_163 [Constant] inputs: \n",
      "[X] Constant_163 [Constant] outputs: [onnx::Unsqueeze_238 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_238\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_238 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_1 for ONNX node: llama_model/layers.0/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_168 [Constant]\n",
      "[X] Constant_168 [Constant] inputs: \n",
      "[X] Constant_168 [Constant] outputs: [onnx::Unsqueeze_245 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_245\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_245 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_170 [Constant]\n",
      "[X] Constant_170 [Constant] inputs: \n",
      "[X] Constant_170 [Constant] outputs: [onnx::Unsqueeze_247 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_247\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_247 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_2 for ONNX node: llama_model/layers.0/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape [Reshape] inputs: [llama_model/layers.0/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape for ONNX node: llama_model/layers.0/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose [Transpose] inputs: [llama_model/layers.0/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose for ONNX node: llama_model/layers.0/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.0/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape_1 for ONNX node: llama_model/layers.0/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.0/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_1 for ONNX node: llama_model/layers.0/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.0/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape_2 for ONNX node: llama_model/layers.0/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.0/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_2 for ONNX node: llama_model/layers.0/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_2 for ONNX node: llama_model/layers.0/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.0/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_2 for ONNX node: llama_model/layers.0/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (128, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.0/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (128, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_8 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_8 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_9 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_9 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_8_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_9_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice_1 [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.0/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_8_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_3 for ONNX node: llama_model/layers.0/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_4 for ONNX node: llama_model/layers.0/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul [Mul] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul for ONNX node: llama_model/layers.0/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul [Mul] outputs: [llama_model/layers.0/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_3 for ONNX node: llama_model/layers.0/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_11_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.0/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_11_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_5 for ONNX node: llama_model/layers.0/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div [Div] inputs: [llama_model/layers.0/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Div for ONNX node: llama_model/layers.0/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Div_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div [Div] outputs: [llama_model/layers.0/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Div_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast [Cast] inputs: [llama_model/layers.0/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast for ONNX node: llama_model/layers.0/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast [Cast] outputs: [llama_model/layers.0/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.0/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_1 for ONNX node: llama_model/layers.0/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_15_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_16_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_15_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice for ONNX node: llama_model/layers.0/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice [Slice] outputs: [llama_model/layers.0/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_17_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.0/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_18_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_18_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice_1 for ONNX node: llama_model/layers.0/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.0/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg [Neg] inputs: [llama_model/layers.0/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Neg for ONNX node: llama_model/layers.0/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg [Neg] outputs: [llama_model/layers.0/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.0/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_3 for ONNX node: llama_model/layers.0/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.0/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul_1 for ONNX node: llama_model/layers.0/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.0/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add [Add] inputs: [llama_model/layers.0/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Add for ONNX node: llama_model/layers.0/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Add_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add [Add] outputs: [llama_model/layers.0/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul_2 for ONNX node: llama_model/layers.0/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.0/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_4 for ONNX node: llama_model/layers.0/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.0/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_21_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_6 for ONNX node: llama_model/layers.0/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_1 [Div] inputs: [llama_model/layers.0/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_22_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Div_1 for ONNX node: llama_model/layers.0/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_1 [Div] outputs: [llama_model/layers.0/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.0/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_2 for ONNX node: llama_model/layers.0/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.0/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_3 for ONNX node: llama_model/layers.0/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_25_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_26_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_25_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice_2 for ONNX node: llama_model/layers.0/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.0/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.0/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_28_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_28_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice_3 for ONNX node: llama_model/layers.0/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.0/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.0/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Neg_1 for ONNX node: llama_model/layers.0/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.0/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.0/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_4 for ONNX node: llama_model/layers.0/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.0/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul_3 for ONNX node: llama_model/layers.0/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.0/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_1 [Add] inputs: [llama_model/layers.0/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Add_1 for ONNX node: llama_model/layers.0/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_1 [Add] outputs: [llama_model/layers.0/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.0/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_3 for ONNX node: llama_model/layers.0/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul [MatMul] inputs: [llama_model/layers.0/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/MatMul for ONNX node: llama_model/layers.0/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_2 [Div] inputs: [llama_model/layers.0/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.0/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_31_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_31_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Div_2 for ONNX node: llama_model/layers.0/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_2 [Div] outputs: [llama_model/layers.0/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_2 [Add] inputs: [llama_model/layers.0/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Add_2 for ONNX node: llama_model/layers.0/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_2 [Add] outputs: [llama_model/layers.0/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Softmax [Softmax] inputs: [llama_model/layers.0/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Softmax for ONNX node: llama_model/layers.0/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.0/self_attn/Softmax [Softmax] outputs: [llama_model/layers.0/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.0/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_4 for ONNX node: llama_model/layers.0/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.0/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_5 for ONNX node: llama_model/layers.0/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.0/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.0/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/MatMul_1 for ONNX node: llama_model/layers.0/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.0/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.0/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_4 for ONNX node: llama_model/layers.0/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_264 [Constant]\n",
      "[X] Constant_264 [Constant] inputs: \n",
      "[X] Constant_264 [Constant] outputs: [onnx::Unsqueeze_356 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_356\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_356 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_266 [Constant]\n",
      "[X] Constant_266 [Constant] inputs: \n",
      "[X] Constant_266 [Constant] outputs: [onnx::Unsqueeze_358 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_358\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_358 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_32_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_5 for ONNX node: llama_model/layers.0/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.0/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.0/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape_3 for ONNX node: llama_model/layers.0/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_1711\n",
      "[X] llama_model/layers.0/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.0/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1711 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1711 for ONNX node: onnx::MatMul_1711\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/Add [Add] inputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/Add for ONNX node: llama_model/layers.0/Add\n",
      "[X] Registering tensor: llama_model/layers.0/Add_output_0 for ONNX tensor: llama_model/layers.0/Add_output_0\n",
      "[X] llama_model/layers.0/Add [Add] outputs: [llama_model/layers.0/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/Add_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.0/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Cast for ONNX node: llama_model/layers.0/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.0/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.0/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Pow for ONNX node: llama_model/layers.0/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.0/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.0/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.0/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.0/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Add for ONNX node: llama_model/layers.0/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.0/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.0/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.0/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.0/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.0/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Div for ONNX node: llama_model/layers.0/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.0/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Mul for ONNX node: llama_model/layers.0/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.0/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.0/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.0/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.0/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.0.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.0.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.0/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1712\n",
      "[X] llama_model/layers.0/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1712 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1712 for ONNX node: onnx::MatMul_1712\n",
      "[X] Registering layer: llama_model/layers.0/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.0/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.0/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.0/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.0/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.0/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.0/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.0/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.0/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/mlp/act_fn/Mul for ONNX node: llama_model/layers.0/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.0/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.0/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1713\n",
      "[X] llama_model/layers.0/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1713 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1713 for ONNX node: onnx::MatMul_1713\n",
      "[X] Registering layer: llama_model/layers.0/mlp/up_proj/MatMul for ONNX node: llama_model/layers.0/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.0/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/Mul [Mul] inputs: [llama_model/layers.0/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.0/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/mlp/Mul for ONNX node: llama_model/layers.0/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.0/mlp/Mul_output_0\n",
      "[X] llama_model/layers.0/mlp/Mul [Mul] outputs: [llama_model/layers.0/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_1714\n",
      "[X] llama_model/layers.0/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.0/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_1714 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1714 for ONNX node: onnx::MatMul_1714\n",
      "[X] Registering layer: llama_model/layers.0/mlp/down_proj/MatMul for ONNX node: llama_model/layers.0/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.0/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/Add_1 [Add] inputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/Add_1 for ONNX node: llama_model/layers.0/Add_1\n",
      "[X] Registering tensor: llama_model/layers.0/Add_1_output_0 for ONNX tensor: llama_model/layers.0/Add_1_output_0\n",
      "[X] llama_model/layers.0/Add_1 [Add] outputs: [llama_model/layers.0/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/Add_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast [Cast] inputs: [llama_model/layers.0/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Cast for ONNX node: llama_model/layers.1/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast [Cast] outputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.1/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/input_layernorm/Constant [Constant] outputs: [llama_model/layers.1/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Pow [Pow] inputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.1/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Pow for ONNX node: llama_model/layers.1/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Pow [Pow] outputs: [llama_model/layers.1/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.1/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/ReduceMean for ONNX node: llama_model/layers.1/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.1/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.1/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Add [Add] inputs: [llama_model/layers.1/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.1/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.1/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Add for ONNX node: llama_model/layers.1/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Add [Add] outputs: [llama_model/layers.1/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.1/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Sqrt for ONNX node: llama_model/layers.1/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.1/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.1/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Div [Div] inputs: [llama_model/layers.1/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.1/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.1/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Div for ONNX node: llama_model/layers.1/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Div [Div] outputs: [llama_model/layers.1/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul [Mul] inputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Mul for ONNX node: llama_model/layers.1/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul [Mul] outputs: [llama_model/layers.1/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.1/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Cast_1 for ONNX node: llama_model/layers.1/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.1/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.1.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.1.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.1/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Mul_1 for ONNX node: llama_model/layers.1/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape [Shape] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape for ONNX node: llama_model/layers.1/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape [Shape] outputs: [llama_model/layers.1/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant [Constant] outputs: [llama_model/layers.1/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather [Gather] inputs: [llama_model/layers.1/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.1/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather for ONNX node: llama_model/layers.1/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather [Gather] outputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_1 for ONNX node: llama_model/layers.1/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.1/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_1 for ONNX node: llama_model/layers.1/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1715\n",
      "[X] llama_model/layers.1/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1715 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1715 for ONNX node: onnx::MatMul_1715\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1716\n",
      "[X] llama_model/layers.1/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1716 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1716 for ONNX node: onnx::MatMul_1716\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1717\n",
      "[X] llama_model/layers.1/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1717 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1717 for ONNX node: onnx::MatMul_1717\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_313 [Constant]\n",
      "[X] Constant_313 [Constant] inputs: \n",
      "[X] Constant_313 [Constant] outputs: [onnx::Unsqueeze_413 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_413\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_413 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze for ONNX node: llama_model/layers.1/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_315 [Constant]\n",
      "[X] Constant_315 [Constant] inputs: \n",
      "[X] Constant_315 [Constant] outputs: [onnx::Unsqueeze_415 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_415\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_415 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat for ONNX node: llama_model/layers.1/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat [Concat] outputs: [llama_model/layers.1/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_320 [Constant]\n",
      "[X] Constant_320 [Constant] inputs: \n",
      "[X] Constant_320 [Constant] outputs: [onnx::Unsqueeze_422 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_422\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_422 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_322 [Constant]\n",
      "[X] Constant_322 [Constant] inputs: \n",
      "[X] Constant_322 [Constant] outputs: [onnx::Unsqueeze_424 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_424\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_424 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_1 for ONNX node: llama_model/layers.1/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_327 [Constant]\n",
      "[X] Constant_327 [Constant] inputs: \n",
      "[X] Constant_327 [Constant] outputs: [onnx::Unsqueeze_431 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_431\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_431 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_329 [Constant]\n",
      "[X] Constant_329 [Constant] inputs: \n",
      "[X] Constant_329 [Constant] outputs: [onnx::Unsqueeze_433 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_433\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_433 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_2 for ONNX node: llama_model/layers.1/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape [Reshape] inputs: [llama_model/layers.1/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape for ONNX node: llama_model/layers.1/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose [Transpose] inputs: [llama_model/layers.1/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose for ONNX node: llama_model/layers.1/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.1/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape_1 for ONNX node: llama_model/layers.1/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.1/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_1 for ONNX node: llama_model/layers.1/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.1/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape_2 for ONNX node: llama_model/layers.1/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.1/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_2 for ONNX node: llama_model/layers.1/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_2 for ONNX node: llama_model/layers.1/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.1/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_2 for ONNX node: llama_model/layers.1/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.1/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice_1 [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.1/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_3 for ONNX node: llama_model/layers.1/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_4 for ONNX node: llama_model/layers.1/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul [Mul] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul for ONNX node: llama_model/layers.1/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul [Mul] outputs: [llama_model/layers.1/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_3 for ONNX node: llama_model/layers.1/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_11_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.1/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_11_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_5 for ONNX node: llama_model/layers.1/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div [Div] inputs: [llama_model/layers.1/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Div for ONNX node: llama_model/layers.1/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Div_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div [Div] outputs: [llama_model/layers.1/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Div_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast [Cast] inputs: [llama_model/layers.1/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast for ONNX node: llama_model/layers.1/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast [Cast] outputs: [llama_model/layers.1/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.1/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_1 for ONNX node: llama_model/layers.1/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_15_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_16_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_15_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice for ONNX node: llama_model/layers.1/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice [Slice] outputs: [llama_model/layers.1/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_17_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.1/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_18_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_18_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice_1 for ONNX node: llama_model/layers.1/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.1/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg [Neg] inputs: [llama_model/layers.1/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Neg for ONNX node: llama_model/layers.1/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg [Neg] outputs: [llama_model/layers.1/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.1/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_3 for ONNX node: llama_model/layers.1/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.1/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul_1 for ONNX node: llama_model/layers.1/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.1/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add [Add] inputs: [llama_model/layers.1/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Add for ONNX node: llama_model/layers.1/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Add_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add [Add] outputs: [llama_model/layers.1/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul_2 for ONNX node: llama_model/layers.1/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.1/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_4 for ONNX node: llama_model/layers.1/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.1/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_21_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_6 for ONNX node: llama_model/layers.1/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_1 [Div] inputs: [llama_model/layers.1/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_22_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Div_1 for ONNX node: llama_model/layers.1/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_1 [Div] outputs: [llama_model/layers.1/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.1/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_2 for ONNX node: llama_model/layers.1/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.1/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_3 for ONNX node: llama_model/layers.1/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_25_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_26_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_25_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice_2 for ONNX node: llama_model/layers.1/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.1/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.1/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_28_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_28_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice_3 for ONNX node: llama_model/layers.1/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.1/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.1/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Neg_1 for ONNX node: llama_model/layers.1/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.1/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.1/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_4 for ONNX node: llama_model/layers.1/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.1/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul_3 for ONNX node: llama_model/layers.1/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.1/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_1 [Add] inputs: [llama_model/layers.1/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Add_1 for ONNX node: llama_model/layers.1/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_1 [Add] outputs: [llama_model/layers.1/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.1/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_3 for ONNX node: llama_model/layers.1/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul [MatMul] inputs: [llama_model/layers.1/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/MatMul for ONNX node: llama_model/layers.1/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_2 [Div] inputs: [llama_model/layers.1/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.1/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_31_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_31_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Div_2 for ONNX node: llama_model/layers.1/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_2 [Div] outputs: [llama_model/layers.1/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_2 [Add] inputs: [llama_model/layers.1/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Add_2 for ONNX node: llama_model/layers.1/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_2 [Add] outputs: [llama_model/layers.1/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Softmax [Softmax] inputs: [llama_model/layers.1/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Softmax for ONNX node: llama_model/layers.1/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.1/self_attn/Softmax [Softmax] outputs: [llama_model/layers.1/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.1/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_4 for ONNX node: llama_model/layers.1/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.1/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_5 for ONNX node: llama_model/layers.1/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.1/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.1/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/MatMul_1 for ONNX node: llama_model/layers.1/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.1/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.1/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_4 for ONNX node: llama_model/layers.1/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_421 [Constant]\n",
      "[X] Constant_421 [Constant] inputs: \n",
      "[X] Constant_421 [Constant] outputs: [onnx::Unsqueeze_539 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_539\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_539 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_423 [Constant]\n",
      "[X] Constant_423 [Constant] inputs: \n",
      "[X] Constant_423 [Constant] outputs: [onnx::Unsqueeze_541 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_541\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_541 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_32_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_5 for ONNX node: llama_model/layers.1/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.1/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.1/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape_3 for ONNX node: llama_model/layers.1/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_1737\n",
      "[X] llama_model/layers.1/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.1/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1737 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1737 for ONNX node: onnx::MatMul_1737\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/Add [Add] inputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/Add for ONNX node: llama_model/layers.1/Add\n",
      "[X] Registering tensor: llama_model/layers.1/Add_output_0 for ONNX tensor: llama_model/layers.1/Add_output_0\n",
      "[X] llama_model/layers.1/Add [Add] outputs: [llama_model/layers.1/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/Add_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.1/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Cast for ONNX node: llama_model/layers.1/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.1/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.1/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Pow for ONNX node: llama_model/layers.1/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.1/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.1/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.1/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.1/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Add for ONNX node: llama_model/layers.1/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.1/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.1/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.1/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.1/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.1/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Div for ONNX node: llama_model/layers.1/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.1/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Mul for ONNX node: llama_model/layers.1/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.1/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.1/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.1/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.1/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.1.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.1.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.1/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1738\n",
      "[X] llama_model/layers.1/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1738 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1738 for ONNX node: onnx::MatMul_1738\n",
      "[X] Registering layer: llama_model/layers.1/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.1/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.1/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.1/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.1/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.1/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.1/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.1/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.1/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/mlp/act_fn/Mul for ONNX node: llama_model/layers.1/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.1/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.1/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1739\n",
      "[X] llama_model/layers.1/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1739 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1739 for ONNX node: onnx::MatMul_1739\n",
      "[X] Registering layer: llama_model/layers.1/mlp/up_proj/MatMul for ONNX node: llama_model/layers.1/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.1/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/Mul [Mul] inputs: [llama_model/layers.1/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.1/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/mlp/Mul for ONNX node: llama_model/layers.1/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.1/mlp/Mul_output_0\n",
      "[X] llama_model/layers.1/mlp/Mul [Mul] outputs: [llama_model/layers.1/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_1740\n",
      "[X] llama_model/layers.1/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.1/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_1740 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1740 for ONNX node: onnx::MatMul_1740\n",
      "[X] Registering layer: llama_model/layers.1/mlp/down_proj/MatMul for ONNX node: llama_model/layers.1/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.1/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/Add_1 [Add] inputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/Add_1 for ONNX node: llama_model/layers.1/Add_1\n",
      "[X] Registering tensor: llama_model/layers.1/Add_1_output_0 for ONNX tensor: llama_model/layers.1/Add_1_output_0\n",
      "[X] llama_model/layers.1/Add_1 [Add] outputs: [llama_model/layers.1/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/Add_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast [Cast] inputs: [llama_model/layers.1/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Cast for ONNX node: llama_model/layers.2/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast [Cast] outputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.2/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/input_layernorm/Constant [Constant] outputs: [llama_model/layers.2/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Pow [Pow] inputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.2/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Pow for ONNX node: llama_model/layers.2/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Pow [Pow] outputs: [llama_model/layers.2/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.2/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/ReduceMean for ONNX node: llama_model/layers.2/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.2/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.2/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Add [Add] inputs: [llama_model/layers.2/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.2/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.2/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Add for ONNX node: llama_model/layers.2/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Add [Add] outputs: [llama_model/layers.2/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.2/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Sqrt for ONNX node: llama_model/layers.2/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.2/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.2/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Div [Div] inputs: [llama_model/layers.2/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.2/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.2/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Div for ONNX node: llama_model/layers.2/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Div [Div] outputs: [llama_model/layers.2/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul [Mul] inputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Mul for ONNX node: llama_model/layers.2/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul [Mul] outputs: [llama_model/layers.2/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.2/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Cast_1 for ONNX node: llama_model/layers.2/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.2/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.2.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.2.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.2/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Mul_1 for ONNX node: llama_model/layers.2/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape [Shape] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape for ONNX node: llama_model/layers.2/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape [Shape] outputs: [llama_model/layers.2/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant [Constant] outputs: [llama_model/layers.2/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather [Gather] inputs: [llama_model/layers.2/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.2/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather for ONNX node: llama_model/layers.2/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather [Gather] outputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_1 for ONNX node: llama_model/layers.2/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.2/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_1 for ONNX node: llama_model/layers.2/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1741\n",
      "[X] llama_model/layers.2/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1741 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1741 for ONNX node: onnx::MatMul_1741\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1742\n",
      "[X] llama_model/layers.2/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1742 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1742 for ONNX node: onnx::MatMul_1742\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1743\n",
      "[X] llama_model/layers.2/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1743 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1743 for ONNX node: onnx::MatMul_1743\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_470 [Constant]\n",
      "[X] Constant_470 [Constant] inputs: \n",
      "[X] Constant_470 [Constant] outputs: [onnx::Unsqueeze_596 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_596\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_596 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze for ONNX node: llama_model/layers.2/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_472 [Constant]\n",
      "[X] Constant_472 [Constant] inputs: \n",
      "[X] Constant_472 [Constant] outputs: [onnx::Unsqueeze_598 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_598\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_598 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat for ONNX node: llama_model/layers.2/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat [Concat] outputs: [llama_model/layers.2/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_477 [Constant]\n",
      "[X] Constant_477 [Constant] inputs: \n",
      "[X] Constant_477 [Constant] outputs: [onnx::Unsqueeze_605 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_605\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_605 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_479 [Constant]\n",
      "[X] Constant_479 [Constant] inputs: \n",
      "[X] Constant_479 [Constant] outputs: [onnx::Unsqueeze_607 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_607\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_607 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_1 for ONNX node: llama_model/layers.2/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_484 [Constant]\n",
      "[X] Constant_484 [Constant] inputs: \n",
      "[X] Constant_484 [Constant] outputs: [onnx::Unsqueeze_614 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_614\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_614 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_486 [Constant]\n",
      "[X] Constant_486 [Constant] inputs: \n",
      "[X] Constant_486 [Constant] outputs: [onnx::Unsqueeze_616 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_616\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_616 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_2 for ONNX node: llama_model/layers.2/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape [Reshape] inputs: [llama_model/layers.2/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape for ONNX node: llama_model/layers.2/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose [Transpose] inputs: [llama_model/layers.2/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose for ONNX node: llama_model/layers.2/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.2/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape_1 for ONNX node: llama_model/layers.2/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.2/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_1 for ONNX node: llama_model/layers.2/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.2/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape_2 for ONNX node: llama_model/layers.2/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.2/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_2 for ONNX node: llama_model/layers.2/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_2 for ONNX node: llama_model/layers.2/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.2/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_2 for ONNX node: llama_model/layers.2/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.2/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice_1 [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.2/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_3 for ONNX node: llama_model/layers.2/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_4 for ONNX node: llama_model/layers.2/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul [Mul] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul for ONNX node: llama_model/layers.2/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul [Mul] outputs: [llama_model/layers.2/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_3 for ONNX node: llama_model/layers.2/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_11_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.2/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_11_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_5 for ONNX node: llama_model/layers.2/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div [Div] inputs: [llama_model/layers.2/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Div for ONNX node: llama_model/layers.2/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Div_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div [Div] outputs: [llama_model/layers.2/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Div_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast [Cast] inputs: [llama_model/layers.2/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast for ONNX node: llama_model/layers.2/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast [Cast] outputs: [llama_model/layers.2/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.2/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_1 for ONNX node: llama_model/layers.2/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_15_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_16_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_15_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice for ONNX node: llama_model/layers.2/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice [Slice] outputs: [llama_model/layers.2/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_17_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.2/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_18_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_18_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice_1 for ONNX node: llama_model/layers.2/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.2/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg [Neg] inputs: [llama_model/layers.2/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Neg for ONNX node: llama_model/layers.2/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg [Neg] outputs: [llama_model/layers.2/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.2/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_3 for ONNX node: llama_model/layers.2/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.2/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul_1 for ONNX node: llama_model/layers.2/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.2/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add [Add] inputs: [llama_model/layers.2/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Add for ONNX node: llama_model/layers.2/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Add_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add [Add] outputs: [llama_model/layers.2/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul_2 for ONNX node: llama_model/layers.2/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.2/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_4 for ONNX node: llama_model/layers.2/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.2/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_21_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_6 for ONNX node: llama_model/layers.2/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_1 [Div] inputs: [llama_model/layers.2/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_22_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Div_1 for ONNX node: llama_model/layers.2/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_1 [Div] outputs: [llama_model/layers.2/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.2/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_2 for ONNX node: llama_model/layers.2/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.2/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_3 for ONNX node: llama_model/layers.2/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_25_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_26_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_25_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice_2 for ONNX node: llama_model/layers.2/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.2/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.2/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_28_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_28_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice_3 for ONNX node: llama_model/layers.2/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.2/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.2/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Neg_1 for ONNX node: llama_model/layers.2/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.2/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.2/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_4 for ONNX node: llama_model/layers.2/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.2/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul_3 for ONNX node: llama_model/layers.2/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.2/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_1 [Add] inputs: [llama_model/layers.2/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Add_1 for ONNX node: llama_model/layers.2/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_1 [Add] outputs: [llama_model/layers.2/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.2/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_3 for ONNX node: llama_model/layers.2/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul [MatMul] inputs: [llama_model/layers.2/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/MatMul for ONNX node: llama_model/layers.2/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_2 [Div] inputs: [llama_model/layers.2/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.2/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_31_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_31_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Div_2 for ONNX node: llama_model/layers.2/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_2 [Div] outputs: [llama_model/layers.2/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_2 [Add] inputs: [llama_model/layers.2/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Add_2 for ONNX node: llama_model/layers.2/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_2 [Add] outputs: [llama_model/layers.2/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Softmax [Softmax] inputs: [llama_model/layers.2/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Softmax for ONNX node: llama_model/layers.2/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.2/self_attn/Softmax [Softmax] outputs: [llama_model/layers.2/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.2/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_4 for ONNX node: llama_model/layers.2/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.2/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_5 for ONNX node: llama_model/layers.2/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.2/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.2/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/MatMul_1 for ONNX node: llama_model/layers.2/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.2/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.2/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_4 for ONNX node: llama_model/layers.2/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_578 [Constant]\n",
      "[X] Constant_578 [Constant] inputs: \n",
      "[X] Constant_578 [Constant] outputs: [onnx::Unsqueeze_722 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_722\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_722 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_580 [Constant]\n",
      "[X] Constant_580 [Constant] inputs: \n",
      "[X] Constant_580 [Constant] outputs: [onnx::Unsqueeze_724 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_724\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_724 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_32_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_5 for ONNX node: llama_model/layers.2/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.2/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.2/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape_3 for ONNX node: llama_model/layers.2/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_1763\n",
      "[X] llama_model/layers.2/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.2/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1763 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1763 for ONNX node: onnx::MatMul_1763\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/Add [Add] inputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/Add for ONNX node: llama_model/layers.2/Add\n",
      "[X] Registering tensor: llama_model/layers.2/Add_output_0 for ONNX tensor: llama_model/layers.2/Add_output_0\n",
      "[X] llama_model/layers.2/Add [Add] outputs: [llama_model/layers.2/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/Add_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.2/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Cast for ONNX node: llama_model/layers.2/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.2/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.2/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Pow for ONNX node: llama_model/layers.2/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.2/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.2/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.2/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.2/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Add for ONNX node: llama_model/layers.2/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.2/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.2/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.2/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.2/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.2/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Div for ONNX node: llama_model/layers.2/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.2/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Mul for ONNX node: llama_model/layers.2/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.2/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.2/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.2/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.2/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.2.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.2.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.2/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1764\n",
      "[X] llama_model/layers.2/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1764 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1764 for ONNX node: onnx::MatMul_1764\n",
      "[X] Registering layer: llama_model/layers.2/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.2/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.2/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.2/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.2/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.2/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.2/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.2/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.2/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/mlp/act_fn/Mul for ONNX node: llama_model/layers.2/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.2/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.2/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1765\n",
      "[X] llama_model/layers.2/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1765 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1765 for ONNX node: onnx::MatMul_1765\n",
      "[X] Registering layer: llama_model/layers.2/mlp/up_proj/MatMul for ONNX node: llama_model/layers.2/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.2/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/Mul [Mul] inputs: [llama_model/layers.2/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.2/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/mlp/Mul for ONNX node: llama_model/layers.2/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.2/mlp/Mul_output_0\n",
      "[X] llama_model/layers.2/mlp/Mul [Mul] outputs: [llama_model/layers.2/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_1766\n",
      "[X] llama_model/layers.2/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.2/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_1766 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1766 for ONNX node: onnx::MatMul_1766\n",
      "[X] Registering layer: llama_model/layers.2/mlp/down_proj/MatMul for ONNX node: llama_model/layers.2/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.2/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/Add_1 [Add] inputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/Add_1 for ONNX node: llama_model/layers.2/Add_1\n",
      "[X] Registering tensor: llama_model/layers.2/Add_1_output_0 for ONNX tensor: llama_model/layers.2/Add_1_output_0\n",
      "[X] llama_model/layers.2/Add_1 [Add] outputs: [llama_model/layers.2/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/Add_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast [Cast] inputs: [llama_model/layers.2/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Cast for ONNX node: llama_model/layers.3/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast [Cast] outputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.3/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/input_layernorm/Constant [Constant] outputs: [llama_model/layers.3/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Pow [Pow] inputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.3/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Pow for ONNX node: llama_model/layers.3/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Pow [Pow] outputs: [llama_model/layers.3/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.3/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/ReduceMean for ONNX node: llama_model/layers.3/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.3/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.3/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Add [Add] inputs: [llama_model/layers.3/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.3/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.3/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Add for ONNX node: llama_model/layers.3/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Add [Add] outputs: [llama_model/layers.3/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.3/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Sqrt for ONNX node: llama_model/layers.3/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.3/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.3/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Div [Div] inputs: [llama_model/layers.3/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.3/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.3/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Div for ONNX node: llama_model/layers.3/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Div [Div] outputs: [llama_model/layers.3/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul [Mul] inputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Mul for ONNX node: llama_model/layers.3/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul [Mul] outputs: [llama_model/layers.3/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.3/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Cast_1 for ONNX node: llama_model/layers.3/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.3/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.3.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.3.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.3/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Mul_1 for ONNX node: llama_model/layers.3/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape [Shape] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape for ONNX node: llama_model/layers.3/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape [Shape] outputs: [llama_model/layers.3/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant [Constant] outputs: [llama_model/layers.3/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather [Gather] inputs: [llama_model/layers.3/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.3/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather for ONNX node: llama_model/layers.3/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather [Gather] outputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_1 for ONNX node: llama_model/layers.3/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.3/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_1 for ONNX node: llama_model/layers.3/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1767\n",
      "[X] llama_model/layers.3/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1767 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1767 for ONNX node: onnx::MatMul_1767\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1768\n",
      "[X] llama_model/layers.3/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1768 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1768 for ONNX node: onnx::MatMul_1768\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1769\n",
      "[X] llama_model/layers.3/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1769 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1769 for ONNX node: onnx::MatMul_1769\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_627 [Constant]\n",
      "[X] Constant_627 [Constant] inputs: \n",
      "[X] Constant_627 [Constant] outputs: [onnx::Unsqueeze_779 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_779\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_779 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze for ONNX node: llama_model/layers.3/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_629 [Constant]\n",
      "[X] Constant_629 [Constant] inputs: \n",
      "[X] Constant_629 [Constant] outputs: [onnx::Unsqueeze_781 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_781\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_781 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat for ONNX node: llama_model/layers.3/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat [Concat] outputs: [llama_model/layers.3/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_634 [Constant]\n",
      "[X] Constant_634 [Constant] inputs: \n",
      "[X] Constant_634 [Constant] outputs: [onnx::Unsqueeze_788 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_788\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_788 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_636 [Constant]\n",
      "[X] Constant_636 [Constant] inputs: \n",
      "[X] Constant_636 [Constant] outputs: [onnx::Unsqueeze_790 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_790\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_790 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_1 for ONNX node: llama_model/layers.3/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_641 [Constant]\n",
      "[X] Constant_641 [Constant] inputs: \n",
      "[X] Constant_641 [Constant] outputs: [onnx::Unsqueeze_797 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_797\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_797 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_643 [Constant]\n",
      "[X] Constant_643 [Constant] inputs: \n",
      "[X] Constant_643 [Constant] outputs: [onnx::Unsqueeze_799 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_799\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_799 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_2 for ONNX node: llama_model/layers.3/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape [Reshape] inputs: [llama_model/layers.3/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape for ONNX node: llama_model/layers.3/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose [Transpose] inputs: [llama_model/layers.3/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose for ONNX node: llama_model/layers.3/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.3/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape_1 for ONNX node: llama_model/layers.3/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.3/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_1 for ONNX node: llama_model/layers.3/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.3/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape_2 for ONNX node: llama_model/layers.3/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.3/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_2 for ONNX node: llama_model/layers.3/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_2 for ONNX node: llama_model/layers.3/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.3/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_2 for ONNX node: llama_model/layers.3/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.3/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice_1 [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.3/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_3 for ONNX node: llama_model/layers.3/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_4 for ONNX node: llama_model/layers.3/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul [Mul] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul for ONNX node: llama_model/layers.3/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul [Mul] outputs: [llama_model/layers.3/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_3 for ONNX node: llama_model/layers.3/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_11_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.3/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_11_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_5 for ONNX node: llama_model/layers.3/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div [Div] inputs: [llama_model/layers.3/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Div for ONNX node: llama_model/layers.3/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Div_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div [Div] outputs: [llama_model/layers.3/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Div_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast [Cast] inputs: [llama_model/layers.3/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast for ONNX node: llama_model/layers.3/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast [Cast] outputs: [llama_model/layers.3/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.3/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_1 for ONNX node: llama_model/layers.3/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_15_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_16_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_15_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice for ONNX node: llama_model/layers.3/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice [Slice] outputs: [llama_model/layers.3/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_17_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.3/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_18_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_18_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice_1 for ONNX node: llama_model/layers.3/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.3/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg [Neg] inputs: [llama_model/layers.3/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Neg for ONNX node: llama_model/layers.3/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg [Neg] outputs: [llama_model/layers.3/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.3/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_3 for ONNX node: llama_model/layers.3/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.3/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul_1 for ONNX node: llama_model/layers.3/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.3/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add [Add] inputs: [llama_model/layers.3/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Add for ONNX node: llama_model/layers.3/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Add_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add [Add] outputs: [llama_model/layers.3/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul_2 for ONNX node: llama_model/layers.3/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.3/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_4 for ONNX node: llama_model/layers.3/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.3/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_21_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_6 for ONNX node: llama_model/layers.3/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_1 [Div] inputs: [llama_model/layers.3/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_22_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Div_1 for ONNX node: llama_model/layers.3/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_1 [Div] outputs: [llama_model/layers.3/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.3/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_2 for ONNX node: llama_model/layers.3/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.3/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_3 for ONNX node: llama_model/layers.3/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_25_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_26_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_25_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice_2 for ONNX node: llama_model/layers.3/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.3/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.3/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_28_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_28_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice_3 for ONNX node: llama_model/layers.3/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.3/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.3/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Neg_1 for ONNX node: llama_model/layers.3/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.3/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.3/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_4 for ONNX node: llama_model/layers.3/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.3/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul_3 for ONNX node: llama_model/layers.3/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.3/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_1 [Add] inputs: [llama_model/layers.3/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Add_1 for ONNX node: llama_model/layers.3/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_1 [Add] outputs: [llama_model/layers.3/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.3/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_3 for ONNX node: llama_model/layers.3/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul [MatMul] inputs: [llama_model/layers.3/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/MatMul for ONNX node: llama_model/layers.3/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_2 [Div] inputs: [llama_model/layers.3/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.3/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_31_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_31_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Div_2 for ONNX node: llama_model/layers.3/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_2 [Div] outputs: [llama_model/layers.3/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_2 [Add] inputs: [llama_model/layers.3/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Add_2 for ONNX node: llama_model/layers.3/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_2 [Add] outputs: [llama_model/layers.3/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Softmax [Softmax] inputs: [llama_model/layers.3/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Softmax for ONNX node: llama_model/layers.3/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.3/self_attn/Softmax [Softmax] outputs: [llama_model/layers.3/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.3/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_4 for ONNX node: llama_model/layers.3/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.3/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_5 for ONNX node: llama_model/layers.3/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.3/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.3/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/MatMul_1 for ONNX node: llama_model/layers.3/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.3/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.3/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_4 for ONNX node: llama_model/layers.3/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_735 [Constant]\n",
      "[X] Constant_735 [Constant] inputs: \n",
      "[X] Constant_735 [Constant] outputs: [onnx::Unsqueeze_905 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_905\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_905 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_737 [Constant]\n",
      "[X] Constant_737 [Constant] inputs: \n",
      "[X] Constant_737 [Constant] outputs: [onnx::Unsqueeze_907 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_907\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_907 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_32_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_5 for ONNX node: llama_model/layers.3/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.3/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.3/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape_3 for ONNX node: llama_model/layers.3/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_1789\n",
      "[X] llama_model/layers.3/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.3/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1789 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1789 for ONNX node: onnx::MatMul_1789\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/Add [Add] inputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/Add for ONNX node: llama_model/layers.3/Add\n",
      "[X] Registering tensor: llama_model/layers.3/Add_output_0 for ONNX tensor: llama_model/layers.3/Add_output_0\n",
      "[X] llama_model/layers.3/Add [Add] outputs: [llama_model/layers.3/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/Add_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.3/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Cast for ONNX node: llama_model/layers.3/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.3/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.3/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Pow for ONNX node: llama_model/layers.3/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.3/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.3/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.3/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.3/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Add for ONNX node: llama_model/layers.3/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.3/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.3/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.3/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.3/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.3/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Div for ONNX node: llama_model/layers.3/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.3/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Mul for ONNX node: llama_model/layers.3/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.3/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.3/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.3/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.3/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.3.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.3.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.3/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1790\n",
      "[X] llama_model/layers.3/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1790 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1790 for ONNX node: onnx::MatMul_1790\n",
      "[X] Registering layer: llama_model/layers.3/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.3/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.3/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.3/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.3/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.3/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.3/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.3/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.3/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/mlp/act_fn/Mul for ONNX node: llama_model/layers.3/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.3/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.3/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1791\n",
      "[X] llama_model/layers.3/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1791 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1791 for ONNX node: onnx::MatMul_1791\n",
      "[X] Registering layer: llama_model/layers.3/mlp/up_proj/MatMul for ONNX node: llama_model/layers.3/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.3/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/Mul [Mul] inputs: [llama_model/layers.3/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.3/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/mlp/Mul for ONNX node: llama_model/layers.3/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.3/mlp/Mul_output_0\n",
      "[X] llama_model/layers.3/mlp/Mul [Mul] outputs: [llama_model/layers.3/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_1792\n",
      "[X] llama_model/layers.3/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.3/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_1792 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1792 for ONNX node: onnx::MatMul_1792\n",
      "[X] Registering layer: llama_model/layers.3/mlp/down_proj/MatMul for ONNX node: llama_model/layers.3/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.3/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/Add_1 [Add] inputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/Add_1 for ONNX node: llama_model/layers.3/Add_1\n",
      "[X] Registering tensor: llama_model/layers.3/Add_1_output_0 for ONNX tensor: llama_model/layers.3/Add_1_output_0\n",
      "[X] llama_model/layers.3/Add_1 [Add] outputs: [llama_model/layers.3/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/Add_1_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Cast [Cast] inputs: [llama_model/layers.3/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Cast for ONNX node: llama_model/layers.4/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Cast [Cast] outputs: [llama_model/layers.4/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.4/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.4/input_layernorm/Constant [Constant] outputs: [llama_model/layers.4/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Pow [Pow] inputs: [llama_model/layers.4/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.4/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Pow for ONNX node: llama_model/layers.4/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Pow [Pow] outputs: [llama_model/layers.4/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.4/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/ReduceMean for ONNX node: llama_model/layers.4/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.4/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.4/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.4/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.4/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Add [Add] inputs: [llama_model/layers.4/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.4/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.4/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Add for ONNX node: llama_model/layers.4/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Add [Add] outputs: [llama_model/layers.4/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.4/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Sqrt for ONNX node: llama_model/layers.4/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.4/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.4/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.4/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.4/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Div [Div] inputs: [llama_model/layers.4/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.4/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.4/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Div for ONNX node: llama_model/layers.4/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Div [Div] outputs: [llama_model/layers.4/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Mul [Mul] inputs: [llama_model/layers.4/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Mul for ONNX node: llama_model/layers.4/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Mul [Mul] outputs: [llama_model/layers.4/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.4/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Cast_1 for ONNX node: llama_model/layers.4/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.4/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.4.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.4.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.4/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Mul_1 for ONNX node: llama_model/layers.4/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape [Shape] inputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape for ONNX node: llama_model/layers.4/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape [Shape] outputs: [llama_model/layers.4/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant [Constant] outputs: [llama_model/layers.4/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather [Gather] inputs: [llama_model/layers.4/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.4/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather for ONNX node: llama_model/layers.4/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather [Gather] outputs: [llama_model/layers.4/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape_1 for ONNX node: llama_model/layers.4/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.4/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.4/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.4/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather_1 for ONNX node: llama_model/layers.4/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.4/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1793\n",
      "[X] llama_model/layers.4/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1793 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1793 for ONNX node: onnx::MatMul_1793\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.4/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.4/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1794\n",
      "[X] llama_model/layers.4/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1794 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1794 for ONNX node: onnx::MatMul_1794\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.4/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.4/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1795\n",
      "[X] llama_model/layers.4/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1795 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1795 for ONNX node: onnx::MatMul_1795\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.4/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.4/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_784 [Constant]\n",
      "[X] Constant_784 [Constant] inputs: \n",
      "[X] Constant_784 [Constant] outputs: [onnx::Unsqueeze_962 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_962\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_962 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze for ONNX node: llama_model/layers.4/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_786 [Constant]\n",
      "[X] Constant_786 [Constant] inputs: \n",
      "[X] Constant_786 [Constant] outputs: [onnx::Unsqueeze_964 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_964\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_964 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat for ONNX node: llama_model/layers.4/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat [Concat] outputs: [llama_model/layers.4/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_791 [Constant]\n",
      "[X] Constant_791 [Constant] inputs: \n",
      "[X] Constant_791 [Constant] outputs: [onnx::Unsqueeze_971 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_971\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_971 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_793 [Constant]\n",
      "[X] Constant_793 [Constant] inputs: \n",
      "[X] Constant_793 [Constant] outputs: [onnx::Unsqueeze_973 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_973\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_973 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_1 for ONNX node: llama_model/layers.4/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_798 [Constant]\n",
      "[X] Constant_798 [Constant] inputs: \n",
      "[X] Constant_798 [Constant] outputs: [onnx::Unsqueeze_980 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_980\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_980 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_800 [Constant]\n",
      "[X] Constant_800 [Constant] inputs: \n",
      "[X] Constant_800 [Constant] outputs: [onnx::Unsqueeze_982 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_982\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_982 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_2 for ONNX node: llama_model/layers.4/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape [Reshape] inputs: [llama_model/layers.4/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Reshape for ONNX node: llama_model/layers.4/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape [Reshape] outputs: [llama_model/layers.4/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose [Transpose] inputs: [llama_model/layers.4/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Transpose for ONNX node: llama_model/layers.4/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose [Transpose] outputs: [llama_model/layers.4/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.4/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Reshape_1 for ONNX node: llama_model/layers.4/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.4/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.4/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Transpose_1 for ONNX node: llama_model/layers.4/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.4/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Reshape_2 for ONNX node: llama_model/layers.4/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.4/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.4/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Transpose_2 for ONNX node: llama_model/layers.4/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.4/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape_2 for ONNX node: llama_model/layers.4/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.4/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.4/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.4/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather_2 for ONNX node: llama_model/layers.4/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.4/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Slice [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.4/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.4/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.4/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.4/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Slice_1 [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.4/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.4/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.4/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.4/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.4/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather_3 for ONNX node: llama_model/layers.4/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.4/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.4/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather_4 for ONNX node: llama_model/layers.4/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.4/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul [Mul] inputs: [llama_model/layers.4/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Mul for ONNX node: llama_model/layers.4/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul [Mul] outputs: [llama_model/layers.4/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.4/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape_3 for ONNX node: llama_model/layers.4/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.4/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_11_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.4/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.4/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_11_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather_5 for ONNX node: llama_model/layers.4/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.4/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div [Div] inputs: [llama_model/layers.4/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Div for ONNX node: llama_model/layers.4/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Div_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div [Div] outputs: [llama_model/layers.4/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Div_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast [Cast] inputs: [llama_model/layers.4/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast for ONNX node: llama_model/layers.4/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast [Cast] outputs: [llama_model/layers.4/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.4/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast_1 for ONNX node: llama_model/layers.4/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.4/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_15_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_16_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice [Slice] inputs: [llama_model/layers.4/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_15_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Slice for ONNX node: llama_model/layers.4/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice [Slice] outputs: [llama_model/layers.4/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_17_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.4/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_18_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.4/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_18_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Slice_1 for ONNX node: llama_model/layers.4/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.4/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Neg [Neg] inputs: [llama_model/layers.4/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Neg for ONNX node: llama_model/layers.4/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.4/self_attn/Neg [Neg] outputs: [llama_model/layers.4/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.4/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.4/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_3 for ONNX node: llama_model/layers.4/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.4/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Mul_1 for ONNX node: llama_model/layers.4/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.4/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add [Add] inputs: [llama_model/layers.4/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Add for ONNX node: llama_model/layers.4/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Add_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add [Add] outputs: [llama_model/layers.4/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Mul_2 for ONNX node: llama_model/layers.4/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.4/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape_4 for ONNX node: llama_model/layers.4/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.4/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.4/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.4/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_21_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather_6 for ONNX node: llama_model/layers.4/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.4/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div_1 [Div] inputs: [llama_model/layers.4/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_22_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Div_1 for ONNX node: llama_model/layers.4/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div_1 [Div] outputs: [llama_model/layers.4/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.4/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast_2 for ONNX node: llama_model/layers.4/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.4/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.4/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast_3 for ONNX node: llama_model/layers.4/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.4/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_25_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_26_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_25_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Slice_2 for ONNX node: llama_model/layers.4/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.4/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.4/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_28_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_28_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Slice_3 for ONNX node: llama_model/layers.4/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.4/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.4/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Neg_1 for ONNX node: llama_model/layers.4/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.4/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.4/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.4/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_4 for ONNX node: llama_model/layers.4/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.4/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Mul_3 for ONNX node: llama_model/layers.4/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.4/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add_1 [Add] inputs: [llama_model/layers.4/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Add_1 for ONNX node: llama_model/layers.4/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add_1 [Add] outputs: [llama_model/layers.4/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.4/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Transpose_3 for ONNX node: llama_model/layers.4/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.4/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/MatMul [MatMul] inputs: [llama_model/layers.4/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/MatMul for ONNX node: llama_model/layers.4/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.4/self_attn/MatMul [MatMul] outputs: [llama_model/layers.4/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div_2 [Div] inputs: [llama_model/layers.4/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.4/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_31_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_31_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Div_2 for ONNX node: llama_model/layers.4/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div_2 [Div] outputs: [llama_model/layers.4/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add_2 [Add] inputs: [llama_model/layers.4/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Add_2 for ONNX node: llama_model/layers.4/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add_2 [Add] outputs: [llama_model/layers.4/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Softmax [Softmax] inputs: [llama_model/layers.4/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Softmax for ONNX node: llama_model/layers.4/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.4/self_attn/Softmax [Softmax] outputs: [llama_model/layers.4/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.4/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast_4 for ONNX node: llama_model/layers.4/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.4/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.4/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast_5 for ONNX node: llama_model/layers.4/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.4/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.4/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.4/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/MatMul_1 for ONNX node: llama_model/layers.4/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.4/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.4/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Transpose_4 for ONNX node: llama_model/layers.4/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.4/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_892 [Constant]\n",
      "[X] Constant_892 [Constant] inputs: \n",
      "[X] Constant_892 [Constant] outputs: [onnx::Unsqueeze_1088 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1088\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1088 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_894 [Constant]\n",
      "[X] Constant_894 [Constant] inputs: \n",
      "[X] Constant_894 [Constant] outputs: [onnx::Unsqueeze_1090 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1090\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1090 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_32_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_5 for ONNX node: llama_model/layers.4/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.4/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.4/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Reshape_3 for ONNX node: llama_model/layers.4/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.4/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_1815\n",
      "[X] llama_model/layers.4/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.4/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1815 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1815 for ONNX node: onnx::MatMul_1815\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.4/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.4/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/Add [Add] inputs: [llama_model/layers.4/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/Add for ONNX node: llama_model/layers.4/Add\n",
      "[X] Registering tensor: llama_model/layers.4/Add_output_0 for ONNX tensor: llama_model/layers.4/Add_output_0\n",
      "[X] llama_model/layers.4/Add [Add] outputs: [llama_model/layers.4/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/Add_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.4/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Cast for ONNX node: llama_model/layers.4/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.4/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.4/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.4/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.4/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Pow for ONNX node: llama_model/layers.4/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.4/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.4/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.4/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.4/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.4/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.4/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.4/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.4/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Add for ONNX node: llama_model/layers.4/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.4/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.4/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.4/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.4/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.4/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.4/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.4/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.4/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Div for ONNX node: llama_model/layers.4/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.4/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.4/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Mul for ONNX node: llama_model/layers.4/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.4/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.4/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.4/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.4/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.4.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.4.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.4/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.4/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.4/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1816\n",
      "[X] llama_model/layers.4/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.4/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1816 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1816 for ONNX node: onnx::MatMul_1816\n",
      "[X] Registering layer: llama_model/layers.4/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.4/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.4/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.4/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.4/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.4/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.4/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.4/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.4/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.4/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.4/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.4/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/mlp/act_fn/Mul for ONNX node: llama_model/layers.4/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.4/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.4/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.4/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1817\n",
      "[X] llama_model/layers.4/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.4/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1817 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1817 for ONNX node: onnx::MatMul_1817\n",
      "[X] Registering layer: llama_model/layers.4/mlp/up_proj/MatMul for ONNX node: llama_model/layers.4/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.4/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/mlp/Mul [Mul] inputs: [llama_model/layers.4/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.4/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/mlp/Mul for ONNX node: llama_model/layers.4/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.4/mlp/Mul_output_0\n",
      "[X] llama_model/layers.4/mlp/Mul [Mul] outputs: [llama_model/layers.4/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_1818\n",
      "[X] llama_model/layers.4/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.4/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_1818 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1818 for ONNX node: onnx::MatMul_1818\n",
      "[X] Registering layer: llama_model/layers.4/mlp/down_proj/MatMul for ONNX node: llama_model/layers.4/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.4/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/Add_1 [Add] inputs: [llama_model/layers.4/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/Add_1 for ONNX node: llama_model/layers.4/Add_1\n",
      "[X] Registering tensor: llama_model/layers.4/Add_1_output_0 for ONNX tensor: llama_model/layers.4/Add_1_output_0\n",
      "[X] llama_model/layers.4/Add_1 [Add] outputs: [llama_model/layers.4/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/Add_1_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Cast [Cast] inputs: [llama_model/layers.4/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Cast for ONNX node: llama_model/layers.5/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Cast [Cast] outputs: [llama_model/layers.5/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.5/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.5/input_layernorm/Constant [Constant] outputs: [llama_model/layers.5/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Pow [Pow] inputs: [llama_model/layers.5/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.5/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Pow for ONNX node: llama_model/layers.5/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Pow [Pow] outputs: [llama_model/layers.5/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.5/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/ReduceMean for ONNX node: llama_model/layers.5/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.5/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.5/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.5/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.5/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Add [Add] inputs: [llama_model/layers.5/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.5/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.5/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Add for ONNX node: llama_model/layers.5/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Add [Add] outputs: [llama_model/layers.5/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.5/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Sqrt for ONNX node: llama_model/layers.5/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.5/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.5/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.5/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.5/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Div [Div] inputs: [llama_model/layers.5/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.5/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.5/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Div for ONNX node: llama_model/layers.5/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Div [Div] outputs: [llama_model/layers.5/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Mul [Mul] inputs: [llama_model/layers.5/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Mul for ONNX node: llama_model/layers.5/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Mul [Mul] outputs: [llama_model/layers.5/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.5/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Cast_1 for ONNX node: llama_model/layers.5/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.5/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.5.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.5.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.5/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Mul_1 for ONNX node: llama_model/layers.5/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape [Shape] inputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape for ONNX node: llama_model/layers.5/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape [Shape] outputs: [llama_model/layers.5/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant [Constant] outputs: [llama_model/layers.5/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather [Gather] inputs: [llama_model/layers.5/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.5/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather for ONNX node: llama_model/layers.5/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather [Gather] outputs: [llama_model/layers.5/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape_1 for ONNX node: llama_model/layers.5/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.5/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.5/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.5/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather_1 for ONNX node: llama_model/layers.5/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.5/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1819\n",
      "[X] llama_model/layers.5/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1819 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1819 for ONNX node: onnx::MatMul_1819\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.5/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.5/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1820\n",
      "[X] llama_model/layers.5/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1820 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1820 for ONNX node: onnx::MatMul_1820\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.5/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.5/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1821\n",
      "[X] llama_model/layers.5/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1821 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1821 for ONNX node: onnx::MatMul_1821\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.5/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.5/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_941 [Constant]\n",
      "[X] Constant_941 [Constant] inputs: \n",
      "[X] Constant_941 [Constant] outputs: [onnx::Unsqueeze_1145 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1145\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1145 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze for ONNX node: llama_model/layers.5/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_943 [Constant]\n",
      "[X] Constant_943 [Constant] inputs: \n",
      "[X] Constant_943 [Constant] outputs: [onnx::Unsqueeze_1147 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1147\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1147 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat for ONNX node: llama_model/layers.5/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat [Concat] outputs: [llama_model/layers.5/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_948 [Constant]\n",
      "[X] Constant_948 [Constant] inputs: \n",
      "[X] Constant_948 [Constant] outputs: [onnx::Unsqueeze_1154 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1154\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1154 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_950 [Constant]\n",
      "[X] Constant_950 [Constant] inputs: \n",
      "[X] Constant_950 [Constant] outputs: [onnx::Unsqueeze_1156 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1156\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1156 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_1 for ONNX node: llama_model/layers.5/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_955 [Constant]\n",
      "[X] Constant_955 [Constant] inputs: \n",
      "[X] Constant_955 [Constant] outputs: [onnx::Unsqueeze_1163 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1163\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1163 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_957 [Constant]\n",
      "[X] Constant_957 [Constant] inputs: \n",
      "[X] Constant_957 [Constant] outputs: [onnx::Unsqueeze_1165 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1165\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1165 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_2 for ONNX node: llama_model/layers.5/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape [Reshape] inputs: [llama_model/layers.5/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Reshape for ONNX node: llama_model/layers.5/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape [Reshape] outputs: [llama_model/layers.5/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose [Transpose] inputs: [llama_model/layers.5/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Transpose for ONNX node: llama_model/layers.5/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose [Transpose] outputs: [llama_model/layers.5/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.5/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Reshape_1 for ONNX node: llama_model/layers.5/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.5/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.5/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Transpose_1 for ONNX node: llama_model/layers.5/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.5/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Reshape_2 for ONNX node: llama_model/layers.5/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.5/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.5/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Transpose_2 for ONNX node: llama_model/layers.5/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.5/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape_2 for ONNX node: llama_model/layers.5/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.5/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.5/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.5/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather_2 for ONNX node: llama_model/layers.5/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.5/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Slice [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.5/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.5/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.5/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.5/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Slice_1 [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.5/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.5/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.5/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.5/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.5/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather_3 for ONNX node: llama_model/layers.5/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.5/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.5/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather_4 for ONNX node: llama_model/layers.5/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.5/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul [Mul] inputs: [llama_model/layers.5/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Mul for ONNX node: llama_model/layers.5/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul [Mul] outputs: [llama_model/layers.5/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.5/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape_3 for ONNX node: llama_model/layers.5/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.5/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_11_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.5/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.5/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_11_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather_5 for ONNX node: llama_model/layers.5/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.5/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div [Div] inputs: [llama_model/layers.5/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Div for ONNX node: llama_model/layers.5/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Div_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div [Div] outputs: [llama_model/layers.5/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Div_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast [Cast] inputs: [llama_model/layers.5/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast for ONNX node: llama_model/layers.5/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast [Cast] outputs: [llama_model/layers.5/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.5/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast_1 for ONNX node: llama_model/layers.5/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.5/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_15_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_16_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice [Slice] inputs: [llama_model/layers.5/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_15_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Slice for ONNX node: llama_model/layers.5/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice [Slice] outputs: [llama_model/layers.5/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_17_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.5/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_18_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.5/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_18_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Slice_1 for ONNX node: llama_model/layers.5/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.5/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Neg [Neg] inputs: [llama_model/layers.5/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Neg for ONNX node: llama_model/layers.5/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.5/self_attn/Neg [Neg] outputs: [llama_model/layers.5/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.5/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.5/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_3 for ONNX node: llama_model/layers.5/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.5/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Mul_1 for ONNX node: llama_model/layers.5/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.5/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add [Add] inputs: [llama_model/layers.5/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Add for ONNX node: llama_model/layers.5/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Add_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add [Add] outputs: [llama_model/layers.5/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Mul_2 for ONNX node: llama_model/layers.5/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.5/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape_4 for ONNX node: llama_model/layers.5/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.5/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.5/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.5/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_21_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather_6 for ONNX node: llama_model/layers.5/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.5/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div_1 [Div] inputs: [llama_model/layers.5/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_22_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Div_1 for ONNX node: llama_model/layers.5/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div_1 [Div] outputs: [llama_model/layers.5/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.5/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast_2 for ONNX node: llama_model/layers.5/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.5/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.5/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast_3 for ONNX node: llama_model/layers.5/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.5/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_25_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_26_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_25_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Slice_2 for ONNX node: llama_model/layers.5/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.5/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.5/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_28_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_28_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Slice_3 for ONNX node: llama_model/layers.5/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.5/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.5/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Neg_1 for ONNX node: llama_model/layers.5/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.5/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.5/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.5/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_4 for ONNX node: llama_model/layers.5/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.5/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Mul_3 for ONNX node: llama_model/layers.5/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.5/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add_1 [Add] inputs: [llama_model/layers.5/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Add_1 for ONNX node: llama_model/layers.5/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add_1 [Add] outputs: [llama_model/layers.5/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.5/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Transpose_3 for ONNX node: llama_model/layers.5/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.5/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/MatMul [MatMul] inputs: [llama_model/layers.5/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/MatMul for ONNX node: llama_model/layers.5/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.5/self_attn/MatMul [MatMul] outputs: [llama_model/layers.5/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div_2 [Div] inputs: [llama_model/layers.5/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.5/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_31_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_31_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Div_2 for ONNX node: llama_model/layers.5/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div_2 [Div] outputs: [llama_model/layers.5/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add_2 [Add] inputs: [llama_model/layers.5/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Add_2 for ONNX node: llama_model/layers.5/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add_2 [Add] outputs: [llama_model/layers.5/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Softmax [Softmax] inputs: [llama_model/layers.5/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Softmax for ONNX node: llama_model/layers.5/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.5/self_attn/Softmax [Softmax] outputs: [llama_model/layers.5/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.5/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast_4 for ONNX node: llama_model/layers.5/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.5/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.5/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast_5 for ONNX node: llama_model/layers.5/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.5/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.5/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.5/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/MatMul_1 for ONNX node: llama_model/layers.5/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.5/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.5/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Transpose_4 for ONNX node: llama_model/layers.5/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.5/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_1049 [Constant]\n",
      "[X] Constant_1049 [Constant] inputs: \n",
      "[X] Constant_1049 [Constant] outputs: [onnx::Unsqueeze_1271 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1271\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1271 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1051 [Constant]\n",
      "[X] Constant_1051 [Constant] inputs: \n",
      "[X] Constant_1051 [Constant] outputs: [onnx::Unsqueeze_1273 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1273\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1273 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_32_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_5 for ONNX node: llama_model/layers.5/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.5/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.5/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Reshape_3 for ONNX node: llama_model/layers.5/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.5/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_1841\n",
      "[X] llama_model/layers.5/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.5/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1841 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1841 for ONNX node: onnx::MatMul_1841\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.5/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.5/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/Add [Add] inputs: [llama_model/layers.5/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/Add for ONNX node: llama_model/layers.5/Add\n",
      "[X] Registering tensor: llama_model/layers.5/Add_output_0 for ONNX tensor: llama_model/layers.5/Add_output_0\n",
      "[X] llama_model/layers.5/Add [Add] outputs: [llama_model/layers.5/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/Add_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.5/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Cast for ONNX node: llama_model/layers.5/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.5/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.5/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.5/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.5/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Pow for ONNX node: llama_model/layers.5/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.5/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.5/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.5/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.5/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.5/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.5/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.5/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.5/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Add for ONNX node: llama_model/layers.5/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.5/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.5/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.5/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.5/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.5/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.5/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.5/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.5/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Div for ONNX node: llama_model/layers.5/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.5/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.5/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Mul for ONNX node: llama_model/layers.5/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.5/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.5/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.5/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.5/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.5.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.5.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.5/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.5/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.5/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1842\n",
      "[X] llama_model/layers.5/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.5/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1842 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1842 for ONNX node: onnx::MatMul_1842\n",
      "[X] Registering layer: llama_model/layers.5/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.5/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.5/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.5/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.5/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.5/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.5/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.5/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.5/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.5/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.5/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.5/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/mlp/act_fn/Mul for ONNX node: llama_model/layers.5/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.5/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.5/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.5/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1843\n",
      "[X] llama_model/layers.5/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.5/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1843 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1843 for ONNX node: onnx::MatMul_1843\n",
      "[X] Registering layer: llama_model/layers.5/mlp/up_proj/MatMul for ONNX node: llama_model/layers.5/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.5/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/mlp/Mul [Mul] inputs: [llama_model/layers.5/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.5/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/mlp/Mul for ONNX node: llama_model/layers.5/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.5/mlp/Mul_output_0\n",
      "[X] llama_model/layers.5/mlp/Mul [Mul] outputs: [llama_model/layers.5/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_1844\n",
      "[X] llama_model/layers.5/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.5/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_1844 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1844 for ONNX node: onnx::MatMul_1844\n",
      "[X] Registering layer: llama_model/layers.5/mlp/down_proj/MatMul for ONNX node: llama_model/layers.5/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.5/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/Add_1 [Add] inputs: [llama_model/layers.5/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/Add_1 for ONNX node: llama_model/layers.5/Add_1\n",
      "[X] Registering tensor: llama_model/layers.5/Add_1_output_0 for ONNX tensor: llama_model/layers.5/Add_1_output_0\n",
      "[X] llama_model/layers.5/Add_1 [Add] outputs: [llama_model/layers.5/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/Add_1_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Cast [Cast] inputs: [llama_model/layers.5/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Cast for ONNX node: llama_model/layers.6/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Cast [Cast] outputs: [llama_model/layers.6/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.6/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.6/input_layernorm/Constant [Constant] outputs: [llama_model/layers.6/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Pow [Pow] inputs: [llama_model/layers.6/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.6/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Pow for ONNX node: llama_model/layers.6/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Pow [Pow] outputs: [llama_model/layers.6/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.6/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/ReduceMean for ONNX node: llama_model/layers.6/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.6/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.6/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.6/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.6/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Add [Add] inputs: [llama_model/layers.6/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.6/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.6/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Add for ONNX node: llama_model/layers.6/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Add [Add] outputs: [llama_model/layers.6/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.6/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Sqrt for ONNX node: llama_model/layers.6/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.6/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.6/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.6/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.6/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Div [Div] inputs: [llama_model/layers.6/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.6/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.6/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Div for ONNX node: llama_model/layers.6/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Div [Div] outputs: [llama_model/layers.6/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Mul [Mul] inputs: [llama_model/layers.6/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Mul for ONNX node: llama_model/layers.6/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Mul [Mul] outputs: [llama_model/layers.6/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.6/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Cast_1 for ONNX node: llama_model/layers.6/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.6/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.6.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.6.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.6/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Mul_1 for ONNX node: llama_model/layers.6/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape [Shape] inputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape for ONNX node: llama_model/layers.6/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape [Shape] outputs: [llama_model/layers.6/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant [Constant] outputs: [llama_model/layers.6/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather [Gather] inputs: [llama_model/layers.6/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.6/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather for ONNX node: llama_model/layers.6/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather [Gather] outputs: [llama_model/layers.6/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape_1 for ONNX node: llama_model/layers.6/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.6/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.6/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.6/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather_1 for ONNX node: llama_model/layers.6/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.6/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1845\n",
      "[X] llama_model/layers.6/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1845 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1845 for ONNX node: onnx::MatMul_1845\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.6/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.6/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1846\n",
      "[X] llama_model/layers.6/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1846 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1846 for ONNX node: onnx::MatMul_1846\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.6/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.6/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1847\n",
      "[X] llama_model/layers.6/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1847 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1847 for ONNX node: onnx::MatMul_1847\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.6/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.6/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_1098 [Constant]\n",
      "[X] Constant_1098 [Constant] inputs: \n",
      "[X] Constant_1098 [Constant] outputs: [onnx::Unsqueeze_1328 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1328\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1328 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze for ONNX node: llama_model/layers.6/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1100 [Constant]\n",
      "[X] Constant_1100 [Constant] inputs: \n",
      "[X] Constant_1100 [Constant] outputs: [onnx::Unsqueeze_1330 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1330\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1330 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat for ONNX node: llama_model/layers.6/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat [Concat] outputs: [llama_model/layers.6/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1105 [Constant]\n",
      "[X] Constant_1105 [Constant] inputs: \n",
      "[X] Constant_1105 [Constant] outputs: [onnx::Unsqueeze_1337 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1337\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1337 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1107 [Constant]\n",
      "[X] Constant_1107 [Constant] inputs: \n",
      "[X] Constant_1107 [Constant] outputs: [onnx::Unsqueeze_1339 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1339\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1339 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_1 for ONNX node: llama_model/layers.6/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1112 [Constant]\n",
      "[X] Constant_1112 [Constant] inputs: \n",
      "[X] Constant_1112 [Constant] outputs: [onnx::Unsqueeze_1346 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1346\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1346 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1114 [Constant]\n",
      "[X] Constant_1114 [Constant] inputs: \n",
      "[X] Constant_1114 [Constant] outputs: [onnx::Unsqueeze_1348 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1348\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1348 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_2 for ONNX node: llama_model/layers.6/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape [Reshape] inputs: [llama_model/layers.6/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Reshape for ONNX node: llama_model/layers.6/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape [Reshape] outputs: [llama_model/layers.6/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose [Transpose] inputs: [llama_model/layers.6/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Transpose for ONNX node: llama_model/layers.6/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose [Transpose] outputs: [llama_model/layers.6/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.6/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Reshape_1 for ONNX node: llama_model/layers.6/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.6/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.6/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Transpose_1 for ONNX node: llama_model/layers.6/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.6/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Reshape_2 for ONNX node: llama_model/layers.6/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.6/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.6/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Transpose_2 for ONNX node: llama_model/layers.6/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.6/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape_2 for ONNX node: llama_model/layers.6/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.6/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.6/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.6/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather_2 for ONNX node: llama_model/layers.6/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.6/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Slice [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.6/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.6/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.6/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.6/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Slice_1 [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.6/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.6/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.6/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.6/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.6/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather_3 for ONNX node: llama_model/layers.6/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.6/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.6/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather_4 for ONNX node: llama_model/layers.6/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.6/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul [Mul] inputs: [llama_model/layers.6/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Mul for ONNX node: llama_model/layers.6/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul [Mul] outputs: [llama_model/layers.6/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.6/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape_3 for ONNX node: llama_model/layers.6/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.6/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_11_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.6/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.6/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_11_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather_5 for ONNX node: llama_model/layers.6/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.6/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div [Div] inputs: [llama_model/layers.6/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Div for ONNX node: llama_model/layers.6/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Div_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div [Div] outputs: [llama_model/layers.6/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Div_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast [Cast] inputs: [llama_model/layers.6/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast for ONNX node: llama_model/layers.6/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast [Cast] outputs: [llama_model/layers.6/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.6/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast_1 for ONNX node: llama_model/layers.6/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.6/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_15_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_16_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice [Slice] inputs: [llama_model/layers.6/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_15_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Slice for ONNX node: llama_model/layers.6/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice [Slice] outputs: [llama_model/layers.6/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_17_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.6/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_18_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.6/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_18_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Slice_1 for ONNX node: llama_model/layers.6/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.6/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Neg [Neg] inputs: [llama_model/layers.6/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Neg for ONNX node: llama_model/layers.6/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.6/self_attn/Neg [Neg] outputs: [llama_model/layers.6/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.6/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.6/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_3 for ONNX node: llama_model/layers.6/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.6/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Mul_1 for ONNX node: llama_model/layers.6/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.6/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add [Add] inputs: [llama_model/layers.6/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Add for ONNX node: llama_model/layers.6/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Add_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add [Add] outputs: [llama_model/layers.6/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Mul_2 for ONNX node: llama_model/layers.6/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.6/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape_4 for ONNX node: llama_model/layers.6/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.6/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.6/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.6/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_21_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather_6 for ONNX node: llama_model/layers.6/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.6/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div_1 [Div] inputs: [llama_model/layers.6/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_22_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Div_1 for ONNX node: llama_model/layers.6/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div_1 [Div] outputs: [llama_model/layers.6/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.6/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast_2 for ONNX node: llama_model/layers.6/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.6/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.6/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast_3 for ONNX node: llama_model/layers.6/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.6/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_25_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_26_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_25_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Slice_2 for ONNX node: llama_model/layers.6/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.6/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.6/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_28_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_28_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Slice_3 for ONNX node: llama_model/layers.6/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.6/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.6/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Neg_1 for ONNX node: llama_model/layers.6/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.6/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.6/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.6/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_4 for ONNX node: llama_model/layers.6/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.6/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Mul_3 for ONNX node: llama_model/layers.6/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.6/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add_1 [Add] inputs: [llama_model/layers.6/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Add_1 for ONNX node: llama_model/layers.6/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add_1 [Add] outputs: [llama_model/layers.6/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.6/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Transpose_3 for ONNX node: llama_model/layers.6/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.6/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/MatMul [MatMul] inputs: [llama_model/layers.6/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/MatMul for ONNX node: llama_model/layers.6/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.6/self_attn/MatMul [MatMul] outputs: [llama_model/layers.6/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div_2 [Div] inputs: [llama_model/layers.6/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.6/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_31_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_31_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Div_2 for ONNX node: llama_model/layers.6/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div_2 [Div] outputs: [llama_model/layers.6/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add_2 [Add] inputs: [llama_model/layers.6/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Add_2 for ONNX node: llama_model/layers.6/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add_2 [Add] outputs: [llama_model/layers.6/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Softmax [Softmax] inputs: [llama_model/layers.6/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Softmax for ONNX node: llama_model/layers.6/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.6/self_attn/Softmax [Softmax] outputs: [llama_model/layers.6/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.6/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast_4 for ONNX node: llama_model/layers.6/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.6/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.6/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast_5 for ONNX node: llama_model/layers.6/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.6/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.6/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.6/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/MatMul_1 for ONNX node: llama_model/layers.6/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.6/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.6/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Transpose_4 for ONNX node: llama_model/layers.6/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.6/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_1206 [Constant]\n",
      "[X] Constant_1206 [Constant] inputs: \n",
      "[X] Constant_1206 [Constant] outputs: [onnx::Unsqueeze_1454 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1454\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1454 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1208 [Constant]\n",
      "[X] Constant_1208 [Constant] inputs: \n",
      "[X] Constant_1208 [Constant] outputs: [onnx::Unsqueeze_1456 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1456\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1456 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_32_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_5 for ONNX node: llama_model/layers.6/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.6/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.6/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Reshape_3 for ONNX node: llama_model/layers.6/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.6/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_1867\n",
      "[X] llama_model/layers.6/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.6/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1867 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1867 for ONNX node: onnx::MatMul_1867\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.6/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.6/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/Add [Add] inputs: [llama_model/layers.6/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/Add for ONNX node: llama_model/layers.6/Add\n",
      "[X] Registering tensor: llama_model/layers.6/Add_output_0 for ONNX tensor: llama_model/layers.6/Add_output_0\n",
      "[X] llama_model/layers.6/Add [Add] outputs: [llama_model/layers.6/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/Add_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.6/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Cast for ONNX node: llama_model/layers.6/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.6/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.6/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.6/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.6/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Pow for ONNX node: llama_model/layers.6/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.6/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.6/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.6/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.6/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.6/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.6/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.6/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.6/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Add for ONNX node: llama_model/layers.6/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.6/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.6/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.6/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.6/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.6/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.6/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.6/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.6/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Div for ONNX node: llama_model/layers.6/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.6/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.6/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Mul for ONNX node: llama_model/layers.6/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.6/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.6/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.6/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.6/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.6.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.6.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.6/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.6/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.6/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1868\n",
      "[X] llama_model/layers.6/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.6/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1868 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1868 for ONNX node: onnx::MatMul_1868\n",
      "[X] Registering layer: llama_model/layers.6/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.6/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.6/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.6/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.6/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.6/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.6/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.6/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.6/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.6/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.6/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.6/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/mlp/act_fn/Mul for ONNX node: llama_model/layers.6/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.6/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.6/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.6/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1869\n",
      "[X] llama_model/layers.6/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.6/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1869 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1869 for ONNX node: onnx::MatMul_1869\n",
      "[X] Registering layer: llama_model/layers.6/mlp/up_proj/MatMul for ONNX node: llama_model/layers.6/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.6/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/mlp/Mul [Mul] inputs: [llama_model/layers.6/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.6/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/mlp/Mul for ONNX node: llama_model/layers.6/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.6/mlp/Mul_output_0\n",
      "[X] llama_model/layers.6/mlp/Mul [Mul] outputs: [llama_model/layers.6/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_1870\n",
      "[X] llama_model/layers.6/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.6/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_1870 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1870 for ONNX node: onnx::MatMul_1870\n",
      "[X] Registering layer: llama_model/layers.6/mlp/down_proj/MatMul for ONNX node: llama_model/layers.6/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.6/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/Add_1 [Add] inputs: [llama_model/layers.6/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/Add_1 for ONNX node: llama_model/layers.6/Add_1\n",
      "[X] Registering tensor: llama_model/layers.6/Add_1_output_0 for ONNX tensor: llama_model/layers.6/Add_1_output_0\n",
      "[X] llama_model/layers.6/Add_1 [Add] outputs: [llama_model/layers.6/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/Add_1_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Cast [Cast] inputs: [llama_model/layers.6/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Cast for ONNX node: llama_model/layers.7/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Cast [Cast] outputs: [llama_model/layers.7/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.7/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.7/input_layernorm/Constant [Constant] outputs: [llama_model/layers.7/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Pow [Pow] inputs: [llama_model/layers.7/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.7/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Pow for ONNX node: llama_model/layers.7/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Pow [Pow] outputs: [llama_model/layers.7/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.7/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/ReduceMean for ONNX node: llama_model/layers.7/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.7/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.7/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.7/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.7/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Add [Add] inputs: [llama_model/layers.7/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.7/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.7/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Add for ONNX node: llama_model/layers.7/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Add [Add] outputs: [llama_model/layers.7/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.7/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Sqrt for ONNX node: llama_model/layers.7/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.7/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.7/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.7/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.7/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Div [Div] inputs: [llama_model/layers.7/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.7/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.7/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Div for ONNX node: llama_model/layers.7/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Div [Div] outputs: [llama_model/layers.7/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Mul [Mul] inputs: [llama_model/layers.7/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Mul for ONNX node: llama_model/layers.7/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Mul [Mul] outputs: [llama_model/layers.7/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.7/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Cast_1 for ONNX node: llama_model/layers.7/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.7/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.7.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.7.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.7/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Mul_1 for ONNX node: llama_model/layers.7/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape [Shape] inputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape for ONNX node: llama_model/layers.7/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape [Shape] outputs: [llama_model/layers.7/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant [Constant] outputs: [llama_model/layers.7/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather [Gather] inputs: [llama_model/layers.7/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.7/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather for ONNX node: llama_model/layers.7/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather [Gather] outputs: [llama_model/layers.7/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape_1 for ONNX node: llama_model/layers.7/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.7/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.7/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.7/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather_1 for ONNX node: llama_model/layers.7/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.7/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1871\n",
      "[X] llama_model/layers.7/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1871 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1871 for ONNX node: onnx::MatMul_1871\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.7/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.7/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1872\n",
      "[X] llama_model/layers.7/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1872 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1872 for ONNX node: onnx::MatMul_1872\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.7/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.7/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1873\n",
      "[X] llama_model/layers.7/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1873 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1873 for ONNX node: onnx::MatMul_1873\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.7/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.7/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_1255 [Constant]\n",
      "[X] Constant_1255 [Constant] inputs: \n",
      "[X] Constant_1255 [Constant] outputs: [onnx::Unsqueeze_1511 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1511\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1511 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze for ONNX node: llama_model/layers.7/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1257 [Constant]\n",
      "[X] Constant_1257 [Constant] inputs: \n",
      "[X] Constant_1257 [Constant] outputs: [onnx::Unsqueeze_1513 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1513\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1513 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat for ONNX node: llama_model/layers.7/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat [Concat] outputs: [llama_model/layers.7/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1262 [Constant]\n",
      "[X] Constant_1262 [Constant] inputs: \n",
      "[X] Constant_1262 [Constant] outputs: [onnx::Unsqueeze_1520 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1520\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1520 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1264 [Constant]\n",
      "[X] Constant_1264 [Constant] inputs: \n",
      "[X] Constant_1264 [Constant] outputs: [onnx::Unsqueeze_1522 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1522\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1522 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_1 for ONNX node: llama_model/layers.7/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1269 [Constant]\n",
      "[X] Constant_1269 [Constant] inputs: \n",
      "[X] Constant_1269 [Constant] outputs: [onnx::Unsqueeze_1529 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1529\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1529 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1271 [Constant]\n",
      "[X] Constant_1271 [Constant] inputs: \n",
      "[X] Constant_1271 [Constant] outputs: [onnx::Unsqueeze_1531 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1531\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1531 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_2 for ONNX node: llama_model/layers.7/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape [Reshape] inputs: [llama_model/layers.7/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Reshape for ONNX node: llama_model/layers.7/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape [Reshape] outputs: [llama_model/layers.7/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose [Transpose] inputs: [llama_model/layers.7/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Transpose for ONNX node: llama_model/layers.7/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose [Transpose] outputs: [llama_model/layers.7/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.7/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Reshape_1 for ONNX node: llama_model/layers.7/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.7/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.7/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Transpose_1 for ONNX node: llama_model/layers.7/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.7/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Reshape_2 for ONNX node: llama_model/layers.7/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.7/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.7/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Transpose_2 for ONNX node: llama_model/layers.7/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.7/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape_2 for ONNX node: llama_model/layers.7/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.7/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.7/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.7/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather_2 for ONNX node: llama_model/layers.7/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.7/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Slice [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.7/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.7/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.7/self_attn/rotary_emb/Slice_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.7/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Slice_1 [Slice] inputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (128, 8)[FLOAT]], [llama_model/layers.7/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.7/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.7/self_attn/rotary_emb/Slice_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.7/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.7/self_attn/rotary_emb/Cast_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather_3 for ONNX node: llama_model/layers.7/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.7/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.7/self_attn/rotary_emb/Cast_1_output_0 -> (-1, 8)[FLOAT]], [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather_4 for ONNX node: llama_model/layers.7/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.7/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_3_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_10_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_4_output_0 -> (1, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, 8), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul [Mul] inputs: [llama_model/layers.7/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Mul for ONNX node: llama_model/layers.7/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul [Mul] outputs: [llama_model/layers.7/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.7/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape_3 for ONNX node: llama_model/layers.7/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.7/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_11_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.7/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.7/self_attn/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_11_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather_5 for ONNX node: llama_model/layers.7/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.7/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div [Div] inputs: [llama_model/layers.7/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Div for ONNX node: llama_model/layers.7/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Div_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div [Div] outputs: [llama_model/layers.7/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Div_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast [Cast] inputs: [llama_model/layers.7/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast for ONNX node: llama_model/layers.7/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast [Cast] outputs: [llama_model/layers.7/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.7/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast_1 for ONNX node: llama_model/layers.7/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.7/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_14_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_15_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_16_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice [Slice] inputs: [llama_model/layers.7/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_15_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Slice for ONNX node: llama_model/layers.7/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice [Slice] outputs: [llama_model/layers.7/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_17_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.7/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_18_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_20_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.7/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_18_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Slice_1 for ONNX node: llama_model/layers.7/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.7/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Neg [Neg] inputs: [llama_model/layers.7/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Neg for ONNX node: llama_model/layers.7/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.7/self_attn/Neg [Neg] outputs: [llama_model/layers.7/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.7/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.7/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_3 for ONNX node: llama_model/layers.7/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_3_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.7/self_attn/Concat_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Mul_1 for ONNX node: llama_model/layers.7/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.7/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add [Add] inputs: [llama_model/layers.7/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Add for ONNX node: llama_model/layers.7/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Add_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add [Add] outputs: [llama_model/layers.7/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Unsqueeze_6_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Mul_2 for ONNX node: llama_model/layers.7/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.7/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape_4 for ONNX node: llama_model/layers.7/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.7/self_attn/Shape_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather_6 [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_6 [Gather] inputs: [llama_model/layers.7/self_attn/Shape_4_output_0 -> (4)[INT32]], [llama_model/layers.7/self_attn/Constant_21_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_21_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather_6 for ONNX node: llama_model/layers.7/self_attn/Gather_6\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_6_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_6_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_6 [Gather] outputs: [llama_model/layers.7/self_attn/Gather_6_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_6_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div_1 [Div] inputs: [llama_model/layers.7/self_attn/Gather_6_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_22_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Div_1 for ONNX node: llama_model/layers.7/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div_1 [Div] outputs: [llama_model/layers.7/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.7/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast_2 for ONNX node: llama_model/layers.7/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.7/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.7/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast_3 for ONNX node: llama_model/layers.7/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.7/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_23_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_25_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_26_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Constant_23_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_25_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Slice_2 for ONNX node: llama_model/layers.7/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.7/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.7/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_28_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_28_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Slice_3 for ONNX node: llama_model/layers.7/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.7/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.7/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Neg_1 for ONNX node: llama_model/layers.7/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.7/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.7/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.7/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_4 for ONNX node: llama_model/layers.7/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.7/self_attn/Concat_4_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Unsqueeze_7_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Mul_3 for ONNX node: llama_model/layers.7/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.7/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add_1 [Add] inputs: [llama_model/layers.7/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Add_1 for ONNX node: llama_model/layers.7/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add_1 [Add] outputs: [llama_model/layers.7/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.7/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Transpose_3 for ONNX node: llama_model/layers.7/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.7/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/MatMul [MatMul] inputs: [llama_model/layers.7/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/MatMul for ONNX node: llama_model/layers.7/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.7/self_attn/MatMul [MatMul] outputs: [llama_model/layers.7/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div_2 [Div] inputs: [llama_model/layers.7/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.7/self_attn/Constant_31_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_31_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_31_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Div_2 for ONNX node: llama_model/layers.7/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div_2 [Div] outputs: [llama_model/layers.7/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add_2 [Add] inputs: [llama_model/layers.7/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Add_2 for ONNX node: llama_model/layers.7/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add_2 [Add] outputs: [llama_model/layers.7/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Softmax [Softmax] inputs: [llama_model/layers.7/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Softmax for ONNX node: llama_model/layers.7/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.7/self_attn/Softmax [Softmax] outputs: [llama_model/layers.7/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.7/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast_4 for ONNX node: llama_model/layers.7/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.7/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.7/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast_5 for ONNX node: llama_model/layers.7/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.7/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.7/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.7/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/MatMul_1 for ONNX node: llama_model/layers.7/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.7/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.7/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Transpose_4 for ONNX node: llama_model/layers.7/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.7/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_1363 [Constant]\n",
      "[X] Constant_1363 [Constant] inputs: \n",
      "[X] Constant_1363 [Constant] outputs: [onnx::Unsqueeze_1637 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1637\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1637 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1365 [Constant]\n",
      "[X] Constant_1365 [Constant] inputs: \n",
      "[X] Constant_1365 [Constant] outputs: [onnx::Unsqueeze_1639 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1639\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1639 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_32_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_5 for ONNX node: llama_model/layers.7/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.7/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.7/self_attn/Concat_5_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Reshape_3 for ONNX node: llama_model/layers.7/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.7/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_1893\n",
      "[X] llama_model/layers.7/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.7/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1893 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1893 for ONNX node: onnx::MatMul_1893\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.7/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.7/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/Add [Add] inputs: [llama_model/layers.7/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/Add for ONNX node: llama_model/layers.7/Add\n",
      "[X] Registering tensor: llama_model/layers.7/Add_output_0 for ONNX tensor: llama_model/layers.7/Add_output_0\n",
      "[X] llama_model/layers.7/Add [Add] outputs: [llama_model/layers.7/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/Add_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.7/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Cast for ONNX node: llama_model/layers.7/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.7/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.7/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.7/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.7/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Pow for ONNX node: llama_model/layers.7/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.7/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.7/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.7/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.7/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.7/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.7/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.7/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.7/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Add for ONNX node: llama_model/layers.7/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.7/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.7/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.7/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.7/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.7/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.7/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.7/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.7/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Div for ONNX node: llama_model/layers.7/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.7/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.7/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Mul for ONNX node: llama_model/layers.7/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.7/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.7/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.7/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.7/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.7.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.7.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.7/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.7/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.7/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1894\n",
      "[X] llama_model/layers.7/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.7/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1894 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1894 for ONNX node: onnx::MatMul_1894\n",
      "[X] Registering layer: llama_model/layers.7/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.7/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.7/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.7/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.7/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.7/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.7/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.7/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.7/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.7/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.7/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.7/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/mlp/act_fn/Mul for ONNX node: llama_model/layers.7/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.7/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.7/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.7/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1895\n",
      "[X] llama_model/layers.7/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.7/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1895 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1895 for ONNX node: onnx::MatMul_1895\n",
      "[X] Registering layer: llama_model/layers.7/mlp/up_proj/MatMul for ONNX node: llama_model/layers.7/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.7/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/mlp/Mul [Mul] inputs: [llama_model/layers.7/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.7/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/mlp/Mul for ONNX node: llama_model/layers.7/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.7/mlp/Mul_output_0\n",
      "[X] llama_model/layers.7/mlp/Mul [Mul] outputs: [llama_model/layers.7/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_1896\n",
      "[X] llama_model/layers.7/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.7/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_1896 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1896 for ONNX node: onnx::MatMul_1896\n",
      "[X] Registering layer: llama_model/layers.7/mlp/down_proj/MatMul for ONNX node: llama_model/layers.7/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.7/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/Add_1 [Add] inputs: [llama_model/layers.7/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/Add_1 for ONNX node: llama_model/layers.7/Add_1\n",
      "[X] Registering tensor: llama_model/layers.7/Add_1_output_0 for ONNX tensor: llama_model/layers.7/Add_1_output_0\n",
      "[X] llama_model/layers.7/Add_1 [Add] outputs: [llama_model/layers.7/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/Add_1_output_0\n",
      "[X] llama_model/norm/Cast [Cast] inputs: [llama_model/layers.7/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/norm/Cast for ONNX node: llama_model/norm/Cast\n",
      "[X] Registering tensor: llama_model/norm/Cast_output_0 for ONNX tensor: llama_model/norm/Cast_output_0\n",
      "[X] llama_model/norm/Cast [Cast] outputs: [llama_model/norm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Constant [Constant]\n",
      "[X] llama_model/norm/Constant [Constant] inputs: \n",
      "[X] llama_model/norm/Constant [Constant] outputs: [llama_model/norm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/norm/Cast_output_0\n",
      "[X] Searching for input: llama_model/norm/Constant_output_0\n",
      "[X] llama_model/norm/Pow [Pow] inputs: [llama_model/norm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/norm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Constant_output_0 for ONNX node: llama_model/norm/Constant_output_0\n",
      "[X] Registering layer: llama_model/norm/Pow for ONNX node: llama_model/norm/Pow\n",
      "[X] Registering tensor: llama_model/norm/Pow_output_0 for ONNX tensor: llama_model/norm/Pow_output_0\n",
      "[X] llama_model/norm/Pow [Pow] outputs: [llama_model/norm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/norm/Pow_output_0\n",
      "[X] llama_model/norm/ReduceMean [ReduceMean] inputs: [llama_model/norm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/ReduceMean for ONNX node: llama_model/norm/ReduceMean\n",
      "[X] Registering tensor: llama_model/norm/ReduceMean_output_0 for ONNX tensor: llama_model/norm/ReduceMean_output_0\n",
      "[X] llama_model/norm/ReduceMean [ReduceMean] outputs: [llama_model/norm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Constant_1 [Constant]\n",
      "[X] llama_model/norm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/norm/Constant_1 [Constant] outputs: [llama_model/norm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Add [Add]\n",
      "[X] Searching for input: llama_model/norm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/norm/Constant_1_output_0\n",
      "[X] llama_model/norm/Add [Add] inputs: [llama_model/norm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/norm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Constant_1_output_0 for ONNX node: llama_model/norm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/norm/Add for ONNX node: llama_model/norm/Add\n",
      "[X] Registering tensor: llama_model/norm/Add_output_0 for ONNX tensor: llama_model/norm/Add_output_0\n",
      "[X] llama_model/norm/Add [Add] outputs: [llama_model/norm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/norm/Add_output_0\n",
      "[X] llama_model/norm/Sqrt [Sqrt] inputs: [llama_model/norm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Sqrt for ONNX node: llama_model/norm/Sqrt\n",
      "[X] Registering tensor: llama_model/norm/Sqrt_output_0 for ONNX tensor: llama_model/norm/Sqrt_output_0\n",
      "[X] llama_model/norm/Sqrt [Sqrt] outputs: [llama_model/norm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Constant_2 [Constant]\n",
      "[X] llama_model/norm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/norm/Constant_2 [Constant] outputs: [llama_model/norm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Div [Div]\n",
      "[X] Searching for input: llama_model/norm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/norm/Sqrt_output_0\n",
      "[X] llama_model/norm/Div [Div] inputs: [llama_model/norm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/norm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Constant_2_output_0 for ONNX node: llama_model/norm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/norm/Div for ONNX node: llama_model/norm/Div\n",
      "[X] Registering tensor: llama_model/norm/Div_output_0 for ONNX tensor: llama_model/norm/Div_output_0\n",
      "[X] llama_model/norm/Div [Div] outputs: [llama_model/norm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/norm/Cast_output_0\n",
      "[X] Searching for input: llama_model/norm/Div_output_0\n",
      "[X] llama_model/norm/Mul [Mul] inputs: [llama_model/norm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/norm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Mul for ONNX node: llama_model/norm/Mul\n",
      "[X] Registering tensor: llama_model/norm/Mul_output_0 for ONNX tensor: llama_model/norm/Mul_output_0\n",
      "[X] llama_model/norm/Mul [Mul] outputs: [llama_model/norm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/norm/Mul_output_0\n",
      "[X] llama_model/norm/Cast_1 [Cast] inputs: [llama_model/norm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/norm/Cast_1 for ONNX node: llama_model/norm/Cast_1\n",
      "[X] Registering tensor: llama_model/norm/Cast_1_output_0 for ONNX tensor: llama_model/norm/Cast_1_output_0\n",
      "[X] llama_model/norm/Cast_1 [Cast] outputs: [llama_model/norm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.norm.weight\n",
      "[X] Searching for input: llama_model/norm/Cast_1_output_0\n",
      "[X] llama_model/norm/Mul_1 [Mul] inputs: [llama_model.norm.weight -> (64)[FLOAT]], [llama_model/norm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Mul_1 for ONNX node: llama_model/norm/Mul_1\n",
      "[X] Registering tensor: llama_model/norm/Mul_1_output_0 for ONNX tensor: llama_model/norm/Mul_1_output_0\n",
      "[X] llama_model/norm/Mul_1 [Mul] outputs: [llama_model/norm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: lm_head/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/norm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_1897\n",
      "[X] lm_head/MatMul [MatMul] inputs: [llama_model/norm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_1897 -> (64, 32000)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_1897 for ONNX node: onnx::MatMul_1897\n",
      "[X] Registering layer: lm_head/MatMul for ONNX node: lm_head/MatMul\n",
      "[X] Registering tensor: logits_866 for ONNX tensor: logits\n",
      "[X] lm_head/MatMul [MatMul] outputs: [logits -> (-1, -1, 32000)[FLOAT]], \n",
      "[X] Marking logits_866 as output: logits\n",
      "[V] Setting TensorRT Optimization Profiles\n",
      "[V] Input tensor: input_ids (dtype=DataType.INT32, shape=(-1, -1)) | Setting input tensor shapes to: (min=(1, 1), opt=(1, 10), max=(1, 20))\n",
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {input_ids [min=(1, 1), opt=(1, 10), max=(1, 20)]}\n",
      "    ]\n",
      "[I] Building engine with configuration:\n",
      "    Flags                  | [FP16, TF32, OBEY_PRECISION_CONSTRAINTS]\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 3072.00 MiB, TACTIC_DRAM: 15102.06 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
      "[V] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[X] Original: 869 layers\n",
      "[X] After dead-layer removal: 869 layers\n",
      "[X] Graph construction completed in 0.0331121 seconds.\n",
      "[X] Running: IdentityToCastTransform on Identity_0\n",
      "[X] Swap the layer type of Identity_0 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_1\n",
      "[X] Swap the layer type of Identity_1 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_2\n",
      "[X] Swap the layer type of Identity_2 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_3\n",
      "[X] Swap the layer type of Identity_3 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_4\n",
      "[X] Swap the layer type of Identity_4 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_5\n",
      "[X] Swap the layer type of Identity_5 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_6\n",
      "[X] Swap the layer type of Identity_6 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_7\n",
      "[X] Swap the layer type of Identity_7 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_8\n",
      "[X] Swap the layer type of Identity_8 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_9\n",
      "[X] Swap the layer type of Identity_9 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_10\n",
      "[X] Swap the layer type of Identity_10 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_11\n",
      "[X] Swap the layer type of Identity_11 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_12\n",
      "[X] Swap the layer type of Identity_12 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_13\n",
      "[X] Swap the layer type of Identity_13 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_14\n",
      "[X] Swap the layer type of Identity_14 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_15\n",
      "[X] Swap the layer type of Identity_15 from IDENTITY to CAST\n",
      "[X] Running: ConstShuffleFusion on (Unnamed Layer* 46) [Constant]\n",
      "[X] ConstShuffleFusion: Fusing (Unnamed Layer* 46) [Constant] with (Unnamed Layer* 47) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on (Unnamed Layer* 52) [Constant]\n",
      "[X] ConstShuffleFusion: Fusing (Unnamed Layer* 52) [Constant] with (Unnamed Layer* 53) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/Constant_18_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/Constant_18_output_0 with (Unnamed Layer* 83) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/Constant_37_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/Constant_37_output_0 with (Unnamed Layer* 166) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/Constant_38_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/Constant_38_output_0 with (Unnamed Layer* 171) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/input_layernorm/Constant_output_0 with (Unnamed Layer* 177) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 181) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 185) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1689\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1689 with (Unnamed Layer* 198) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1690\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1690 with (Unnamed Layer* 201) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1691\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1691 with (Unnamed Layer* 204) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/self_attn/Constant_31_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/self_attn/Constant_31_output_0 with (Unnamed Layer* 421) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1711\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1711 with (Unnamed Layer* 437) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 442) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 446) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 450) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1712\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1712 with (Unnamed Layer* 457) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1713\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1713 with (Unnamed Layer* 462) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1714\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1714 with (Unnamed Layer* 466) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/input_layernorm/Constant_output_0 with (Unnamed Layer* 471) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 475) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 479) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1715\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1715 with (Unnamed Layer* 492) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1716\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1716 with (Unnamed Layer* 495) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1717\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1717 with (Unnamed Layer* 498) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/self_attn/Constant_31_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/self_attn/Constant_31_output_0 with (Unnamed Layer* 713) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1737\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1737 with (Unnamed Layer* 729) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 734) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 738) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 742) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1738\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1738 with (Unnamed Layer* 749) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1739\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1739 with (Unnamed Layer* 754) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1740\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1740 with (Unnamed Layer* 758) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/input_layernorm/Constant_output_0 with (Unnamed Layer* 763) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 767) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 771) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1741\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1741 with (Unnamed Layer* 784) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1742\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1742 with (Unnamed Layer* 787) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1743\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1743 with (Unnamed Layer* 790) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/self_attn/Constant_31_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/self_attn/Constant_31_output_0 with (Unnamed Layer* 1005) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1763\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1763 with (Unnamed Layer* 1021) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 1026) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 1030) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 1034) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1764\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1764 with (Unnamed Layer* 1041) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1765\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1765 with (Unnamed Layer* 1046) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1766\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1766 with (Unnamed Layer* 1050) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/input_layernorm/Constant_output_0 with (Unnamed Layer* 1055) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 1059) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 1063) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1767\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1767 with (Unnamed Layer* 1076) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1768\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1768 with (Unnamed Layer* 1079) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1769\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1769 with (Unnamed Layer* 1082) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/self_attn/Constant_31_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/self_attn/Constant_31_output_0 with (Unnamed Layer* 1297) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1789\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1789 with (Unnamed Layer* 1313) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 1318) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 1322) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 1326) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1790\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1790 with (Unnamed Layer* 1333) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1791\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1791 with (Unnamed Layer* 1338) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1792\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1792 with (Unnamed Layer* 1342) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/input_layernorm/Constant_output_0 with (Unnamed Layer* 1347) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 1351) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 1355) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1793\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1793 with (Unnamed Layer* 1368) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1794\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1794 with (Unnamed Layer* 1371) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1795\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1795 with (Unnamed Layer* 1374) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/self_attn/Constant_31_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/self_attn/Constant_31_output_0 with (Unnamed Layer* 1589) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1815\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1815 with (Unnamed Layer* 1605) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 1610) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 1614) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 1618) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1816\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1816 with (Unnamed Layer* 1625) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1817\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1817 with (Unnamed Layer* 1630) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1818\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1818 with (Unnamed Layer* 1634) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/input_layernorm/Constant_output_0 with (Unnamed Layer* 1639) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 1643) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 1647) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1819\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1819 with (Unnamed Layer* 1660) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1820\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1820 with (Unnamed Layer* 1663) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1821\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1821 with (Unnamed Layer* 1666) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/self_attn/Constant_31_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/self_attn/Constant_31_output_0 with (Unnamed Layer* 1881) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1841\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1841 with (Unnamed Layer* 1897) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 1902) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 1906) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 1910) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1842\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1842 with (Unnamed Layer* 1917) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1843\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1843 with (Unnamed Layer* 1922) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1844\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1844 with (Unnamed Layer* 1926) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/input_layernorm/Constant_output_0 with (Unnamed Layer* 1931) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 1935) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 1939) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1845\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1845 with (Unnamed Layer* 1952) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1846\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1846 with (Unnamed Layer* 1955) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1847\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1847 with (Unnamed Layer* 1958) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/self_attn/Constant_31_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/self_attn/Constant_31_output_0 with (Unnamed Layer* 2173) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1867\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1867 with (Unnamed Layer* 2189) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 2194) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 2198) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 2202) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1868\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1868 with (Unnamed Layer* 2209) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1869\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1869 with (Unnamed Layer* 2214) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1870\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1870 with (Unnamed Layer* 2218) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/input_layernorm/Constant_output_0 with (Unnamed Layer* 2223) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 2227) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 2231) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1871\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1871 with (Unnamed Layer* 2244) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1872\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1872 with (Unnamed Layer* 2247) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1873\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1873 with (Unnamed Layer* 2250) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/self_attn/Constant_31_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/self_attn/Constant_31_output_0 with (Unnamed Layer* 2465) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1893\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1893 with (Unnamed Layer* 2481) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 2486) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 2490) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 2494) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1894\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1894 with (Unnamed Layer* 2501) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1895\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1895 with (Unnamed Layer* 2506) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1896\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1896 with (Unnamed Layer* 2510) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/norm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/norm/Constant_output_0 with (Unnamed Layer* 2515) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/norm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/norm/Constant_1_output_0 with (Unnamed Layer* 2519) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/norm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/norm/Constant_2_output_0 with (Unnamed Layer* 2523) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_1897\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_1897 with (Unnamed Layer* 2530) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/Unsqueeze_12\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/Unsqueeze_12 with llama_model/Unsqueeze_13\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/Unsqueeze\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/Unsqueeze with llama_model/Reshape\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Reshape with llama_model/layers.0/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Reshape_1 with llama_model/layers.0/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Reshape_2 with llama_model/layers.0/self_attn/Transpose_2\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/Unsqueeze_7\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/Unsqueeze_7 with llama_model/Unsqueeze_8\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 425) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 425) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Transpose_4 with llama_model/layers.0/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Reshape with llama_model/layers.1/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Reshape_1 with llama_model/layers.1/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Reshape_2 with llama_model/layers.1/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 717) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 717) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Transpose_4 with llama_model/layers.1/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Reshape with llama_model/layers.2/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Reshape_1 with llama_model/layers.2/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Reshape_2 with llama_model/layers.2/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 1009) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 1009) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Transpose_4 with llama_model/layers.2/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Reshape with llama_model/layers.3/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Reshape_1 with llama_model/layers.3/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Reshape_2 with llama_model/layers.3/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 1301) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 1301) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Transpose_4 with llama_model/layers.3/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.4/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.4/self_attn/Reshape with llama_model/layers.4/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.4/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.4/self_attn/Reshape_1 with llama_model/layers.4/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.4/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.4/self_attn/Reshape_2 with llama_model/layers.4/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 1593) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 1593) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.4/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.4/self_attn/Transpose_4 with llama_model/layers.4/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.5/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.5/self_attn/Reshape with llama_model/layers.5/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.5/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.5/self_attn/Reshape_1 with llama_model/layers.5/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.5/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.5/self_attn/Reshape_2 with llama_model/layers.5/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 1885) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 1885) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.5/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.5/self_attn/Transpose_4 with llama_model/layers.5/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.6/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.6/self_attn/Reshape with llama_model/layers.6/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.6/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.6/self_attn/Reshape_1 with llama_model/layers.6/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.6/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.6/self_attn/Reshape_2 with llama_model/layers.6/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 2177) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 2177) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.6/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.6/self_attn/Transpose_4 with llama_model/layers.6/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.7/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.7/self_attn/Reshape with llama_model/layers.7/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.7/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.7/self_attn/Reshape_1 with llama_model/layers.7/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.7/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.7/self_attn/Reshape_2 with llama_model/layers.7/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 2469) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 2469) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.7/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.7/self_attn/Transpose_4 with llama_model/layers.7/self_attn/Reshape_3\n",
      "[X] After Myelin optimization: 1 layers\n",
      "[X] Applying ScaleNodes fusions.\n",
      "[X] After scale fusion: 1 layers\n",
      "[X] After dupe layer removal: 1 layers\n",
      "[X] After final dead-layer removal: 1 layers\n",
      "[X] After tensor merging: 1 layers\n",
      "[X] After vertical fusions: 1 layers\n",
      "[X] After dupe layer removal: 1 layers\n",
      "[X] After final dead-layer removal: 1 layers\n",
      "[X] After tensor merging: 1 layers\n",
      "[X] After slice removal: 1 layers\n",
      "[X] After concat removal: 1 layers\n",
      "[X] Trying to split Reshape and strided tensor\n",
      "[V] Graph optimization time: 0.0838369 seconds.\n",
      "[V] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[X] Building graph using backend strategy 2\n",
      "[V] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[X] Constructing optimization profile number 0 [1/1].\n",
      "[X] Applying generic optimizations to the graph for inference.\n",
      "[X] 0 requirements combinations were removed due to type constraints.\n",
      "[X] Reserving memory for host IO tensors. Host: 0 bytes\n",
      "[X] =============== Computing costs for {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]}\n",
      "[X] *************** Autotuning format combination: Int32(sequence,1) -> Float((* 32000 sequence),32000,1) ***************\n",
      "[X] --------------- Timing Runner: {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023])\n",
      "[X]  (foreignNode) Set user's cuda kernel library\n",
      "[X] Tactic: 0x0000000000000000 Time: 0.492992\n",
      "[X] {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023]) profiling completed in 10.09 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.492992\n",
      "[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000\n",
      "[X] *************** Autotuning format combination: Int32(sequence,1) -> Half((* 32000 sequence),32000,1) ***************\n",
      "[X] --------------- Timing Runner: {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023])\n",
      "[X]  (foreignNode) Set user's cuda kernel library\n",
      "[X] Tactic: 0x0000000000000000 Time: 0.425547\n",
      "[X] {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023]) profiling completed in 10.3495 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.425547\n",
      "[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000\n",
      "[X] =============== Computing reformatting costs\n",
      "[X] =============== Computing reformatting costs\n",
      "[X] =============== Computing reformatting costs: \n",
      "[X] *************** Autotuning Reformat: Float((* 32000 sequence),32000,1) -> Half((* 32000 sequence),32000,1) ***************\n",
      "[X] --------------- Timing Runner: Optimizer Reformat(<in> -> logits) (Reformat[0x80000006])\n",
      "[X] Tactic: 0x00000000000003e8 Time: 0.00410224\n",
      "[X] Tactic: 0x00000000000003ea Time: 0.0100599\n",
      "[X] Tactic: 0x0000000000000000 Time: 0.0102135\n",
      "[X] Optimizer Reformat(<in> -> logits) (Reformat[0x80000006]) profiling completed in 0.0408193 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00410224\n",
      "[X] Formats and tactics selection completed in 20.5153 seconds.\n",
      "[X] After reformat layers: 1 layers\n",
      "[X] Total number of blocks in pre-optimized block assignment: 1\n",
      "[V] Detected 1 inputs and 1 output network tensors.\n",
      "[X] Layer: {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]} Host Persistent: 32 Device Persistent: 0 Scratch Memory: 33280\n",
      "[X] Skipped printing memory information for 0 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.\n",
      "[V] Total Host Persistent Memory: 32\n",
      "[V] Total Device Persistent Memory: 0\n",
      "[V] Total Scratch Memory: 33280\n",
      "[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 8 MiB, GPU 20 MiB\n",
      "[V] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.\n",
      "[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.023939ms to assign 1 blocks to 1 nodes requiring 33280 bytes.\n",
      "[X] Total number of blocks in optimized block assignment: 1\n",
      "[V] Total Activation Memory: 33280\n",
      "[X] Total number of generated kernels selected for the engine: 0\n",
      "[X] Disabling unused tactic source: EDGE_MASK_CONVOLUTIONS\n",
      "[X] Disabling unused tactic source: JIT_CONVOLUTIONS\n",
      "[X] Engine generation completed in 20.8164 seconds.\n",
      "[W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[W] Check verbose logs for the list of affected weights.\n",
      "[W] - 75 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[X]   List of affected weights: llama_model/layers.0/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 181) [Shuffle], llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 446) [Shuffle], llama_model/layers.1/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 475) [Shuffle], llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 738) [Shuffle], llama_model/layers.2/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 767) [Shuffle], llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 1030) [Shuffle], llama_model/layers.3/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 1059) [Shuffle], llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 1322) [Shuffle], llama_model/layers.4/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 1351) [Shuffle], llama_model/layers.4/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 1614) [Shuffle], llama_model/layers.5/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 1643) [Shuffle], llama_model/layers.5/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 1906) [Shuffle], llama_model/layers.6/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 1935) [Shuffle], llama_model/layers.6/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 2198) [Shuffle], llama_model/layers.7/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 2227) [Shuffle], llama_model/layers.7/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 2490) [Shuffle], llama_model/norm/Constant_1_output_0 + (Unnamed Layer* 2519) [Shuffle], llama_model_embed_tokens_weight_constant, onnx__MatMul_1689 _ (Unnamed Layer_ 198) [Shuffle]_constant, onnx__MatMul_1690 _ (Unnamed Layer_ 201) [Shuffle]_constant, onnx__MatMul_1691 _ (Unnamed Layer_ 204) [Shuffle]_constant, onnx__MatMul_1711 _ (Unnamed Layer_ 437) [Shuffle]_constant, onnx__MatMul_1712 _ (Unnamed Layer_ 457) [Shuffle]_constant, onnx__MatMul_1713 _ (Unnamed Layer_ 462) [Shuffle]_constant, onnx__MatMul_1714 _ (Unnamed Layer_ 466) [Shuffle]_constant, onnx__MatMul_1715 _ (Unnamed Layer_ 492) [Shuffle]_constant, onnx__MatMul_1716 _ (Unnamed Layer_ 495) [Shuffle]_constant, onnx__MatMul_1717 _ (Unnamed Layer_ 498) [Shuffle]_constant, onnx__MatMul_1737 _ (Unnamed Layer_ 729) [Shuffle]_constant, onnx__MatMul_1738 _ (Unnamed Layer_ 749) [Shuffle]_constant, onnx__MatMul_1739 _ (Unnamed Layer_ 754) [Shuffle]_constant, onnx__MatMul_1740 _ (Unnamed Layer_ 758) [Shuffle]_constant, onnx__MatMul_1741 _ (Unnamed Layer_ 784) [Shuffle]_constant, onnx__MatMul_1742 _ (Unnamed Layer_ 787) [Shuffle]_constant, onnx__MatMul_1743 _ (Unnamed Layer_ 790) [Shuffle]_constant, onnx__MatMul_1763 _ (Unnamed Layer_ 1021) [Shuffle]_constant, onnx__MatMul_1764 _ (Unnamed Layer_ 1041) [Shuffle]_constant, onnx__MatMul_1765 _ (Unnamed Layer_ 1046) [Shuffle]_constant, onnx__MatMul_1766 _ (Unnamed Layer_ 1050) [Shuffle]_constant, onnx__MatMul_1767 _ (Unnamed Layer_ 1076) [Shuffle]_constant, onnx__MatMul_1768 _ (Unnamed Layer_ 1079) [Shuffle]_constant, onnx__MatMul_1769 _ (Unnamed Layer_ 1082) [Shuffle]_constant, onnx__MatMul_1789 _ (Unnamed Layer_ 1313) [Shuffle]_constant, onnx__MatMul_1790 _ (Unnamed Layer_ 1333) [Shuffle]_constant, onnx__MatMul_1791 _ (Unnamed Layer_ 1338) [Shuffle]_constant, onnx__MatMul_1792 _ (Unnamed Layer_ 1342) [Shuffle]_constant, onnx__MatMul_1793 _ (Unnamed Layer_ 1368) [Shuffle]_constant, onnx__MatMul_1794 _ (Unnamed Layer_ 1371) [Shuffle]_constant, onnx__MatMul_1795 _ (Unnamed Layer_ 1374) [Shuffle]_constant, onnx__MatMul_1815 _ (Unnamed Layer_ 1605) [Shuffle]_constant, onnx__MatMul_1816 _ (Unnamed Layer_ 1625) [Shuffle]_constant, onnx__MatMul_1817 _ (Unnamed Layer_ 1630) [Shuffle]_constant, onnx__MatMul_1818 _ (Unnamed Layer_ 1634) [Shuffle]_constant, onnx__MatMul_1819 _ (Unnamed Layer_ 1660) [Shuffle]_constant, onnx__MatMul_1820 _ (Unnamed Layer_ 1663) [Shuffle]_constant, onnx__MatMul_1821 _ (Unnamed Layer_ 1666) [Shuffle]_constant, onnx__MatMul_1841 _ (Unnamed Layer_ 1897) [Shuffle]_constant, onnx__MatMul_1842 _ (Unnamed Layer_ 1917) [Shuffle]_constant, onnx__MatMul_1843 _ (Unnamed Layer_ 1922) [Shuffle]_constant, onnx__MatMul_1844 _ (Unnamed Layer_ 1926) [Shuffle]_constant, onnx__MatMul_1845 _ (Unnamed Layer_ 1952) [Shuffle]_constant, onnx__MatMul_1846 _ (Unnamed Layer_ 1955) [Shuffle]_constant, onnx__MatMul_1847 _ (Unnamed Layer_ 1958) [Shuffle]_constant, onnx__MatMul_1867 _ (Unnamed Layer_ 2189) [Shuffle]_constant, onnx__MatMul_1868 _ (Unnamed Layer_ 2209) [Shuffle]_constant, onnx__MatMul_1869 _ (Unnamed Layer_ 2214) [Shuffle]_constant, onnx__MatMul_1870 _ (Unnamed Layer_ 2218) [Shuffle]_constant, onnx__MatMul_1871 _ (Unnamed Layer_ 2244) [Shuffle]_constant, onnx__MatMul_1872 _ (Unnamed Layer_ 2247) [Shuffle]_constant, onnx__MatMul_1873 _ (Unnamed Layer_ 2250) [Shuffle]_constant, onnx__MatMul_1893 _ (Unnamed Layer_ 2481) [Shuffle]_constant, onnx__MatMul_1894 _ (Unnamed Layer_ 2501) [Shuffle]_constant, onnx__MatMul_1895 _ (Unnamed Layer_ 2506) [Shuffle]_constant, onnx__MatMul_1896 _ (Unnamed Layer_ 2510) [Shuffle]_constant, onnx__MatMul_1897 _ (Unnamed Layer_ 2530) [Shuffle]_constant\n",
      "[W] - 3 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "[X]   List of affected weights: llama_model_embed_tokens_weight_constant, onnx__MatMul_1714 _ (Unnamed Layer_ 466) [Shuffle]_constant, onnx__MatMul_1897 _ (Unnamed Layer_ 2530) [Shuffle]_constant\n",
      "[W] - 2 weights are affected by this issue: Detected finite FP32 values which would overflow in FP16 and converted them to the closest finite FP16 value.\n",
      "[X]   List of affected weights: (Unnamed Layer* 52) [Constant] + (Unnamed Layer* 53) [Shuffle], llama_model/Constant_38_output_0 + (Unnamed Layer* 171) [Shuffle]\n",
      "[X] Engine Layer Information:\n",
      "    Layer(Myelin): {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]}, Tactic: 0x0000000000000000, input_ids (Int32[1,-1]) -> logits (Half[1,-1,32000])\n",
      "[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +16, now: CPU 0, GPU 16 (MiB)\n",
      "[X] Adding 1 engine(s) to plan file.\n",
      "[I] Finished engine building in 20.957 seconds\n",
      "[X] Deleting timing cache: 1 entries, served 0 hits since creation.\n",
      "[X] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1\n",
      "[X] Plugin creator already registered - ::BatchedNMS_TRT version 1\n",
      "[X] Plugin creator already registered - ::BatchTilePlugin_TRT version 1\n",
      "[X] Plugin creator already registered - ::Clip_TRT version 1\n",
      "[X] Plugin creator already registered - ::CoordConvAC version 1\n",
      "[X] Plugin creator already registered - ::CropAndResizeDynamic version 1\n",
      "[X] Plugin creator already registered - ::CropAndResize version 1\n",
      "[X] Plugin creator already registered - ::DecodeBbox3DPlugin version 1\n",
      "[X] Plugin creator already registered - ::DetectionLayer_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_TRT version 1\n",
      "[X] Plugin creator already registered - ::FlattenConcat_TRT version 1\n",
      "[X] Plugin creator already registered - ::GenerateDetection_TRT version 1\n",
      "[X] Plugin creator already registered - ::GridAnchor_TRT version 1\n",
      "[X] Plugin creator already registered - ::GridAnchorRect_TRT version 1\n",
      "[X] Plugin creator already registered - ::InstanceNormalization_TRT version 1\n",
      "[X] Plugin creator already registered - ::InstanceNormalization_TRT version 2\n",
      "[X] Plugin creator already registered - ::LReLU_TRT version 1\n",
      "[X] Plugin creator already registered - ::ModulatedDeformConv2d version 1\n",
      "[X] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1\n",
      "[X] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1\n",
      "[X] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[X] Plugin creator already registered - ::NMSDynamic_TRT version 1\n",
      "[X] Plugin creator already registered - ::NMS_TRT version 1\n",
      "[X] Plugin creator already registered - ::Normalize_TRT version 1\n",
      "[X] Plugin creator already registered - ::PillarScatterPlugin version 1\n",
      "[X] Plugin creator already registered - ::PriorBox_TRT version 1\n",
      "[X] Plugin creator already registered - ::ProposalDynamic version 1\n",
      "[X] Plugin creator already registered - ::ProposalLayer_TRT version 1\n",
      "[X] Plugin creator already registered - ::Proposal version 1\n",
      "[X] Plugin creator already registered - ::PyramidROIAlign_TRT version 1\n",
      "[X] Plugin creator already registered - ::Region_TRT version 1\n",
      "[X] Plugin creator already registered - ::Reorg_TRT version 1\n",
      "[X] Plugin creator already registered - ::ResizeNearest_TRT version 1\n",
      "[X] Plugin creator already registered - ::ROIAlign_TRT version 1\n",
      "[X] Plugin creator already registered - ::RPROI_TRT version 1\n",
      "[X] Plugin creator already registered - ::ScatterND version 1\n",
      "[X] Plugin creator already registered - ::SpecialSlice_TRT version 1\n",
      "[X] Plugin creator already registered - ::Split version 1\n",
      "[X] Plugin creator already registered - ::VoxelGeneratorPlugin version 1\n",
      "[V] Loaded engine size: 10 MiB\n",
      "[X] Deserialization required 6311 microseconds.\n",
      "[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +8, now: CPU 0, GPU 8 (MiB)\n",
      "[X] Adding 1 engine(s) to plan file.\n",
      "[I] Saving engine to ./models/llama-xs/trt-engine/llama-xs-bs1.engine\n"
     ]
    }
   ],
   "source": [
    "engine_path = os.path.join(trt_engine_folder, f\"{model_name}-{engine_tag}.engine\")\n",
    "\n",
    "if os.path.exists(engine_path):\n",
    "    os.remove(engine_path)\n",
    "\n",
    "if not os.path.exists(engine_path):\n",
    "    gpt2_engine = GPT2ONNXFile(onnx_path, metadata).as_trt_engine(output_fpath=engine_path, profiles=profiles, preview_features=preview_features)\n",
    "else:\n",
    "    gpt2_engine = GPT2TRTEngine(engine_path, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82d909c9-6783-4a92-8ffd-7b0d598d8daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GPT2.export.GPT2TRTEngine at 0x7fd5eff04f70>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9ef7153-808d-4bd0-90c9-ebc44b1eb6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.523076"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(engine_path).stat().st_size / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3378c4dc-ad06-41ae-81ed-29cf7cfc533e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {},
   "source": [
    "### Inference with TensorRT engine\n",
    "\n",
    "Great, if you have reached this stage, it means we now have an optimized TensorRT engine for the GPT-2 model, ready for us to carry out inference. \n",
    "\n",
    "The GPT-2 model with TensorRT backend can now be employed in place of the original HuggingFace GPT-2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae13aa-bf6f-4eb7-a453-389865562ae4",
   "metadata": {},
   "source": [
    "#### Single batch inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ed7e6-b893-4bf6-b31b-b029a07f899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport Llama\n",
    "%aimport Llama.trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30585cb4-1d08-43d3-84c6-ed0757dbc6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b58f1-3d9f-4844-85c9-73058bd36a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Llama.trt import GPT2TRTDecoder\n",
    "from Llama.measurements import gpt2_inference, full_inference\n",
    "from NNDF.networks import TimingProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfda583-b684-48b1-9046-15ab022ef982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2_trt = GPT2TRTDecoder(gpt2_engine, metadata, xs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc60ad-73a7-46df-85d7-a292a8abbd80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Benchmarking TensorRT performance on single batch\n",
    "_, decoder_e2e_median_time = gpt2_inference(\n",
    "            gpt2_trt, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    "        )\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d86d29-1c7b-4020-9ef2-b77ea5e52764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = gpt2_trt(input_ids=input_ids)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e0162-c9eb-473d-ace6-c4c61ff578b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22122064-5a17-4990-bd6b-073fca5a3e9b",
   "metadata": {},
   "source": [
    "#### Open-end text generation\n",
    "Let's generate the same task again. Since GPT-2 is an open-ended model, a small turbulent in the model might have a very different result. Since we have done some format changes and input/output restriction while exporting the model, you might see a different result compared to raw HuggingFace model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bffb8-a7a4-4fcb-91c9-f4e9f7263e6c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sample_output = gpt2_trt.generate(input_ids.cuda(), max_length=max_length)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b3124-cefa-4470-90cc-66a94ef868ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = gpt2_trt.generate(input_ids.cuda(), max_length=max_length)\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068bdb2d-e59d-4ef1-8d5e-3c82931af03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_trt.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8bc4c-bf3e-4cb5-afc6-c0bd7d8655cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get complete decoder inference result and its timing profile\n",
    "# _, full_e2e_median_runtime = full_inference(\n",
    "#     gpt2_trt, input_ids.cuda(), tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "#     max_length=max_length\n",
    "# )\n",
    "# full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68a915-2c32-49e5-b1f6-e93d7618f637",
   "metadata": {},
   "source": [
    "You can now compare the output of the original PyTorch model and the TensorRT engine. Notice the speed difference. On an NVIDIA V100 32GB GPU, this results in about ~5x performance improvement for the GPT-2 model (from an average of 0.704s to 0.134s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2562388-d97b-45dd-8569-3f6c053f4e98",
   "metadata": {},
   "source": [
    "Now you have known how to convert a model to onnx, build TRT engine and optimize it. As you might have recalled, using kv cache and beam search are two important ways to improve the performance of the decoder models. We have recently added thse support to our HuggingFace demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4132e54-aba7-42ec-8324-c68d82c17296",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. Advanced Topic: KV Cache\n",
    "\n",
    "As you have seen above, we put `use_cache = False` in some code blocks. This is because in the simplified model, we only take `input_ids` as input and `logits` as output. `input_ids` is growing as the sequence goes longer. In reality, we sometimes cache the self-attentions for each layer and reuse them in the later computations. This allows us to only take the last generated `input_ids`. This is a trade-off between space and time. When the model is small or the sequence is small, the D2D data copy time usually outweights the performance improvement of the model. However, performance improvements have been found in larger models with larger sequence length like 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d1dcb-250f-4d86-9726-b114d4962fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "kv_config = GPT2Config.from_pretrained(GPT2_VARIANT, use_cache = use_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fdf0f-2da0-46c0-a948-e4e6e16b898a",
   "metadata": {},
   "source": [
    "#### Raw HuggingFace\n",
    "\n",
    "The model that we download from `GPT2LMHeadModel.from_pretrained` is dynamic in its inputs. It can take both kv and non-kv configurations. Changing `use_cache` will do it. You can see that changing this configuration, the output is changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3c51a-07ee-4936-b620-50766a45b945",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    model, input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, use_cache = use_cache\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14607bf-f449-4151-9076-d099ae1a3ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_output = model.generate(input_ids, max_length=max_length, use_cache = use_cache)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057ef83-0cdc-4631-9958-66d04fc7fc22",
   "metadata": {},
   "source": [
    "#### TensorRT\n",
    "\n",
    "For the 1st decoding step, we take `input_ids` and generate both `logits` and the kv cache. In other steps, we take the new `input_ids` with `past` kv-cache and the outputs are `logits` and the updated `present` kv-cache. Taking dynamic number of inputs for trt is not currently supported in our demo, so we need to output 2 onnx files and build 2 engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbcfad-9c9c-47e2-894a-731c7a3a04df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_metadata = NetworkMetadata(variant=GPT2_VARIANT, precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=use_cache))\n",
    "kv_gpt2 = GPT2TorchFile(model.to('cpu'), kv_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe680c5-d9ff-466f-87fe-a7bb0cbee944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_onnx_path = ('./models/{}/ONNX/{}-kv_cache.onnx'.format(GPT2_VARIANT, GPT2_VARIANT))\n",
    "kv_gpt2.as_onnx_model(kv_onnx_path, force_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f6824-286d-4afa-926b-7eed4cafafc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_onnx_model = onnx.load(kv_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f0012-7a2d-41be-a8d8-c818dcb7c244",
   "metadata": {},
   "source": [
    "We could see that the kv model has #inputs = #outputs = num_layers * 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579aeec-2c7a-43de-b8f7-beff8d3d7784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(kv_onnx_model.graph.input), len(kv_onnx_model.graph.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add1139-aab0-4531-b2ac-c3aca90e5d49",
   "metadata": {},
   "source": [
    "The next blocks will set up the profile and build the engine. The only difference is that we now have the profile for kv cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae055cb-41b7-4523-86bc-490bc9edf204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "disable_preview_dynamic_shapes = False\n",
    "\n",
    "engine_tag = \"bs{}\".format(batch_size)\n",
    "\n",
    "preview_features = [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-disableFasterDynamicShapes\"\n",
    "    preview_features = []\n",
    "\n",
    "use_input_length = False\n",
    "num_heads = kv_config.n_head\n",
    "embedding_size_per_head = kv_config.n_embd // num_heads\n",
    "num_layers = kv_config.n_layer\n",
    "\n",
    "max_sequence_length = max_length\n",
    "max_output_length = max_length\n",
    "if not use_input_length:\n",
    "    opt_input_seq_len = max_sequence_length // 2\n",
    "else:\n",
    "    opt_input_seq_len = input_ids.shape[1]\n",
    "\n",
    "opt_output_seq_len = max_output_length // 2\n",
    "\n",
    "# context phase uses the provided input_ids to generate hidden states and self attention kv cache\n",
    "# It is only used in the 1st decoder run.\n",
    "dec_profiles_context = Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, opt_output_seq_len),\n",
    "    max=(batch_size, max_output_length),\n",
    ")\n",
    "self_attention_profile_context = {\n",
    "    \"min\": (batch_size, num_heads, 0, embedding_size_per_head),\n",
    "    \"opt\": (batch_size, num_heads, 0, embedding_size_per_head),\n",
    "    \"max\": (batch_size, num_heads, 0, embedding_size_per_head),\n",
    "}\n",
    "\n",
    "# generation phase uses previous self attention kv cache with the last input_ids token to generate the next hidden states and self attention kv cache\n",
    "# This optimization profile is used after the 1st decoder run.\n",
    "dec_profiles_generation = Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, 1),\n",
    "    max=(batch_size, 1),\n",
    ")\n",
    "\n",
    "self_attention_profile_generation = {\n",
    "    \"min\": (batch_size, num_heads, 1, embedding_size_per_head),\n",
    "    \"opt\": (batch_size, num_heads, opt_output_seq_len - 1, embedding_size_per_head),\n",
    "    \"max\": (batch_size, num_heads, max_output_length - 1, embedding_size_per_head),\n",
    "}\n",
    "\n",
    "for i in range(num_layers):\n",
    "    dec_profiles_context = dec_profiles_context.add(\n",
    "        f\"past_key_values.{i}.decoder.key\",\n",
    "        **self_attention_profile_context\n",
    "    ).add(\n",
    "        f\"past_key_values.{i}.decoder.value\",\n",
    "        **self_attention_profile_context\n",
    "    )\n",
    "\n",
    "    dec_profiles_generation = dec_profiles_generation.add(\n",
    "        f\"past_key_values.{i}.decoder.key\",\n",
    "        **self_attention_profile_generation\n",
    "    ).add(\n",
    "        f\"past_key_values.{i}.decoder.value\",\n",
    "        **self_attention_profile_generation\n",
    "    )\n",
    "\n",
    "# TensorRT accepts multiple optimization engines for the same model.\n",
    "# Profile 1 is only used in the first decoder iterations.\n",
    "decoder_profiles = [dec_profiles_generation, dec_profiles_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadf843-9f60-41c7-90a9-098b33ce3603",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_engine_path = os.path.join(trt_engine_folder, f\"{GPT2_VARIANT}-kv_cache_{engine_tag}.engine\")\n",
    "\n",
    "# Set up the trt engine with both kv input/output augmented\n",
    "if not os.path.exists(kv_engine_path):\n",
    "    kv_gpt2_engine = GPT2ONNXFile(kv_onnx_path, kv_metadata).as_trt_engine(kv_engine_path,profiles=decoder_profiles, preview_features=preview_features)\n",
    "else:\n",
    "    kv_gpt2_engine = GPT2TRTEngine(kv_engine_path, kv_metadata)\n",
    "\n",
    "    \n",
    "kv_gpt2_trt = GPT2TRTDecoder(\n",
    "    kv_gpt2_engine, kv_metadata, kv_config, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090007db-9a09-4b6d-95ed-8a688ea05798",
   "metadata": {},
   "source": [
    "Since we have 2 profiles, benchmarking single-run runtime does not make sense. We instead use `full_inference` to measure the time for the entire inference cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b93d88-21bb-4f87-9ff6-709d0babdf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    kv_gpt2_trt, input_ids.cuda(), tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, use_cache = use_cache\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ab217-9ee4-435c-b689-69d98cef1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_gpt2_trt.reset()\n",
    "kv_sample_output = kv_gpt2_trt.generate(input_ids.cuda(), max_length=max_length)\n",
    "tokenizer.decode(kv_sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b614fb8-63d6-4711-84cf-c69ca8b3f141",
   "metadata": {},
   "source": [
    "In this short example, kv cache performance does not improve the performance, and may even be slightly worse than non kv cache mode. However, when we have larger input sequences for the model, it will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764049f-0578-4305-b010-4e7a3156a377",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "\n",
    "## 5. Advanced Topic: Beam Search\n",
    "\n",
    "Beam search is a way to increase the model quality. It looks for the top `num_beams` number of possible words and pick the one that conditions the best to the current position. Similarly, the original HuggingFace PyTorch model supports beam search natively, while we need to build separate trt engine for different `num_beams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5808db-2cc0-4d88-aebe-1b6e17a023e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_config = GPT2Config.from_pretrained(GPT2_VARIANT, use_cache = False)\n",
    "beam_metadata = NetworkMetadata(variant=GPT2_VARIANT, precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=False))\n",
    "num_beams = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1403609-b24d-4e10-a8eb-852d3eab6fa0",
   "metadata": {},
   "source": [
    "#### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd992c8-1eeb-427c-ae32-2c63766c6a69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    model, input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, num_beams = num_beams\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09418760-84bd-4308-b06b-8540945a6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(input_ids, max_length=max_length, num_beams = num_beams)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8d9fa-d74a-40dd-94ce-d98551d24608",
   "metadata": {},
   "source": [
    "You could see that the output is very different from the original one. If you change `num_beams`, the result will also change significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba01e0ec-68ad-4682-8ca4-2ecde7d70f7f",
   "metadata": {},
   "source": [
    "#### TensorRT\n",
    "It uses the same onnx file as the original configuration, but the engine set up is differently, because it expands the inputs by `num_beams` for the first dimension of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055fb314-8e0f-4edd-bf78-16890d196de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimization profile for dynamic shape input. Can modify batch_size / max_sequence_length to build engines for different shapes\n",
    "batch_size = 1\n",
    "disable_preview_dynamic_shapes = False # preview_dynamic_shapes optimizes the trt engine building time\n",
    "# We can either use input length as the optimal length, or use max_length // 2. \n",
    "# In T5 or BART, input_length is better, but in GPT-2, max_length // 2 is better because we need to generate max_length number of tokens\n",
    "\n",
    "use_input_length = False\n",
    "opt_length = input_id.shape[1] if use_input_length else max_length // 2 \n",
    "# Create different engine tags for different configurations\n",
    "engine_tag = f\"bs{batch_size}-beam{num_beams}\"\n",
    "\n",
    "preview_features = [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-disableFasterDynamicShapes\"\n",
    "    preview_features = []\n",
    "    \n",
    "\n",
    "beam_profiles = [Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, opt_length), # Optimized based on the inputs. \n",
    "    max=(batch_size * num_beams, max_length),\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18986d0f-9509-463f-a489-a76dd4d28a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd04a7b-8aa6-4c97-8d85-96f14b06abbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beam_engine_path = os.path.join(trt_engine_folder, f\"{GPT2_VARIANT}-{engine_tag}.engine\")\n",
    "if not os.path.exists(beam_engine_path):\n",
    "    beam_gpt2_engine = GPT2ONNXFile(onnx_path, beam_metadata).as_trt_engine(output_fpath=beam_engine_path, profiles=beam_profiles, preview_features=preview_features)\n",
    "else:\n",
    "    beam_gpt2_engine = GPT2TRTEngine(beam_engine_path, beam_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe1dba-4e84-478e-9ea7-07c21856e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_gpt2_trt = GPT2TRTDecoder(beam_gpt2_engine, beam_metadata, beam_config, num_beams = num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42614e14-c962-4c31-a469-7e0343efbdbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    beam_gpt2_trt, input_ids.cuda(), tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, num_beams=num_beams\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ab05a-fe0d-42c3-9591-605ddab389ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_sample_output = beam_gpt2_trt.generate(input_ids.cuda(), max_length=max_length, num_beams=num_beams)\n",
    "tokenizer.decode(beam_sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543dbfd-4650-46f5-8f77-587dcb05785a",
   "metadata": {},
   "source": [
    "We could see that because of larger batch size, beam search will take slightly longer, but for most sequences, it will generate more meaningful outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc6c04-ca47-4fc6-9a12-ed500722bb4a",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch GPT-2 model to an optimized TensorRT engine for inference in 3 easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace GPT-2 model while providing significant speed up. \n",
    "\n",
    "If you are interested in further details of the conversion process, check out [GPT2/trt.py](../GPT2/trt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14079b8f-738e-4137-9ca3-6a4254e8f006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
