{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66562c2d-1586-4830-be91-e0d7a1ccd10c",
   "metadata": {},
   "source": [
    "# Test Llama 7b model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28559219-9845-4c04-9fac-5e0e71d199d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# import transformers\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f0b9c1-0c66-44e8-9c74-fe3cabc1cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf21a80-97df-4df7-8ea1-41b1d652b217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"codellama/CodeLlama-7b-hf\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176895eb-538e-4586-9d29-377cb70609c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# sequences = pipeline(\n",
    "#     'def fibonacci(',\n",
    "#     do_sample=True,\n",
    "#     temperature=0.2,\n",
    "#     top_p=0.9,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     max_length=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abaa0553-b2ed-46ab-a636-beb36a062854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37c46b-5b0c-48d6-a2ff-7ed85dc080fb",
   "metadata": {},
   "source": [
    "# Test small xs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce23e94-2de5-43f7-9520-c0a28600a375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e75a007a-9811-47e6-8053-e8c554fb6fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00d1972-61c0-45c2-9d0e-860081d732cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# huggingface\n",
    "# from transformers import (\n",
    "#     GPT2LMHeadModel,\n",
    "#     GPT2Tokenizer,\n",
    "#     GPT2Config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "510deb98-a498-45e4-9b32-8e5d92782f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaModel, LlamaConfig, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3b4a5a5-f937-439e-b223-a8e0e78cff15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.34.0.dev0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e814cc8c-e40b-45f5-93f0-a9dee3cc6aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 64,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 128,\n",
       "  \"max_position_embeddings\": 128,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.34.0.dev0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"llama-xs\"\n",
    "max_length = 20\n",
    "\n",
    "xs_config = LlamaConfig(\n",
    "    hidden_size=16*4,\n",
    "    intermediate_size=128,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=None,\n",
    "    hidden_act='silu',\n",
    "    max_position_embeddings=128,\n",
    "    initializer_range=0.02,\n",
    "    rms_norm_eps=1e-06,\n",
    "    use_cache=False,\n",
    "    pad_token_id=None,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pretraining_tp=1,\n",
    "    tie_word_embeddings=False,\n",
    "    rope_theta=10000.0,\n",
    ")\n",
    "xs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9afc5a0f-6bec-4ae9-b830-f76668592279",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = LlamaModel(xs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f40299b3-baa6-4661-9ff0-a741fdd8987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_causalllm = LlamaForCausalLM(xs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d05f3252-c62e-4468-9454-056494bb8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Hello, my dog is a dog\"\n",
    "prompt2 = \"Hey, are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7e97feb-dc8e-4fcf-896b-13d75b001d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([prompt1, prompt2], padding=True, return_tensors=\"pt\")\n",
    "model_inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f42fc4dc-978a-42b3-aa04-d6cd90634cbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is a dogashaashaasha sigloulsût sigloulsût Новût Новût Нов bol sobuls Новheuls Новhe'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "# generate_ids = llama_causalllm.generate(inputs.input_ids, max_length=30)\n",
    "generate_ids = llama_causalllm.generate(model_inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Model saved to ./models/llama-xs/pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(model_name)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "llama_causalllm.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4af5ada0-9358-46f0-a682-f9722cd5f027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/llama-xs/pytorch'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c5766-97ed-4d04-bab5-7fa18e89dee8",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e43067c2-ecd9-4bd6-9047-a3f74621931b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# carry out inference with a single sample\n",
    "input_str = \"Hello, my dog is a dog\"\n",
    "inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d347ddf-4504-4ab7-b15b-29d218bdd7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1, 15043, 29892,   590, 11203,   338,   263, 11203]]),\n",
       " torch.Size([1, 8]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf83454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu\n"
     ]
    }
   ],
   "source": [
    "# WAR: Using an ugly representation because cuda 11.4 does not support GPU models due to cublas errors\n",
    "if \"cuda-11.4\" in os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "    print(\"Using cpu\")\n",
    "    llama_causalllm = llama_causalllm.cpu()\n",
    "    input_ids = input_ids.cpu()\n",
    "    inputs = inputs.to('cpu')\n",
    "else:\n",
    "    print(\"Using gpu\")\n",
    "    llama_causalllm = llama_causalllm.cuda()\n",
    "    input_ids = input_ids.cuda()\n",
    "    inputs = inputs.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c65e83e6-8b89-4c3c-a2fb-19615557cb7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.2 ms ± 1.26 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "generate_ids = llama_causalllm.generate(input_ids, max_length=20)\n",
    "tokenizer.decode(generate_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e47d40a-1ca3-4f31-98a2-04a968827c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is a dogashaashaasha sigloulsût sigloulsût Новût Нов'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ids = llama_causalllm.generate(input_ids, max_length=20)\n",
    "tokenizer.decode(generate_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6c2ea-3450-4b8b-9cc8-09943d967ece",
   "metadata": {},
   "source": [
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b844f057-e768-467d-9185-68fb4c74b5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_causalllm.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = llama_causalllm(**inputs, labels=inputs['input_ids'], use_cache = False)\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "717b2f68-9d92-474e-9937-8b42a1c60d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0401, -0.2120, -0.0292,  ...,  0.2774, -0.1493,  0.1037],\n",
       "          [ 0.0951, -0.1626, -0.0804,  ...,  0.3194,  0.0576,  0.1146],\n",
       "          [ 0.1536, -0.1104, -0.1257,  ...,  0.2606, -0.1510,  0.1010],\n",
       "          ...,\n",
       "          [ 0.0127, -0.1872, -0.1847,  ...,  0.4129, -0.0953,  0.1334],\n",
       "          [ 0.0148,  0.0055,  0.0550,  ...,  0.4017, -0.0178,  0.1838],\n",
       "          [ 0.1210, -0.1297, -0.0359,  ...,  0.4030,  0.0309,  0.1274]]],\n",
       "        device='cuda:0'),\n",
       " torch.Size([1, 8, 32000]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX format\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format: ONNX.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "At a high level, the steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU with the TensorRT engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b79f67ad-080a-4d85-926f-d2812e672794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkMetadata(variant='llama-xs', precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=False))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "from GPT2.GPT2ModelConfig import GPT2Metadata\n",
    "metadata = NetworkMetadata(variant=model_name, precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=False))\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58be125b-7d33-49ad-af84-1558f88251f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/working/repos/transformers/src/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Module\n",
    "from transformers.generation_utils import GenerationMixin\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "class TorchModule(Module, GenerationMixin):\n",
    "    \"\"\"\n",
    "    A simplied definition of Llama.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llama_model, lm_head, config):\n",
    "        super().__init__()\n",
    "        self.llama_model = llama_model\n",
    "        self.lm_head = lm_head\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda') # WAR to avoid beam search in framework\n",
    "        self.main_input_name = \"input_ids\" # For better HuggingFace version compatibility\n",
    "\n",
    "    # def prepare_inputs_for_generation(self, input_ids, past = None, use_cache=None, **kwargs):\n",
    "    #     # Todo (@pchadha): add position_ids, token_type_ids support\n",
    "    #     # cut decoder_input_ids if past is used\n",
    "    #     if past is not None:\n",
    "    #         input_ids = input_ids[:, -1:]\n",
    "\n",
    "    #     return {\n",
    "    #         \"input_ids\": input_ids,\n",
    "    #         \"use_cache\": use_cache,\n",
    "    #         \"past_key_values\": past\n",
    "    #     }\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        outputs = self.llama_model(input_ids, **kwargs)\n",
    "        hidden_states = outputs[0]\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            logits=lm_logits, \n",
    "            past_key_values=outputs.past_key_values\n",
    "        )\n",
    "\n",
    "    # def _reorder_cache(self, past, beam_idx):\n",
    "    #     \"\"\"\n",
    "    #     This function is used to re-order the :obj:`past_key_values` cache if\n",
    "    #     :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n",
    "    #     called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n",
    "    #     \"\"\"\n",
    "    #     return tuple(\n",
    "    #         tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "    #         for layer_past in past\n",
    "    #     )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "238d93a9-e511-4bcf-a2ca-6ca0eeccd567",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = ('input_ids',)\n",
    "output_names = ('logits',)\n",
    "input_dynamic_axis = {'input_ids': {0: 'batch', 1: 'sequence'}}\n",
    "output_dynamic_axis = {'logits': {0: 'batch', 1: 'sequence'}}\n",
    "\n",
    "opt_args = {}\n",
    "\n",
    "output_fpath = ('./models/{}/ONNX/{}.onnx'.format(model_name, model_name))\n",
    "output_fpath_sim = ('./models/{}/ONNX/{}_sim.onnx'.format(model_name, model_name))\n",
    "Path(output_fpath).parent.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68c4ba2c-8450-40d6-80b3-a8eeebd5cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = TorchModule(llama_causalllm.model, llama_causalllm.lm_head, xs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71e01fc7-3670-4db1-9798-7bded28fca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/working/repos/transformers/src/transformers/models/llama/modeling_llama.py:813: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/ec2-user/working/repos/transformers/src/transformers/models/llama/modeling_llama.py:146: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/home/ec2-user/working/repos/transformers/src/transformers/models/llama/modeling_llama.py:383: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "/home/ec2-user/working/repos/transformers/src/transformers/models/llama/modeling_llama.py:390: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "/home/ec2-user/working/repos/transformers/src/transformers/models/llama/modeling_llama.py:400: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_forward = torch_model.forward\n",
    "def _export_forward(input_ids, **kwargs):\n",
    "    kwargs[\"use_cache\"] = False\n",
    "    result = old_forward(input_ids, **kwargs)\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "torch_model.forward = _export_forward\n",
    "\n",
    "torch.onnx.export(\n",
    "    torch_model,\n",
    "    input_ids,\n",
    "    output_fpath,\n",
    "    opset_version=13,\n",
    "    do_constant_folding=True,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes={\n",
    "        **input_dynamic_axis,\n",
    "        **output_dynamic_axis,\n",
    "    },\n",
    "    training=torch.onnx.TrainingMode.EVAL,\n",
    "    **opt_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba8a16bc-165f-4d58-87af-e847c9a179dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/llama-xs/ONNX/llama-xs.onnx'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eea268ad-bb84-4131-8c7c-e74be6ed5964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 18M Sep 26 21:50 ./models/llama-xs/ONNX/llama-xs.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $output_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed8bc4ef-c2e7-4bfc-8c12-81e0e2d26c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !onnxsim $output_fpath $output_fpath_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93a14724-5283-44fe-840c-ff9b77438894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 17M\n",
      "-rw-r--r-- 1 root root 580 Sep 26 21:50 config.json\n",
      "-rw-r--r-- 1 root root 138 Sep 26 21:50 generation_config.json\n",
      "-rw-r--r-- 1 root root 17M Sep 26 21:50 pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $pytorch_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1eb21049-5722-4e78-b073-45ee53ea4fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 18M Sep 26 21:50 ./models/llama-xs/ONNX/llama-xs.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $output_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cab5652c-3e18-4fe9-bc9b-2bc1a9d5dc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.992769"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(output_fpath).stat().st_size / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b04de1-e887-445c-9bc8-e2a7e0fca7ea",
   "metadata": {},
   "source": [
    "Let's take a look at the onnx file and investigate its input and output. You should see that \"input_ids\" as the input, and \"logits\" as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29c82042-b6d7-4739-beec-2f1093f69440",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = output_fpath\n",
    "onnx_path_sim = output_fpath_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e4fff25-97da-4f9f-ae98-e918745faebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03c409e6-d312-4cc7-b13f-4621609d5633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2314caaf-836d-4140-93e4-4b3f4c931347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input_ids\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fe7a8d4-2bc3-49fc-863a-0e7f4be6565e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"logits\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 32000\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.graph.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "567e7779-b0cc-4c9b-887c-bf8384918595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT engine\n",
    "\n",
    "Now we are ready to parse the ONNX model and convert it to an optimized TensorRT model.\n",
    "\n",
    "Since the model contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile.\n",
    "\n",
    "Note: As TensorRT carries out many optimization, this conversion process for the larger model might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from polygraphy.backend.trt import Profile\n",
    "from tensorrt import PreviewFeature\n",
    "from GPT2.export import GPT2ONNXFile, GPT2TRTEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./models/$model_name/trt-engine\n",
    "trt_engine_folder = './models/{}/trt-engine'.format(model_name)\n",
    "\n",
    "# Create optimization profile for dynamic shape input. Can modify batch_size / max_sequence_length to build engines for different shapes\n",
    "batch_size = 1\n",
    "disable_preview_dynamic_shapes = False # preview_dynamic_shapes optimizes the trt engine building time\n",
    "# We can either use input length as the optimal length, or use max_length // 2. \n",
    "# In T5 or BART, input_length is better, but in GPT-2, max_length // 2 is better because we need to generate max_length number of tokens\n",
    "\n",
    "use_input_length = False\n",
    "opt_length = input_id.shape[1] if use_input_length else max_length // 2 \n",
    "# Create different engine tags for different configurations\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "preview_features = [PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-noPreviewFasterDynamicShapes\"\n",
    "else:\n",
    "    preview_features += [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "\n",
    "profiles = [Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, opt_length), # Optimized based on the inputs. \n",
    "    max=(batch_size, max_length),\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5538106b-3ae4-4d5f-b0ee-1f76174dcecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Profile().add('input_ids', min=(1, 1), opt=(1, 10), max=(1, 20))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7852a4d8-ebcd-46be-b725-3ed77fc621a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805: 1>,\n",
       " <PreviewFeature.FASTER_DYNAMIC_SHAPES_0805: 0>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a5934f0-46d3-45d7-8dd5-6cf81de61e66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] 'colored' module is not installed, will not use colors when logging. To enable colors, please install the 'colored' module: python3 -m pip install colored\n",
      "[V] Loaded Module: tensorrt | Version: 8.6.1 | Path: ['/usr/local/lib/python3.10/dist-packages/tensorrt']\n",
      "[V] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 1888, GPU 197 (MiB)\n",
      "[X] Trying to load shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[X] Loaded shared library libnvinfer_builder_resource.so.8.6.1\n",
      "[V] [MemUsageChange] Init builder kernel library: CPU +889, GPU +172, now: CPU 2854, GPU 369 (MiB)\n",
      "[X] CUDA lazy loading is enabled.\n",
      "[V] ----------------------------------------------------------------\n",
      "[V] Input filename:   ./models/llama-xs/ONNX/llama-xs.onnx\n",
      "[V] ONNX IR version:  0.0.7\n",
      "[V] Opset version:    13\n",
      "[V] Producer name:    pytorch\n",
      "[V] Producer version: 2.0.1\n",
      "[V] Domain:           \n",
      "[V] Model version:    0\n",
      "[V] Doc string:       \n",
      "[V] ----------------------------------------------------------------\n",
      "[X] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1\n",
      "[X] Registered plugin creator - ::BatchedNMS_TRT version 1\n",
      "[X] Registered plugin creator - ::BatchTilePlugin_TRT version 1\n",
      "[X] Registered plugin creator - ::Clip_TRT version 1\n",
      "[X] Registered plugin creator - ::CoordConvAC version 1\n",
      "[X] Registered plugin creator - ::CropAndResizeDynamic version 1\n",
      "[X] Registered plugin creator - ::CropAndResize version 1\n",
      "[X] Registered plugin creator - ::DecodeBbox3DPlugin version 1\n",
      "[X] Registered plugin creator - ::DetectionLayer_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1\n",
      "[X] Registered plugin creator - ::EfficientNMS_TRT version 1\n",
      "[X] Registered plugin creator - ::FlattenConcat_TRT version 1\n",
      "[X] Registered plugin creator - ::GenerateDetection_TRT version 1\n",
      "[X] Registered plugin creator - ::GridAnchor_TRT version 1\n",
      "[X] Registered plugin creator - ::GridAnchorRect_TRT version 1\n",
      "[X] Registered plugin creator - ::InstanceNormalization_TRT version 1\n",
      "[X] Registered plugin creator - ::InstanceNormalization_TRT version 2\n",
      "[X] Registered plugin creator - ::LReLU_TRT version 1\n",
      "[X] Registered plugin creator - ::ModulatedDeformConv2d version 1\n",
      "[X] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1\n",
      "[X] Registered plugin creator - ::MultilevelProposeROI_TRT version 1\n",
      "[X] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[X] Registered plugin creator - ::NMSDynamic_TRT version 1\n",
      "[X] Registered plugin creator - ::NMS_TRT version 1\n",
      "[X] Registered plugin creator - ::Normalize_TRT version 1\n",
      "[X] Registered plugin creator - ::PillarScatterPlugin version 1\n",
      "[X] Registered plugin creator - ::PriorBox_TRT version 1\n",
      "[X] Registered plugin creator - ::ProposalDynamic version 1\n",
      "[X] Registered plugin creator - ::ProposalLayer_TRT version 1\n",
      "[X] Registered plugin creator - ::Proposal version 1\n",
      "[X] Registered plugin creator - ::PyramidROIAlign_TRT version 1\n",
      "[X] Registered plugin creator - ::Region_TRT version 1\n",
      "[X] Registered plugin creator - ::Reorg_TRT version 1\n",
      "[X] Registered plugin creator - ::ResizeNearest_TRT version 1\n",
      "[X] Registered plugin creator - ::ROIAlign_TRT version 1\n",
      "[X] Registered plugin creator - ::RPROI_TRT version 1\n",
      "[X] Registered plugin creator - ::ScatterND version 1\n",
      "[X] Registered plugin creator - ::SpecialSlice_TRT version 1\n",
      "[X] Registered plugin creator - ::Split version 1\n",
      "[X] Registered plugin creator - ::VoxelGeneratorPlugin version 1\n",
      "[X] Adding network input: input_ids with dtype: int32, dimensions: (-1, -1)\n",
      "[X] Registering tensor: input_ids for ONNX tensor: input_ids\n",
      "[X] Importing initializer: llama_model.embed_tokens.weight\n",
      "[X] Importing initializer: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Importing initializer: onnx::MatMul_2038\n",
      "[X] Importing initializer: onnx::MatMul_2039\n",
      "[X] Importing initializer: onnx::MatMul_2040\n",
      "[X] Importing initializer: onnx::MatMul_2072\n",
      "[X] Importing initializer: onnx::MatMul_2073\n",
      "[X] Importing initializer: onnx::MatMul_2074\n",
      "[X] Importing initializer: onnx::MatMul_2075\n",
      "[X] Importing initializer: onnx::MatMul_2076\n",
      "[X] Importing initializer: onnx::MatMul_2077\n",
      "[X] Importing initializer: onnx::MatMul_2078\n",
      "[X] Importing initializer: onnx::MatMul_2110\n",
      "[X] Importing initializer: onnx::MatMul_2111\n",
      "[X] Importing initializer: onnx::MatMul_2112\n",
      "[X] Importing initializer: onnx::MatMul_2113\n",
      "[X] Importing initializer: onnx::MatMul_2114\n",
      "[X] Importing initializer: onnx::MatMul_2115\n",
      "[X] Importing initializer: onnx::MatMul_2116\n",
      "[X] Importing initializer: onnx::MatMul_2148\n",
      "[X] Importing initializer: onnx::MatMul_2149\n",
      "[X] Importing initializer: onnx::MatMul_2150\n",
      "[X] Importing initializer: onnx::MatMul_2151\n",
      "[X] Importing initializer: onnx::MatMul_2152\n",
      "[X] Importing initializer: onnx::MatMul_2153\n",
      "[X] Importing initializer: onnx::MatMul_2154\n",
      "[X] Importing initializer: onnx::MatMul_2186\n",
      "[X] Importing initializer: onnx::MatMul_2187\n",
      "[X] Importing initializer: onnx::MatMul_2188\n",
      "[X] Importing initializer: onnx::MatMul_2189\n",
      "[X] Importing initializer: onnx::MatMul_2190\n",
      "[X] Importing initializer: onnx::MatMul_2191\n",
      "[X] Importing initializer: onnx::MatMul_2192\n",
      "[X] Importing initializer: onnx::MatMul_2224\n",
      "[X] Importing initializer: onnx::MatMul_2225\n",
      "[X] Importing initializer: onnx::MatMul_2226\n",
      "[X] Importing initializer: onnx::MatMul_2227\n",
      "[X] Importing initializer: onnx::MatMul_2228\n",
      "[X] Importing initializer: onnx::MatMul_2229\n",
      "[X] Importing initializer: onnx::MatMul_2230\n",
      "[X] Importing initializer: onnx::MatMul_2262\n",
      "[X] Importing initializer: onnx::MatMul_2263\n",
      "[X] Importing initializer: onnx::MatMul_2264\n",
      "[X] Importing initializer: onnx::MatMul_2265\n",
      "[X] Importing initializer: onnx::MatMul_2266\n",
      "[X] Importing initializer: onnx::MatMul_2267\n",
      "[X] Importing initializer: onnx::MatMul_2268\n",
      "[X] Importing initializer: onnx::MatMul_2300\n",
      "[X] Importing initializer: onnx::MatMul_2301\n",
      "[X] Importing initializer: onnx::MatMul_2302\n",
      "[X] Importing initializer: onnx::MatMul_2303\n",
      "[X] Importing initializer: onnx::MatMul_2304\n",
      "[X] Importing initializer: onnx::MatMul_2305\n",
      "[X] Importing initializer: onnx::MatMul_2306\n",
      "[X] Importing initializer: onnx::MatMul_2338\n",
      "[X] Importing initializer: onnx::MatMul_2339\n",
      "[X] Importing initializer: onnx::MatMul_2340\n",
      "[X] Importing initializer: onnx::MatMul_2341\n",
      "[X] Importing initializer: onnx::MatMul_2342\n",
      "[X] Parsing node: Identity_0 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_0 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: llama_model.layers.0.input_layernorm.weight for ONNX node: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Registering layer: Identity_0 for ONNX node: Identity_0\n",
      "[X] Registering tensor: llama_model.norm.weight for ONNX tensor: llama_model.norm.weight\n",
      "[X] Identity_0 [Identity] outputs: [llama_model.norm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_1 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_1 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_1 for ONNX node: Identity_1\n",
      "[X] Registering tensor: llama_model.layers.7.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.7.post_attention_layernorm.weight\n",
      "[X] Identity_1 [Identity] outputs: [llama_model.layers.7.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_2 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_2 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_2 for ONNX node: Identity_2\n",
      "[X] Registering tensor: llama_model.layers.7.input_layernorm.weight for ONNX tensor: llama_model.layers.7.input_layernorm.weight\n",
      "[X] Identity_2 [Identity] outputs: [llama_model.layers.7.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_3 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_3 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_3 for ONNX node: Identity_3\n",
      "[X] Registering tensor: llama_model.layers.6.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.6.post_attention_layernorm.weight\n",
      "[X] Identity_3 [Identity] outputs: [llama_model.layers.6.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_4 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_4 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_4 for ONNX node: Identity_4\n",
      "[X] Registering tensor: llama_model.layers.6.input_layernorm.weight for ONNX tensor: llama_model.layers.6.input_layernorm.weight\n",
      "[X] Identity_4 [Identity] outputs: [llama_model.layers.6.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_5 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_5 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_5 for ONNX node: Identity_5\n",
      "[X] Registering tensor: llama_model.layers.5.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.5.post_attention_layernorm.weight\n",
      "[X] Identity_5 [Identity] outputs: [llama_model.layers.5.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_6 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_6 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_6 for ONNX node: Identity_6\n",
      "[X] Registering tensor: llama_model.layers.5.input_layernorm.weight for ONNX tensor: llama_model.layers.5.input_layernorm.weight\n",
      "[X] Identity_6 [Identity] outputs: [llama_model.layers.5.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_7 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_7 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_7 for ONNX node: Identity_7\n",
      "[X] Registering tensor: llama_model.layers.4.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.4.post_attention_layernorm.weight\n",
      "[X] Identity_7 [Identity] outputs: [llama_model.layers.4.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_8 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_8 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_8 for ONNX node: Identity_8\n",
      "[X] Registering tensor: llama_model.layers.4.input_layernorm.weight for ONNX tensor: llama_model.layers.4.input_layernorm.weight\n",
      "[X] Identity_8 [Identity] outputs: [llama_model.layers.4.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_9 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_9 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_9 for ONNX node: Identity_9\n",
      "[X] Registering tensor: llama_model.layers.3.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.3.post_attention_layernorm.weight\n",
      "[X] Identity_9 [Identity] outputs: [llama_model.layers.3.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_10 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_10 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_10 for ONNX node: Identity_10\n",
      "[X] Registering tensor: llama_model.layers.3.input_layernorm.weight for ONNX tensor: llama_model.layers.3.input_layernorm.weight\n",
      "[X] Identity_10 [Identity] outputs: [llama_model.layers.3.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_11 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_11 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_11 for ONNX node: Identity_11\n",
      "[X] Registering tensor: llama_model.layers.2.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.2.post_attention_layernorm.weight\n",
      "[X] Identity_11 [Identity] outputs: [llama_model.layers.2.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_12 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_12 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_12 for ONNX node: Identity_12\n",
      "[X] Registering tensor: llama_model.layers.2.input_layernorm.weight for ONNX tensor: llama_model.layers.2.input_layernorm.weight\n",
      "[X] Identity_12 [Identity] outputs: [llama_model.layers.2.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_13 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_13 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_13 for ONNX node: Identity_13\n",
      "[X] Registering tensor: llama_model.layers.1.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.1.post_attention_layernorm.weight\n",
      "[X] Identity_13 [Identity] outputs: [llama_model.layers.1.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_14 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_14 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_14 for ONNX node: Identity_14\n",
      "[X] Registering tensor: llama_model.layers.1.input_layernorm.weight for ONNX tensor: llama_model.layers.1.input_layernorm.weight\n",
      "[X] Identity_14 [Identity] outputs: [llama_model.layers.1.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: Identity_15 [Identity]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Identity_15 [Identity] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Registering layer: Identity_15 for ONNX node: Identity_15\n",
      "[X] Registering tensor: llama_model.layers.0.post_attention_layernorm.weight for ONNX tensor: llama_model.layers.0.post_attention_layernorm.weight\n",
      "[X] Identity_15 [Identity] outputs: [llama_model.layers.0.post_attention_layernorm.weight -> (64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Shape [Shape]\n",
      "[X] Searching for input: input_ids\n",
      "[X] llama_model/Shape [Shape] inputs: [input_ids -> (-1, -1)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape for ONNX node: llama_model/Shape\n",
      "[X] Registering tensor: llama_model/Shape_output_0 for ONNX tensor: llama_model/Shape_output_0\n",
      "[X] llama_model/Shape [Shape] outputs: [llama_model/Shape_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant [Constant]\n",
      "[X] llama_model/Constant [Constant] inputs: \n",
      "[W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[X] llama_model/Constant [Constant] outputs: [llama_model/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather [Gather]\n",
      "[X] Searching for input: llama_model/Shape_output_0\n",
      "[X] Searching for input: llama_model/Constant_output_0\n",
      "[X] llama_model/Gather [Gather] inputs: [llama_model/Shape_output_0 -> (2)[INT32]], [llama_model/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_output_0 for ONNX node: llama_model/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather for ONNX node: llama_model/Gather\n",
      "[X] Registering tensor: llama_model/Gather_output_0 for ONNX tensor: llama_model/Gather_output_0\n",
      "[X] llama_model/Gather [Gather] outputs: [llama_model/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_1 [Shape]\n",
      "[X] Searching for input: input_ids\n",
      "[X] llama_model/Shape_1 [Shape] inputs: [input_ids -> (-1, -1)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape_1 for ONNX node: llama_model/Shape_1\n",
      "[X] Registering tensor: llama_model/Shape_1_output_0 for ONNX tensor: llama_model/Shape_1_output_0\n",
      "[X] llama_model/Shape_1 [Shape] outputs: [llama_model/Shape_1_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_1 [Constant]\n",
      "[X] llama_model/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/Constant_1 [Constant] outputs: [llama_model/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_1_output_0\n",
      "[X] llama_model/Gather_1 [Gather] inputs: [llama_model/Shape_1_output_0 -> (2)[INT32]], [llama_model/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_1_output_0 for ONNX node: llama_model/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather_1 for ONNX node: llama_model/Gather_1\n",
      "[X] Registering tensor: llama_model/Gather_1_output_0 for ONNX tensor: llama_model/Gather_1_output_0\n",
      "[X] llama_model/Gather_1 [Gather] outputs: [llama_model/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_2 [Constant]\n",
      "[X] llama_model/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/Constant_2 [Constant] outputs: [llama_model/Constant_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Cast [Cast]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] llama_model/Cast [Cast] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/Cast for ONNX node: llama_model/Cast\n",
      "[X] Registering tensor: llama_model/Cast_output_0 for ONNX tensor: llama_model/Cast_output_0\n",
      "[X] llama_model/Cast [Cast] outputs: [llama_model/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_3 [Constant]\n",
      "[X] llama_model/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/Constant_3 [Constant] outputs: [llama_model/Constant_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Range [Range]\n",
      "[X] Searching for input: llama_model/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/Cast_output_0\n",
      "[X] Searching for input: llama_model/Constant_3_output_0\n",
      "[X] llama_model/Range [Range] inputs: [llama_model/Constant_2_output_0 -> ()[INT32]], [llama_model/Cast_output_0 -> ()[INT32]], [llama_model/Constant_3_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Range for ONNX node: llama_model/Range\n",
      "[X] Registering tensor: llama_model/Range_output_0 for ONNX tensor: llama_model/Range_output_0\n",
      "[X] llama_model/Range [Range] outputs: [llama_model/Range_output_0 -> (-1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_4 [Constant]\n",
      "[X] llama_model/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/Constant_4 [Constant] outputs: [llama_model/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Range_output_0\n",
      "[X] Searching for input: llama_model/Constant_4_output_0\n",
      "[X] llama_model/Unsqueeze [Unsqueeze] inputs: [llama_model/Range_output_0 -> (-1)[INT32]], [llama_model/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_,), unsqueezing to: (_, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze for ONNX node: llama_model/Unsqueeze\n",
      "[X] Registering tensor: llama_model/Unsqueeze_output_0 for ONNX tensor: llama_model/Unsqueeze_output_0\n",
      "[X] llama_model/Unsqueeze [Unsqueeze] outputs: [llama_model/Unsqueeze_output_0 -> (1, -1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_5 [Constant]\n",
      "[X] llama_model/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/Constant_5 [Constant] outputs: [llama_model/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_29 [Constant]\n",
      "[X] Constant_29 [Constant] inputs: \n",
      "[X] Constant_29 [Constant] outputs: [onnx::Unsqueeze_93 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_93\n",
      "[X] llama_model/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_93 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_1 for ONNX node: llama_model/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/Unsqueeze_1_output_0 for ONNX tensor: llama_model/Unsqueeze_1_output_0\n",
      "[X] llama_model/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat [Concat]\n",
      "[X] Searching for input: llama_model/Constant_5_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_1_output_0\n",
      "[X] llama_model/Concat [Concat] inputs: [llama_model/Constant_5_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_5_output_0 for ONNX node: llama_model/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/Concat for ONNX node: llama_model/Concat\n",
      "[X] Registering tensor: llama_model/Concat_output_0 for ONNX tensor: llama_model/Concat_output_0\n",
      "[X] llama_model/Concat [Concat] outputs: [llama_model/Concat_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/Concat_output_0\n",
      "[X] llama_model/Reshape [Reshape] inputs: [llama_model/Unsqueeze_output_0 -> (1, -1)[INT32]], [llama_model/Concat_output_0 -> (2)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape for ONNX node: llama_model/Reshape\n",
      "[X] Registering tensor: llama_model/Reshape_output_0 for ONNX tensor: llama_model/Reshape_output_0\n",
      "[X] llama_model/Reshape [Reshape] outputs: [llama_model/Reshape_output_0 -> (1, -1)[INT32]], \n",
      "[X] Parsing node: llama_model/embed_tokens/Gather [Gather]\n",
      "[X] Searching for input: llama_model.embed_tokens.weight\n",
      "[X] Searching for input: input_ids\n",
      "[X] llama_model/embed_tokens/Gather [Gather] inputs: [llama_model.embed_tokens.weight -> (32000, 64)[FLOAT]], [input_ids -> (-1, -1)[INT32]], \n",
      "[X] Registering layer: llama_model.embed_tokens.weight for ONNX node: llama_model.embed_tokens.weight\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/embed_tokens/Gather for ONNX node: llama_model/embed_tokens/Gather\n",
      "[X] Registering tensor: llama_model/embed_tokens/Gather_output_0 for ONNX tensor: llama_model/embed_tokens/Gather_output_0\n",
      "[X] llama_model/embed_tokens/Gather [Gather] outputs: [llama_model/embed_tokens/Gather_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_34 [Constant]\n",
      "[X] Constant_34 [Constant] inputs: \n",
      "[X] Constant_34 [Constant] outputs: [onnx::Unsqueeze_98 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_98\n",
      "[X] llama_model/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_98 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_2 for ONNX node: llama_model/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/Unsqueeze_2_output_0 for ONNX tensor: llama_model/Unsqueeze_2_output_0\n",
      "[X] llama_model/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_36 [Constant]\n",
      "[X] Constant_36 [Constant] inputs: \n",
      "[X] Constant_36 [Constant] outputs: [onnx::Unsqueeze_100 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_100\n",
      "[X] llama_model/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_100 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_3 for ONNX node: llama_model/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/Unsqueeze_3_output_0 for ONNX tensor: llama_model/Unsqueeze_3_output_0\n",
      "[X] llama_model/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_3_output_0\n",
      "[X] llama_model/Concat_1 [Concat] inputs: [llama_model/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Concat_1 for ONNX node: llama_model/Concat_1\n",
      "[X] Registering tensor: llama_model/Concat_1_output_0 for ONNX tensor: llama_model/Concat_1_output_0\n",
      "[X] llama_model/Concat_1 [Concat] outputs: [llama_model/Concat_1_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Concat_1_output_0\n",
      "[X] llama_model/ConstantOfShape [ConstantOfShape] inputs: [llama_model/Concat_1_output_0 -> (2)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_output_0 for ONNX tensor: llama_model/ConstantOfShape_output_0\n",
      "[X] llama_model/ConstantOfShape [ConstantOfShape] outputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Parsing node: Constant_40 [Constant]\n",
      "[X] Constant_40 [Constant] inputs: \n",
      "[X] Constant_40 [Constant] outputs: [onnx::Unsqueeze_104 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_104\n",
      "[X] llama_model/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_104 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_4 for ONNX node: llama_model/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/Unsqueeze_4_output_0 for ONNX tensor: llama_model/Unsqueeze_4_output_0\n",
      "[X] llama_model/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_42 [Constant]\n",
      "[X] Constant_42 [Constant] inputs: \n",
      "[X] Constant_42 [Constant] outputs: [onnx::Unsqueeze_106 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_106\n",
      "[X] llama_model/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_106 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_5 for ONNX node: llama_model/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/Unsqueeze_5_output_0 for ONNX tensor: llama_model/Unsqueeze_5_output_0\n",
      "[X] llama_model/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_5_output_0\n",
      "[X] llama_model/Concat_2 [Concat] inputs: [llama_model/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Concat_2 for ONNX node: llama_model/Concat_2\n",
      "[X] Registering tensor: llama_model/Concat_2_output_0 for ONNX tensor: llama_model/Concat_2_output_0\n",
      "[X] llama_model/Concat_2 [Concat] outputs: [llama_model/Concat_2_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape_1 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Concat_2_output_0\n",
      "[X] llama_model/ConstantOfShape_1 [ConstantOfShape] inputs: [llama_model/Concat_2_output_0 -> (2)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_1_output_0 for ONNX tensor: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/ConstantOfShape_1 [ConstantOfShape] outputs: [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/Shape_2 [Shape] inputs: [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Shape_2 for ONNX node: llama_model/Shape_2\n",
      "[X] Registering tensor: llama_model/Shape_2_output_0 for ONNX tensor: llama_model/Shape_2_output_0\n",
      "[X] llama_model/Shape_2 [Shape] outputs: [llama_model/Shape_2_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_6 [Constant]\n",
      "[X] llama_model/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/Constant_6 [Constant] outputs: [llama_model/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_7 [Constant]\n",
      "[X] llama_model/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/Constant_7 [Constant] outputs: [llama_model/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_8 [Constant]\n",
      "[X] llama_model/Constant_8 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[W] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n",
      "[X] llama_model/Constant_8 [Constant] outputs: [llama_model/Constant_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Slice [Slice]\n",
      "[X] Searching for input: llama_model/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_7_output_0\n",
      "[X] Searching for input: llama_model/Constant_8_output_0\n",
      "[X] Searching for input: llama_model/Constant_6_output_0\n",
      "[X] llama_model/Slice [Slice] inputs: [llama_model/Shape_2_output_0 -> (2)[INT32]], [llama_model/Constant_7_output_0 -> (1)[INT32]], [llama_model/Constant_8_output_0 -> (1)[INT32]], [llama_model/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Slice for ONNX node: llama_model/Slice\n",
      "[X] Registering tensor: llama_model/Slice_output_0 for ONNX tensor: llama_model/Slice_output_0\n",
      "[X] llama_model/Slice [Slice] outputs: [llama_model/Slice_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_9 [Constant]\n",
      "[X] llama_model/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/Constant_9 [Constant] outputs: [llama_model/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Squeeze [Squeeze]\n",
      "[X] Searching for input: llama_model/Slice_output_0\n",
      "[X] Searching for input: llama_model/Constant_9_output_0\n",
      "[X] llama_model/Squeeze [Squeeze] inputs: [llama_model/Slice_output_0 -> (1)[INT32]], [llama_model/Constant_9_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1,), squeezing to: ()\n",
      "[X] Registering layer: llama_model/Squeeze for ONNX node: llama_model/Squeeze\n",
      "[X] Registering tensor: llama_model/Squeeze_output_0 for ONNX tensor: llama_model/Squeeze_output_0\n",
      "[X] llama_model/Squeeze [Squeeze] outputs: [llama_model/Squeeze_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/Squeeze_output_0\n",
      "[X] llama_model/Cast_1 [Cast] inputs: [llama_model/Squeeze_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/Cast_1 for ONNX node: llama_model/Cast_1\n",
      "[X] Registering tensor: llama_model/Cast_1_output_0 for ONNX tensor: llama_model/Cast_1_output_0\n",
      "[X] llama_model/Cast_1 [Cast] outputs: [llama_model/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_10 [Constant]\n",
      "[X] llama_model/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/Constant_10 [Constant] outputs: [llama_model/Constant_10_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_11 [Constant]\n",
      "[X] llama_model/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/Constant_11 [Constant] outputs: [llama_model/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Range_1 [Range]\n",
      "[X] Searching for input: llama_model/Constant_10_output_0\n",
      "[X] Searching for input: llama_model/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_11_output_0\n",
      "[X] llama_model/Range_1 [Range] inputs: [llama_model/Constant_10_output_0 -> ()[INT32]], [llama_model/Cast_1_output_0 -> ()[INT32]], [llama_model/Constant_11_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Range_1 for ONNX node: llama_model/Range_1\n",
      "[X] Registering tensor: llama_model/Range_1_output_0 for ONNX tensor: llama_model/Range_1_output_0\n",
      "[X] llama_model/Range_1 [Range] outputs: [llama_model/Range_1_output_0 -> (-1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_12 [Constant]\n",
      "[X] llama_model/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/Constant_12 [Constant] outputs: [llama_model/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Add [Add]\n",
      "[X] Searching for input: llama_model/Range_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_12_output_0\n",
      "[X] llama_model/Add [Add] inputs: [llama_model/Range_1_output_0 -> (-1)[INT32]], [llama_model/Constant_12_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_12_output_0 for ONNX node: llama_model/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/Add for ONNX node: llama_model/Add\n",
      "[X] Registering tensor: llama_model/Add_output_0 for ONNX tensor: llama_model/Add_output_0\n",
      "[X] llama_model/Add [Add] outputs: [llama_model/Add_output_0 -> (-1)[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/Shape_3 [Shape] inputs: [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Shape_3 for ONNX node: llama_model/Shape_3\n",
      "[X] Registering tensor: llama_model/Shape_3_output_0 for ONNX tensor: llama_model/Shape_3_output_0\n",
      "[X] llama_model/Shape_3 [Shape] outputs: [llama_model/Shape_3_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_13 [Constant]\n",
      "[X] llama_model/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/Constant_13 [Constant] outputs: [llama_model/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_14 [Constant]\n",
      "[X] llama_model/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/Constant_14 [Constant] outputs: [llama_model/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_15 [Constant]\n",
      "[X] llama_model/Constant_15 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/Constant_15 [Constant] outputs: [llama_model/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_14_output_0\n",
      "[X] Searching for input: llama_model/Constant_15_output_0\n",
      "[X] Searching for input: llama_model/Constant_13_output_0\n",
      "[X] llama_model/Slice_1 [Slice] inputs: [llama_model/Shape_3_output_0 -> (2)[INT32]], [llama_model/Constant_14_output_0 -> (1)[INT32]], [llama_model/Constant_15_output_0 -> (1)[INT32]], [llama_model/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Slice_1 for ONNX node: llama_model/Slice_1\n",
      "[X] Registering tensor: llama_model/Slice_1_output_0 for ONNX tensor: llama_model/Slice_1_output_0\n",
      "[X] llama_model/Slice_1 [Slice] outputs: [llama_model/Slice_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_16 [Constant]\n",
      "[X] llama_model/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/Constant_16 [Constant] outputs: [llama_model/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Squeeze_1 [Squeeze]\n",
      "[X] Searching for input: llama_model/Slice_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_16_output_0\n",
      "[X] llama_model/Squeeze_1 [Squeeze] inputs: [llama_model/Slice_1_output_0 -> (1)[INT32]], [llama_model/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1,), squeezing to: ()\n",
      "[X] Registering layer: llama_model/Squeeze_1 for ONNX node: llama_model/Squeeze_1\n",
      "[X] Registering tensor: llama_model/Squeeze_1_output_0 for ONNX tensor: llama_model/Squeeze_1_output_0\n",
      "[X] llama_model/Squeeze_1 [Squeeze] outputs: [llama_model/Squeeze_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_66 [Constant]\n",
      "[X] Constant_66 [Constant] inputs: \n",
      "[X] Constant_66 [Constant] outputs: [onnx::Unsqueeze_130 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Squeeze_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_130\n",
      "[X] llama_model/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/Squeeze_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_130 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_6 for ONNX node: llama_model/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/Unsqueeze_6_output_0 for ONNX tensor: llama_model/Unsqueeze_6_output_0\n",
      "[X] llama_model/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_17 [Constant]\n",
      "[X] llama_model/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/Constant_17 [Constant] outputs: [llama_model/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/Constant_17_output_0\n",
      "[X] llama_model/Concat_3 [Concat] inputs: [llama_model/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_17_output_0 for ONNX node: llama_model/Constant_17_output_0\n",
      "[X] Registering layer: llama_model/Concat_3 for ONNX node: llama_model/Concat_3\n",
      "[X] Registering tensor: llama_model/Concat_3_output_0 for ONNX tensor: llama_model/Concat_3_output_0\n",
      "[X] llama_model/Concat_3 [Concat] outputs: [llama_model/Concat_3_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/Add_output_0\n",
      "[X] Searching for input: llama_model/Concat_3_output_0\n",
      "[X] llama_model/Reshape_1 [Reshape] inputs: [llama_model/Add_output_0 -> (-1)[INT32]], [llama_model/Concat_3_output_0 -> (2)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape_1 for ONNX node: llama_model/Reshape_1\n",
      "[X] Registering tensor: llama_model/Reshape_1_output_0 for ONNX tensor: llama_model/Reshape_1_output_0\n",
      "[X] llama_model/Reshape_1 [Reshape] outputs: [llama_model/Reshape_1_output_0 -> (-1, 1)[INT32]], \n",
      "[X] Parsing node: llama_model/Less [Less]\n",
      "[X] Searching for input: llama_model/Range_1_output_0\n",
      "[X] Searching for input: llama_model/Reshape_1_output_0\n",
      "[X] llama_model/Less [Less] inputs: [llama_model/Range_1_output_0 -> (-1)[INT32]], [llama_model/Reshape_1_output_0 -> (-1, 1)[INT32]], \n",
      "[X] Registering layer: llama_model/Less for ONNX node: llama_model/Less\n",
      "[X] Registering tensor: llama_model/Less_output_0 for ONNX tensor: llama_model/Less_output_0\n",
      "[X] llama_model/Less [Less] outputs: [llama_model/Less_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/Less_output_0\n",
      "[X] llama_model/Cast_2 [Cast] inputs: [llama_model/Less_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Casting to type: bool\n",
      "[X] Registering layer: llama_model/Cast_2 for ONNX node: llama_model/Cast_2\n",
      "[X] Registering tensor: llama_model/Cast_2_output_0 for ONNX tensor: llama_model/Cast_2_output_0\n",
      "[X] llama_model/Cast_2 [Cast] outputs: [llama_model/Cast_2_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_18 [Constant]\n",
      "[X] llama_model/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/Constant_18 [Constant] outputs: [llama_model/Constant_18_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/Where [Where]\n",
      "[X] Searching for input: llama_model/Cast_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_18_output_0\n",
      "[X] Searching for input: llama_model/ConstantOfShape_1_output_0\n",
      "[X] llama_model/Where [Where] inputs: [llama_model/Cast_2_output_0 -> (-1, -1)[BOOL]], [llama_model/Constant_18_output_0 -> ()[FLOAT]], [llama_model/ConstantOfShape_1_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Constant_18_output_0 for ONNX node: llama_model/Constant_18_output_0\n",
      "[X] Registering layer: llama_model/Where for ONNX node: llama_model/Where\n",
      "[X] Registering tensor: llama_model/Where_output_0 for ONNX tensor: llama_model/Where_output_0\n",
      "[X] llama_model/Where [Where] outputs: [llama_model/Where_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/Where_output_0\n",
      "[X] llama_model/Cast_3 [Cast] inputs: [llama_model/Where_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/Cast_3 for ONNX node: llama_model/Cast_3\n",
      "[X] Registering tensor: llama_model/Cast_3_output_0 for ONNX tensor: llama_model/Cast_3_output_0\n",
      "[X] llama_model/Cast_3 [Cast] outputs: [llama_model/Cast_3_output_0 -> (-1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_19 [Constant]\n",
      "[X] llama_model/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/Constant_19 [Constant] outputs: [llama_model/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_19_output_0\n",
      "[X] llama_model/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/Cast_3_output_0 -> (-1, -1)[FLOAT]], [llama_model/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_, _), unsqueezing to: (_, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_7 for ONNX node: llama_model/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/Unsqueeze_7_output_0 for ONNX tensor: llama_model/Unsqueeze_7_output_0\n",
      "[X] llama_model/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/Unsqueeze_7_output_0 -> (1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_20 [Constant]\n",
      "[X] llama_model/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/Constant_20 [Constant] outputs: [llama_model/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/Constant_20_output_0\n",
      "[X] llama_model/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/Unsqueeze_7_output_0 -> (1, -1, -1)[FLOAT]], [llama_model/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _, _), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_8 for ONNX node: llama_model/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/Unsqueeze_8_output_0 for ONNX tensor: llama_model/Unsqueeze_8_output_0\n",
      "[X] llama_model/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/Unsqueeze_8_output_0 -> (1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_21 [Constant]\n",
      "[X] llama_model/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/Constant_21 [Constant] outputs: [llama_model/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_output_0\n",
      "[X] Searching for input: llama_model/Constant_21_output_0\n",
      "[X] llama_model/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/Gather_output_0 -> ()[INT32]], [llama_model/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_9 for ONNX node: llama_model/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/Unsqueeze_9_output_0 for ONNX tensor: llama_model/Unsqueeze_9_output_0\n",
      "[X] llama_model/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_22 [Constant]\n",
      "[X] llama_model/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/Constant_22 [Constant] outputs: [llama_model/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_23 [Constant]\n",
      "[X] llama_model/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/Constant_23 [Constant] outputs: [llama_model/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_23_output_0\n",
      "[X] llama_model/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [llama_model/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_10 for ONNX node: llama_model/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/Unsqueeze_10_output_0 for ONNX tensor: llama_model/Unsqueeze_10_output_0\n",
      "[X] llama_model/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_24 [Constant]\n",
      "[X] llama_model/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/Constant_24 [Constant] outputs: [llama_model/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_24_output_0\n",
      "[X] llama_model/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [llama_model/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_11 for ONNX node: llama_model/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/Unsqueeze_11_output_0 for ONNX tensor: llama_model/Unsqueeze_11_output_0\n",
      "[X] llama_model/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/Constant_22_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_11_output_0\n",
      "[X] llama_model/Concat_4 [Concat] inputs: [llama_model/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/Constant_22_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_22_output_0 for ONNX node: llama_model/Constant_22_output_0\n",
      "[X] Registering layer: llama_model/Concat_4 for ONNX node: llama_model/Concat_4\n",
      "[X] Registering tensor: llama_model/Concat_4_output_0 for ONNX tensor: llama_model/Concat_4_output_0\n",
      "[X] llama_model/Concat_4 [Concat] outputs: [llama_model/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_25 [Constant]\n",
      "[X] llama_model/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/Constant_25 [Constant] outputs: [llama_model/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/Concat_4_output_0\n",
      "[X] Searching for input: llama_model/Constant_25_output_0\n",
      "[X] llama_model/Reshape_2 [Reshape] inputs: [llama_model/Concat_4_output_0 -> (4)[INT32]], [llama_model/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape_2 for ONNX node: llama_model/Reshape_2\n",
      "[X] Registering tensor: llama_model/Reshape_2_output_0 for ONNX tensor: llama_model/Reshape_2_output_0\n",
      "[X] llama_model/Reshape_2 [Reshape] outputs: [llama_model/Reshape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/Reshape_2_output_0\n",
      "[X] llama_model/Shape_4 [Shape] inputs: [llama_model/Reshape_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape_4 for ONNX node: llama_model/Shape_4\n",
      "[X] Registering tensor: llama_model/Shape_4_output_0 for ONNX tensor: llama_model/Shape_4_output_0\n",
      "[X] llama_model/Shape_4 [Shape] outputs: [llama_model/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape_2 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Shape_4_output_0\n",
      "[X] llama_model/ConstantOfShape_2 [ConstantOfShape] inputs: [llama_model/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_2_output_0 for ONNX tensor: llama_model/ConstantOfShape_2_output_0\n",
      "[X] llama_model/ConstantOfShape_2 [ConstantOfShape] outputs: [llama_model/ConstantOfShape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_26 [Constant]\n",
      "[X] llama_model/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/Constant_26 [Constant] outputs: [llama_model/Constant_26_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Mul [Mul]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_26_output_0\n",
      "[X] llama_model/Mul [Mul] inputs: [llama_model/ConstantOfShape_2_output_0 -> (4)[INT32]], [llama_model/Constant_26_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_26_output_0 for ONNX node: llama_model/Constant_26_output_0\n",
      "[X] Registering layer: llama_model/Mul for ONNX node: llama_model/Mul\n",
      "[X] Registering tensor: llama_model/Mul_output_0 for ONNX tensor: llama_model/Mul_output_0\n",
      "[X] llama_model/Mul [Mul] outputs: [llama_model/Mul_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Equal [Equal]\n",
      "[X] Searching for input: llama_model/Reshape_2_output_0\n",
      "[X] Searching for input: llama_model/Mul_output_0\n",
      "[X] llama_model/Equal [Equal] inputs: [llama_model/Reshape_2_output_0 -> (4)[INT32]], [llama_model/Mul_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Equal for ONNX node: llama_model/Equal\n",
      "[X] Registering tensor: llama_model/Equal_output_0 for ONNX tensor: llama_model/Equal_output_0\n",
      "[X] llama_model/Equal [Equal] outputs: [llama_model/Equal_output_0 -> (4)[BOOL]], \n",
      "[X] Parsing node: llama_model/Where_1 [Where]\n",
      "[X] Searching for input: llama_model/Equal_output_0\n",
      "[X] Searching for input: llama_model/ConstantOfShape_2_output_0\n",
      "[X] Searching for input: llama_model/Reshape_2_output_0\n",
      "[X] llama_model/Where_1 [Where] inputs: [llama_model/Equal_output_0 -> (4)[BOOL]], [llama_model/ConstantOfShape_2_output_0 -> (4)[INT32]], [llama_model/Reshape_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Where_1 for ONNX node: llama_model/Where_1\n",
      "[X] Registering tensor: llama_model/Where_1_output_0 for ONNX tensor: llama_model/Where_1_output_0\n",
      "[X] llama_model/Where_1 [Where] outputs: [llama_model/Where_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Expand [Expand]\n",
      "[X] Searching for input: llama_model/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/Where_1_output_0\n",
      "[X] llama_model/Expand [Expand] inputs: [llama_model/Unsqueeze_8_output_0 -> (1, 1, -1, -1)[FLOAT]], [llama_model/Where_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Expand for ONNX node: llama_model/Expand\n",
      "[X] Registering tensor: llama_model/Expand_output_0 for ONNX tensor: llama_model/Expand_output_0\n",
      "[X] llama_model/Expand [Expand] outputs: [llama_model/Expand_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Shape_5 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_output_0\n",
      "[X] llama_model/Shape_5 [Shape] inputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Registering layer: llama_model/Shape_5 for ONNX node: llama_model/Shape_5\n",
      "[X] Registering tensor: llama_model/Shape_5_output_0 for ONNX tensor: llama_model/Shape_5_output_0\n",
      "[X] llama_model/Shape_5 [Shape] outputs: [llama_model/Shape_5_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_27 [Constant]\n",
      "[X] llama_model/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/Constant_27 [Constant] outputs: [llama_model/Constant_27_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/Shape_5_output_0\n",
      "[X] Searching for input: llama_model/Constant_27_output_0\n",
      "[X] llama_model/Gather_2 [Gather] inputs: [llama_model/Shape_5_output_0 -> (2)[INT32]], [llama_model/Constant_27_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_27_output_0 for ONNX node: llama_model/Constant_27_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather_2 for ONNX node: llama_model/Gather_2\n",
      "[X] Registering tensor: llama_model/Gather_2_output_0 for ONNX tensor: llama_model/Gather_2_output_0\n",
      "[X] llama_model/Gather_2 [Gather] outputs: [llama_model/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_6 [Shape]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_output_0\n",
      "[X] llama_model/Shape_6 [Shape] inputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], \n",
      "[X] Registering layer: llama_model/Shape_6 for ONNX node: llama_model/Shape_6\n",
      "[X] Registering tensor: llama_model/Shape_6_output_0 for ONNX tensor: llama_model/Shape_6_output_0\n",
      "[X] llama_model/Shape_6 [Shape] outputs: [llama_model/Shape_6_output_0 -> (2)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_28 [Constant]\n",
      "[X] llama_model/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/Constant_28 [Constant] outputs: [llama_model/Constant_28_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/Shape_6_output_0\n",
      "[X] Searching for input: llama_model/Constant_28_output_0\n",
      "[X] llama_model/Gather_3 [Gather] inputs: [llama_model/Shape_6_output_0 -> (2)[INT32]], [llama_model/Constant_28_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_28_output_0 for ONNX node: llama_model/Constant_28_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/Gather_3 for ONNX node: llama_model/Gather_3\n",
      "[X] Registering tensor: llama_model/Gather_3_output_0 for ONNX tensor: llama_model/Gather_3_output_0\n",
      "[X] llama_model/Gather_3 [Gather] outputs: [llama_model/Gather_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_29 [Constant]\n",
      "[X] llama_model/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/Constant_29 [Constant] outputs: [llama_model/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_output_0\n",
      "[X] Searching for input: llama_model/Constant_29_output_0\n",
      "[X] llama_model/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/ConstantOfShape_output_0 -> (-1, -1)[BOOL]], [llama_model/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_, _), unsqueezing to: (_, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_12 for ONNX node: llama_model/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/Unsqueeze_12_output_0 for ONNX tensor: llama_model/Unsqueeze_12_output_0\n",
      "[X] llama_model/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/Unsqueeze_12_output_0 -> (-1, 1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_30 [Constant]\n",
      "[X] llama_model/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/Constant_30 [Constant] outputs: [llama_model/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/Constant_30_output_0\n",
      "[X] llama_model/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/Unsqueeze_12_output_0 -> (-1, 1, -1)[BOOL]], [llama_model/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (_, 1, _), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/Unsqueeze_13 for ONNX node: llama_model/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/Unsqueeze_13_output_0 for ONNX tensor: llama_model/Unsqueeze_13_output_0\n",
      "[X] llama_model/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/Unsqueeze_13_output_0 -> (-1, 1, 1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_31 [Constant]\n",
      "[X] llama_model/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/Constant_31 [Constant] outputs: [llama_model/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_14 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/Constant_31_output_0\n",
      "[X] llama_model/Unsqueeze_14 [Unsqueeze] inputs: [llama_model/Gather_2_output_0 -> ()[INT32]], [llama_model/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_14 for ONNX node: llama_model/Unsqueeze_14\n",
      "[X] Registering tensor: llama_model/Unsqueeze_14_output_0 for ONNX tensor: llama_model/Unsqueeze_14_output_0\n",
      "[X] llama_model/Unsqueeze_14 [Unsqueeze] outputs: [llama_model/Unsqueeze_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_32 [Constant]\n",
      "[X] llama_model/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/Constant_32 [Constant] outputs: [llama_model/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_33 [Constant]\n",
      "[X] llama_model/Constant_33 [Constant] inputs: \n",
      "[X] llama_model/Constant_33 [Constant] outputs: [llama_model/Constant_33_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_15 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_1_output_0\n",
      "[X] Searching for input: llama_model/Constant_33_output_0\n",
      "[X] llama_model/Unsqueeze_15 [Unsqueeze] inputs: [llama_model/Gather_1_output_0 -> ()[INT32]], [llama_model/Constant_33_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_15 for ONNX node: llama_model/Unsqueeze_15\n",
      "[X] Registering tensor: llama_model/Unsqueeze_15_output_0 for ONNX tensor: llama_model/Unsqueeze_15_output_0\n",
      "[X] llama_model/Unsqueeze_15 [Unsqueeze] outputs: [llama_model/Unsqueeze_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_34 [Constant]\n",
      "[X] llama_model/Constant_34 [Constant] inputs: \n",
      "[X] llama_model/Constant_34 [Constant] outputs: [llama_model/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Unsqueeze_16 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Gather_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_34_output_0\n",
      "[X] llama_model/Unsqueeze_16 [Unsqueeze] inputs: [llama_model/Gather_3_output_0 -> ()[INT32]], [llama_model/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/Unsqueeze_16 for ONNX node: llama_model/Unsqueeze_16\n",
      "[X] Registering tensor: llama_model/Unsqueeze_16_output_0 for ONNX tensor: llama_model/Unsqueeze_16_output_0\n",
      "[X] llama_model/Unsqueeze_16 [Unsqueeze] outputs: [llama_model/Unsqueeze_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/Unsqueeze_14_output_0\n",
      "[X] Searching for input: llama_model/Constant_32_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_15_output_0\n",
      "[X] Searching for input: llama_model/Unsqueeze_16_output_0\n",
      "[X] llama_model/Concat_5 [Concat] inputs: [llama_model/Unsqueeze_14_output_0 -> (1)[INT32]], [llama_model/Constant_32_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_15_output_0 -> (1)[INT32]], [llama_model/Unsqueeze_16_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_32_output_0 for ONNX node: llama_model/Constant_32_output_0\n",
      "[X] Registering layer: llama_model/Concat_5 for ONNX node: llama_model/Concat_5\n",
      "[X] Registering tensor: llama_model/Concat_5_output_0 for ONNX tensor: llama_model/Concat_5_output_0\n",
      "[X] llama_model/Concat_5 [Concat] outputs: [llama_model/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_35 [Constant]\n",
      "[X] llama_model/Constant_35 [Constant] inputs: \n",
      "[X] llama_model/Constant_35 [Constant] outputs: [llama_model/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/Concat_5_output_0\n",
      "[X] Searching for input: llama_model/Constant_35_output_0\n",
      "[X] llama_model/Reshape_3 [Reshape] inputs: [llama_model/Concat_5_output_0 -> (4)[INT32]], [llama_model/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/Reshape_3 for ONNX node: llama_model/Reshape_3\n",
      "[X] Registering tensor: llama_model/Reshape_3_output_0 for ONNX tensor: llama_model/Reshape_3_output_0\n",
      "[X] llama_model/Reshape_3 [Reshape] outputs: [llama_model/Reshape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Shape_7 [Shape]\n",
      "[X] Searching for input: llama_model/Reshape_3_output_0\n",
      "[X] llama_model/Shape_7 [Shape] inputs: [llama_model/Reshape_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Shape_7 for ONNX node: llama_model/Shape_7\n",
      "[X] Registering tensor: llama_model/Shape_7_output_0 for ONNX tensor: llama_model/Shape_7_output_0\n",
      "[X] llama_model/Shape_7 [Shape] outputs: [llama_model/Shape_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/ConstantOfShape_3 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/Shape_7_output_0\n",
      "[X] llama_model/ConstantOfShape_3 [ConstantOfShape] inputs: [llama_model/Shape_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/ConstantOfShape_3_output_0 for ONNX tensor: llama_model/ConstantOfShape_3_output_0\n",
      "[X] llama_model/ConstantOfShape_3 [ConstantOfShape] outputs: [llama_model/ConstantOfShape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Constant_36 [Constant]\n",
      "[X] llama_model/Constant_36 [Constant] inputs: \n",
      "[X] llama_model/Constant_36 [Constant] outputs: [llama_model/Constant_36_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/ConstantOfShape_3_output_0\n",
      "[X] Searching for input: llama_model/Constant_36_output_0\n",
      "[X] llama_model/Mul_1 [Mul] inputs: [llama_model/ConstantOfShape_3_output_0 -> (4)[INT32]], [llama_model/Constant_36_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/Constant_36_output_0 for ONNX node: llama_model/Constant_36_output_0\n",
      "[X] Registering layer: llama_model/Mul_1 for ONNX node: llama_model/Mul_1\n",
      "[X] Registering tensor: llama_model/Mul_1_output_0 for ONNX tensor: llama_model/Mul_1_output_0\n",
      "[X] llama_model/Mul_1 [Mul] outputs: [llama_model/Mul_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Equal_1 [Equal]\n",
      "[X] Searching for input: llama_model/Reshape_3_output_0\n",
      "[X] Searching for input: llama_model/Mul_1_output_0\n",
      "[X] llama_model/Equal_1 [Equal] inputs: [llama_model/Reshape_3_output_0 -> (4)[INT32]], [llama_model/Mul_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Equal_1 for ONNX node: llama_model/Equal_1\n",
      "[X] Registering tensor: llama_model/Equal_1_output_0 for ONNX tensor: llama_model/Equal_1_output_0\n",
      "[X] llama_model/Equal_1 [Equal] outputs: [llama_model/Equal_1_output_0 -> (4)[BOOL]], \n",
      "[X] Parsing node: llama_model/Where_2 [Where]\n",
      "[X] Searching for input: llama_model/Equal_1_output_0\n",
      "[X] Searching for input: llama_model/ConstantOfShape_3_output_0\n",
      "[X] Searching for input: llama_model/Reshape_3_output_0\n",
      "[X] llama_model/Where_2 [Where] inputs: [llama_model/Equal_1_output_0 -> (4)[BOOL]], [llama_model/ConstantOfShape_3_output_0 -> (4)[INT32]], [llama_model/Reshape_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Where_2 for ONNX node: llama_model/Where_2\n",
      "[X] Registering tensor: llama_model/Where_2_output_0 for ONNX tensor: llama_model/Where_2_output_0\n",
      "[X] llama_model/Where_2 [Where] outputs: [llama_model/Where_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/Expand_1 [Expand]\n",
      "[X] Searching for input: llama_model/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/Where_2_output_0\n",
      "[X] llama_model/Expand_1 [Expand] inputs: [llama_model/Unsqueeze_13_output_0 -> (-1, 1, 1, -1)[BOOL]], [llama_model/Where_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/Expand_1 for ONNX node: llama_model/Expand_1\n",
      "[X] Registering tensor: llama_model/Expand_1_output_0 for ONNX tensor: llama_model/Expand_1_output_0\n",
      "[X] llama_model/Expand_1 [Expand] outputs: [llama_model/Expand_1_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/Expand_1_output_0\n",
      "[X] llama_model/Cast_4 [Cast] inputs: [llama_model/Expand_1_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/Cast_4 for ONNX node: llama_model/Cast_4\n",
      "[X] Registering tensor: llama_model/Cast_4_output_0 for ONNX tensor: llama_model/Cast_4_output_0\n",
      "[X] llama_model/Cast_4 [Cast] outputs: [llama_model/Cast_4_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Constant_37 [Constant]\n",
      "[X] llama_model/Constant_37 [Constant] inputs: \n",
      "[X] llama_model/Constant_37 [Constant] outputs: [llama_model/Constant_37_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/Sub [Sub]\n",
      "[X] Searching for input: llama_model/Constant_37_output_0\n",
      "[X] Searching for input: llama_model/Cast_4_output_0\n",
      "[X] llama_model/Sub [Sub] inputs: [llama_model/Constant_37_output_0 -> ()[FLOAT]], [llama_model/Cast_4_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Constant_37_output_0 for ONNX node: llama_model/Constant_37_output_0\n",
      "[X] Registering layer: llama_model/Sub for ONNX node: llama_model/Sub\n",
      "[X] Registering tensor: llama_model/Sub_output_0 for ONNX tensor: llama_model/Sub_output_0\n",
      "[X] llama_model/Sub [Sub] outputs: [llama_model/Sub_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/Sub_output_0\n",
      "[X] llama_model/Cast_5 [Cast] inputs: [llama_model/Sub_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: bool\n",
      "[X] Registering layer: llama_model/Cast_5 for ONNX node: llama_model/Cast_5\n",
      "[X] Registering tensor: llama_model/Cast_5_output_0 for ONNX tensor: llama_model/Cast_5_output_0\n",
      "[X] llama_model/Cast_5 [Cast] outputs: [llama_model/Cast_5_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Cast_6 [Cast]\n",
      "[X] Searching for input: llama_model/Cast_5_output_0\n",
      "[X] llama_model/Cast_6 [Cast] inputs: [llama_model/Cast_5_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Casting to type: bool\n",
      "[X] Registering layer: llama_model/Cast_6 for ONNX node: llama_model/Cast_6\n",
      "[X] Registering tensor: llama_model/Cast_6_output_0 for ONNX tensor: llama_model/Cast_6_output_0\n",
      "[X] llama_model/Cast_6 [Cast] outputs: [llama_model/Cast_6_output_0 -> (-1, 1, -1, -1)[BOOL]], \n",
      "[X] Parsing node: llama_model/Constant_38 [Constant]\n",
      "[X] llama_model/Constant_38 [Constant] inputs: \n",
      "[X] llama_model/Constant_38 [Constant] outputs: [llama_model/Constant_38_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/Where_3 [Where]\n",
      "[X] Searching for input: llama_model/Cast_6_output_0\n",
      "[X] Searching for input: llama_model/Constant_38_output_0\n",
      "[X] Searching for input: llama_model/Sub_output_0\n",
      "[X] llama_model/Where_3 [Where] inputs: [llama_model/Cast_6_output_0 -> (-1, 1, -1, -1)[BOOL]], [llama_model/Constant_38_output_0 -> ()[FLOAT]], [llama_model/Sub_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Constant_38_output_0 for ONNX node: llama_model/Constant_38_output_0\n",
      "[X] Registering layer: llama_model/Where_3 for ONNX node: llama_model/Where_3\n",
      "[X] Registering tensor: llama_model/Where_3_output_0 for ONNX tensor: llama_model/Where_3_output_0\n",
      "[X] llama_model/Where_3 [Where] outputs: [llama_model/Where_3_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Cast_7 [Cast]\n",
      "[X] Searching for input: llama_model/Where_3_output_0\n",
      "[X] llama_model/Cast_7 [Cast] inputs: [llama_model/Where_3_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/Cast_7 for ONNX node: llama_model/Cast_7\n",
      "[X] Registering tensor: llama_model/Cast_7_output_0 for ONNX tensor: llama_model/Cast_7_output_0\n",
      "[X] llama_model/Cast_7 [Cast] outputs: [llama_model/Cast_7_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/Cast_7_output_0\n",
      "[X] Searching for input: llama_model/Expand_output_0\n",
      "[X] llama_model/Add_1 [Add] inputs: [llama_model/Cast_7_output_0 -> (-1, 1, -1, -1)[FLOAT]], [llama_model/Expand_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/Add_1 for ONNX node: llama_model/Add_1\n",
      "[X] Registering tensor: llama_model/Add_1_output_0 for ONNX tensor: llama_model/Add_1_output_0\n",
      "[X] llama_model/Add_1 [Add] outputs: [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/embed_tokens/Gather_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast [Cast] inputs: [llama_model/embed_tokens/Gather_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Cast for ONNX node: llama_model/layers.0/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast [Cast] outputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.0/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/input_layernorm/Constant [Constant] outputs: [llama_model/layers.0/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Pow [Pow] inputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.0/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Pow for ONNX node: llama_model/layers.0/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Pow [Pow] outputs: [llama_model/layers.0/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.0/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/ReduceMean for ONNX node: llama_model/layers.0/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.0/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.0/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Add [Add] inputs: [llama_model/layers.0/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.0/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.0/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Add for ONNX node: llama_model/layers.0/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Add [Add] outputs: [llama_model/layers.0/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.0/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Sqrt for ONNX node: llama_model/layers.0/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.0/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.0/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Div [Div] inputs: [llama_model/layers.0/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.0/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.0/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Div for ONNX node: llama_model/layers.0/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Div [Div] outputs: [llama_model/layers.0/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul [Mul] inputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Mul for ONNX node: llama_model/layers.0/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul [Mul] outputs: [llama_model/layers.0/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.0/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Cast_1 for ONNX node: llama_model/layers.0/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.0/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.0.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.0.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.0/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/input_layernorm/Mul_1 for ONNX node: llama_model/layers.0/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.0/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape [Shape] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape for ONNX node: llama_model/layers.0/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape [Shape] outputs: [llama_model/layers.0/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant [Constant] outputs: [llama_model/layers.0/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather [Gather] inputs: [llama_model/layers.0/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.0/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather for ONNX node: llama_model/layers.0/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather [Gather] outputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_1 for ONNX node: llama_model/layers.0/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.0/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_1 for ONNX node: llama_model/layers.0/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2038\n",
      "[X] llama_model/layers.0/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2038 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2038 for ONNX node: onnx::MatMul_2038\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2039\n",
      "[X] llama_model/layers.0/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2039 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2039 for ONNX node: onnx::MatMul_2039\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2040\n",
      "[X] llama_model/layers.0/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.0/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2040 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2040 for ONNX node: onnx::MatMul_2040\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_154 [Constant]\n",
      "[X] Constant_154 [Constant] inputs: \n",
      "[X] Constant_154 [Constant] outputs: [onnx::Unsqueeze_228 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_228\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_228 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze for ONNX node: llama_model/layers.0/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_156 [Constant]\n",
      "[X] Constant_156 [Constant] inputs: \n",
      "[X] Constant_156 [Constant] outputs: [onnx::Unsqueeze_230 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_230\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_230 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat for ONNX node: llama_model/layers.0/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat [Concat] outputs: [llama_model/layers.0/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_161 [Constant]\n",
      "[X] Constant_161 [Constant] inputs: \n",
      "[X] Constant_161 [Constant] outputs: [onnx::Unsqueeze_237 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_237\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_237 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_163 [Constant]\n",
      "[X] Constant_163 [Constant] inputs: \n",
      "[X] Constant_163 [Constant] outputs: [onnx::Unsqueeze_239 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_239\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_239 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_1 for ONNX node: llama_model/layers.0/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_168 [Constant]\n",
      "[X] Constant_168 [Constant] inputs: \n",
      "[X] Constant_168 [Constant] outputs: [onnx::Unsqueeze_246 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_246\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_246 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_170 [Constant]\n",
      "[X] Constant_170 [Constant] inputs: \n",
      "[X] Constant_170 [Constant] outputs: [onnx::Unsqueeze_248 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_248\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_248 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_2 for ONNX node: llama_model/layers.0/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape [Reshape] inputs: [llama_model/layers.0/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape for ONNX node: llama_model/layers.0/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose [Transpose] inputs: [llama_model/layers.0/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose for ONNX node: llama_model/layers.0/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.0/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape_1 for ONNX node: llama_model/layers.0/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.0/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_1 for ONNX node: llama_model/layers.0/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.0/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape_2 for ONNX node: llama_model/layers.0/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.0/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_2 for ONNX node: llama_model/layers.0/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_2 for ONNX node: llama_model/layers.0/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.0/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_2 for ONNX node: llama_model/layers.0/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_184 [Constant]\n",
      "[X] Constant_184 [Constant] inputs: \n",
      "[X] Constant_184 [Constant] outputs: [onnx::Slice_264 -> (1, 1, 128, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_264\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_264 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.0/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: onnx::Slice_264 for ONNX node: onnx::Slice_264\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_192 [Constant]\n",
      "[X] Constant_192 [Constant] inputs: \n",
      "[X] Constant_192 [Constant] outputs: [onnx::Slice_274 -> (1, 1, 128, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.0/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_274\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_274 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.0/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: onnx::Slice_274 for ONNX node: onnx::Slice_274\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.0/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.0/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_3 for ONNX node: llama_model/layers.0/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.0/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_3 for ONNX node: llama_model/layers.0/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_203 [Constant]\n",
      "[X] Constant_203 [Constant] inputs: \n",
      "[X] Constant_203 [Constant] outputs: [onnx::Unsqueeze_287 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_287\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_287 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_10_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_10_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_11_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_3 for ONNX node: llama_model/layers.0/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_209 [Constant]\n",
      "[X] Constant_209 [Constant] inputs: \n",
      "[X] Constant_209 [Constant] outputs: [onnx::Unsqueeze_296 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_296\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_296 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_14_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_14_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_13_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_13_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_14_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_14_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_15_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_15_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_4 for ONNX node: llama_model/layers.0/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_215 [Constant]\n",
      "[X] Constant_215 [Constant] inputs: \n",
      "[X] Constant_215 [Constant] outputs: [onnx::Unsqueeze_305 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_305\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_305 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_16_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_16_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_17_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_17_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_18_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_18_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_5 for ONNX node: llama_model/layers.0/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_221 [Constant]\n",
      "[X] Constant_221 [Constant] inputs: \n",
      "[X] Constant_221 [Constant] outputs: [onnx::Unsqueeze_314 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_314\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_314 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_6 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_20_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_6 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_20_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_19_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_21_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_6 for ONNX node: llama_model/layers.0/self_attn/Concat_6\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_6_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_6 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.0/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_4 for ONNX node: llama_model/layers.0/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/ConstantOfShape [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/ConstantOfShape [ConstantOfShape] inputs: [llama_model/layers.0/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/ConstantOfShape_output_0 for ONNX tensor: llama_model/layers.0/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.0/self_attn/ConstantOfShape [ConstantOfShape] outputs: [llama_model/layers.0/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Expand [Expand]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.0/self_attn/Expand [Expand] inputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Expand for ONNX node: llama_model/layers.0/self_attn/Expand\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Expand_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Expand_output_0\n",
      "[X] llama_model/layers.0/self_attn/Expand [Expand] outputs: [llama_model/layers.0/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Tile [Tile]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Expand_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Tile [Tile] inputs: [llama_model/layers.0/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Tile for ONNX node: llama_model/layers.0/self_attn/Tile\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Tile_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Tile_output_0\n",
      "[X] llama_model/layers.0/self_attn/Tile [Tile] outputs: [llama_model/layers.0/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/Reshape_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/Reshape_output_0 -> (1, -1)[INT32]], [llama_model/layers.0/self_attn/Constant_22_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, _), unsqueezing to: (_, _, _)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_10_output_0 -> (1, 1, -1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_23_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Unsqueeze_10_output_0 -> (1, 1, -1)[INT32]], [llama_model/layers.0/self_attn/Constant_23_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (1, 1, _), unsqueezing to: (_, _, _, _)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/GatherElements [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Tile_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.0/self_attn/GatherElements [GatherElements] inputs: [llama_model/layers.0/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/GatherElements for ONNX node: llama_model/layers.0/self_attn/GatherElements\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/GatherElements_output_0 for ONNX tensor: llama_model/layers.0/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.0/self_attn/GatherElements [GatherElements] outputs: [llama_model/layers.0/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_5 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_5 [Shape] inputs: [llama_model/layers.0/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_5 for ONNX node: llama_model/layers.0/self_attn/Shape_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_5 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/ConstantOfShape_1 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/ConstantOfShape_1 [ConstantOfShape] inputs: [llama_model/layers.0/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/ConstantOfShape_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/ConstantOfShape_1 [ConstantOfShape] outputs: [llama_model/layers.0/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Expand_1 [Expand]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Expand_1 [Expand] inputs: [llama_model/layers.0/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Expand_1 for ONNX node: llama_model/layers.0/self_attn/Expand_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Expand_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Expand_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Expand_1 [Expand] outputs: [llama_model/layers.0/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Tile_1 [Tile]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Expand_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.0/self_attn/Tile_1 [Tile] inputs: [llama_model/layers.0/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Tile_1 for ONNX node: llama_model/layers.0/self_attn/Tile_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Tile_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Tile_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Tile_1 [Tile] outputs: [llama_model/layers.0/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/GatherElements_1 [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Tile_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.0/self_attn/GatherElements_1 [GatherElements] inputs: [llama_model/layers.0/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/GatherElements_1 for ONNX node: llama_model/layers.0/self_attn/GatherElements_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/GatherElements_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/GatherElements_1 [GatherElements] outputs: [llama_model/layers.0/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul [Mul] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul for ONNX node: llama_model/layers.0/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul [Mul] outputs: [llama_model/layers.0/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_6 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_6 [Shape] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_6 for ONNX node: llama_model/layers.0/self_attn/Shape_6\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_6_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_6_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_6 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_24_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_6_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_24_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_6_output_0 -> (4)[INT32]], [llama_model/layers.0/self_attn/Constant_24_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_24_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_24_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_4 for ONNX node: llama_model/layers.0/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_4_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_25_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div [Div] inputs: [llama_model/layers.0/self_attn/Gather_4_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_25_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_25_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_25_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Div for ONNX node: llama_model/layers.0/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Div_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div [Div] outputs: [llama_model/layers.0/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Div_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast [Cast] inputs: [llama_model/layers.0/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast for ONNX node: llama_model/layers.0/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast [Cast] outputs: [llama_model/layers.0/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.0/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_1 for ONNX node: llama_model/layers.0/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_28_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_29_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_28_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice for ONNX node: llama_model/layers.0/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice [Slice] outputs: [llama_model/layers.0/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_30_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.0/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_32_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_33 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_33 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_33 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_33_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_31_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_32_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_33_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_31_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_32_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_33_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice_1 for ONNX node: llama_model/layers.0/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.0/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg [Neg] inputs: [llama_model/layers.0/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Neg for ONNX node: llama_model/layers.0/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg [Neg] outputs: [llama_model/layers.0/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_7 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_7 [Concat] inputs: [llama_model/layers.0/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_7 for ONNX node: llama_model/layers.0/self_attn/Concat_7\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_7_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_7 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_7_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.0/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul_1 for ONNX node: llama_model/layers.0/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.0/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add [Add] inputs: [llama_model/layers.0/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Add for ONNX node: llama_model/layers.0/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Add_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add [Add] outputs: [llama_model/layers.0/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul_2 for ONNX node: llama_model/layers.0/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.0/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Shape_7 [Shape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_7 [Shape] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Shape_7 for ONNX node: llama_model/layers.0/self_attn/Shape_7\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Shape_7_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Shape_7_output_0\n",
      "[X] llama_model/layers.0/self_attn/Shape_7 [Shape] outputs: [llama_model/layers.0/self_attn/Shape_7_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_34 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_34 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_34 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_34_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Shape_7_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_34_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.0/self_attn/Shape_7_output_0 -> (4)[INT32]], [llama_model/layers.0/self_attn/Constant_34_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_34_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_34_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Gather_5 for ONNX node: llama_model/layers.0/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.0/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_35 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_35 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_35 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_35_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_35_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_1 [Div] inputs: [llama_model/layers.0/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_35_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_35_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_35_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Div_1 for ONNX node: llama_model/layers.0/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_1 [Div] outputs: [llama_model/layers.0/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.0/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_2 for ONNX node: llama_model/layers.0/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.0/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_3 for ONNX node: llama_model/layers.0/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_36 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_36 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_36 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_36_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_37 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_37 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_37 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_14 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_37_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_14 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_14 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_14\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_14_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_14_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_14 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_38 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_38 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_38 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_39 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_39 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_39 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_39_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_36_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_14_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_38_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_39_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Constant_36_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_38_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_39_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice_2 for ONNX node: llama_model/layers.0/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.0/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_40 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_40 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_40 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_40_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_15 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_40_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_15 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.0/self_attn/Constant_40_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_15 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_15\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_15_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_15_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_15 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_41 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_41 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.0/self_attn/Constant_41 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_42 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_42 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_42 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_42_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_43 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_43 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_43 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_15_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_41_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_42_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_43_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.0/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_41_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_42_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Slice_3 for ONNX node: llama_model/layers.0/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.0/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.0/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Neg_1 for ONNX node: llama_model/layers.0/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.0/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_8 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_8 [Concat] inputs: [llama_model/layers.0/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.0/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_8 for ONNX node: llama_model/layers.0/self_attn/Concat_8\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_8_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_8_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_8 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_8_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.0/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Mul_3 for ONNX node: llama_model/layers.0/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.0/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_1 [Add] inputs: [llama_model/layers.0/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Add_1 for ONNX node: llama_model/layers.0/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_1 [Add] outputs: [llama_model/layers.0/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.0/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_3 for ONNX node: llama_model/layers.0/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul [MatMul] inputs: [llama_model/layers.0/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/MatMul for ONNX node: llama_model/layers.0/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_44 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_44 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_44 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_44_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_44_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_2 [Div] inputs: [llama_model/layers.0/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.0/self_attn/Constant_44_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_44_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_44_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Div_2 for ONNX node: llama_model/layers.0/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Div_2 [Div] outputs: [llama_model/layers.0/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_2 [Add] inputs: [llama_model/layers.0/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Add_2 for ONNX node: llama_model/layers.0/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Add_2 [Add] outputs: [llama_model/layers.0/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/Softmax [Softmax] inputs: [llama_model/layers.0/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Softmax for ONNX node: llama_model/layers.0/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.0/self_attn/Softmax [Softmax] outputs: [llama_model/layers.0/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.0/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_4 for ONNX node: llama_model/layers.0/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.0/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Cast_5 for ONNX node: llama_model/layers.0/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.0/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.0/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.0/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.0/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/MatMul_1 for ONNX node: llama_model/layers.0/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.0/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.0/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.0/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Transpose_4 for ONNX node: llama_model/layers.0/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.0/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.0/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_299 [Constant]\n",
      "[X] Constant_299 [Constant] inputs: \n",
      "[X] Constant_299 [Constant] outputs: [onnx::Unsqueeze_404 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_16 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_404\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_16 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_404 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_16 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_16\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_16_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_16_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_16 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_301 [Constant]\n",
      "[X] Constant_301 [Constant] inputs: \n",
      "[X] Constant_301 [Constant] outputs: [onnx::Unsqueeze_406 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Unsqueeze_17 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_406\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_17 [Unsqueeze] inputs: [llama_model/layers.0/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_406 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Unsqueeze_17 for ONNX node: llama_model/layers.0/self_attn/Unsqueeze_17\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Unsqueeze_17_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Unsqueeze_17_output_0\n",
      "[X] llama_model/layers.0/self_attn/Unsqueeze_17 [Unsqueeze] outputs: [llama_model/layers.0/self_attn/Unsqueeze_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Constant_45 [Constant]\n",
      "[X] llama_model/layers.0/self_attn/Constant_45 [Constant] inputs: \n",
      "[X] llama_model/layers.0/self_attn/Constant_45 [Constant] outputs: [llama_model/layers.0/self_attn/Constant_45_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Concat_9 [Concat]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_16_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_17_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Constant_45_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_9 [Concat] inputs: [llama_model/layers.0/self_attn/Unsqueeze_16_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Unsqueeze_17_output_0 -> (1)[INT32]], [llama_model/layers.0/self_attn/Constant_45_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Constant_45_output_0 for ONNX node: llama_model/layers.0/self_attn/Constant_45_output_0\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Concat_9 for ONNX node: llama_model/layers.0/self_attn/Concat_9\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Concat_9_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.0/self_attn/Concat_9 [Concat] outputs: [llama_model/layers.0/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.0/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.0/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.0/self_attn/Reshape_3 for ONNX node: llama_model/layers.0/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.0/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.0/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.0/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_2072\n",
      "[X] llama_model/layers.0/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.0/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2072 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2072 for ONNX node: onnx::MatMul_2072\n",
      "[X] Registering layer: llama_model/layers.0/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.0/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.0/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/Add [Add] inputs: [llama_model/layers.0/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/Add for ONNX node: llama_model/layers.0/Add\n",
      "[X] Registering tensor: llama_model/layers.0/Add_output_0 for ONNX tensor: llama_model/layers.0/Add_output_0\n",
      "[X] llama_model/layers.0/Add [Add] outputs: [llama_model/layers.0/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/Add_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.0/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Cast for ONNX node: llama_model/layers.0/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.0/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.0/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Pow for ONNX node: llama_model/layers.0/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.0/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.0/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.0/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.0/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.0/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Add for ONNX node: llama_model/layers.0/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.0/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.0/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.0/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.0/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.0/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.0/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Div for ONNX node: llama_model/layers.0/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.0/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Mul for ONNX node: llama_model/layers.0/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.0/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.0/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.0/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.0/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.0.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.0.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.0/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.0/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.0/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2073\n",
      "[X] llama_model/layers.0/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2073 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2073 for ONNX node: onnx::MatMul_2073\n",
      "[X] Registering layer: llama_model/layers.0/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.0/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.0/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.0/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.0/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.0/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.0/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.0/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.0/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/mlp/act_fn/Mul for ONNX node: llama_model/layers.0/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.0/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.0/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.0/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2074\n",
      "[X] llama_model/layers.0/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.0/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2074 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2074 for ONNX node: onnx::MatMul_2074\n",
      "[X] Registering layer: llama_model/layers.0/mlp/up_proj/MatMul for ONNX node: llama_model/layers.0/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.0/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.0/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/Mul [Mul] inputs: [llama_model/layers.0/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.0/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/mlp/Mul for ONNX node: llama_model/layers.0/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.0/mlp/Mul_output_0\n",
      "[X] llama_model/layers.0/mlp/Mul [Mul] outputs: [llama_model/layers.0/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.0/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_2075\n",
      "[X] llama_model/layers.0/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.0/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_2075 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2075 for ONNX node: onnx::MatMul_2075\n",
      "[X] Registering layer: llama_model/layers.0/mlp/down_proj/MatMul for ONNX node: llama_model/layers.0/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.0/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.0/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.0/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.0/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.0/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.0/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.0/Add_1 [Add] inputs: [llama_model/layers.0/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.0/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.0/Add_1 for ONNX node: llama_model/layers.0/Add_1\n",
      "[X] Registering tensor: llama_model/layers.0/Add_1_output_0 for ONNX tensor: llama_model/layers.0/Add_1_output_0\n",
      "[X] llama_model/layers.0/Add_1 [Add] outputs: [llama_model/layers.0/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.0/Add_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast [Cast] inputs: [llama_model/layers.0/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Cast for ONNX node: llama_model/layers.1/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast [Cast] outputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.1/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/input_layernorm/Constant [Constant] outputs: [llama_model/layers.1/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Pow [Pow] inputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.1/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Pow for ONNX node: llama_model/layers.1/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Pow [Pow] outputs: [llama_model/layers.1/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.1/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/ReduceMean for ONNX node: llama_model/layers.1/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.1/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.1/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Add [Add] inputs: [llama_model/layers.1/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.1/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.1/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Add for ONNX node: llama_model/layers.1/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Add [Add] outputs: [llama_model/layers.1/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.1/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Sqrt for ONNX node: llama_model/layers.1/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.1/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.1/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Div [Div] inputs: [llama_model/layers.1/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.1/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.1/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Div for ONNX node: llama_model/layers.1/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Div [Div] outputs: [llama_model/layers.1/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul [Mul] inputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Mul for ONNX node: llama_model/layers.1/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul [Mul] outputs: [llama_model/layers.1/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.1/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Cast_1 for ONNX node: llama_model/layers.1/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.1/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.1.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.1.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.1/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/input_layernorm/Mul_1 for ONNX node: llama_model/layers.1/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.1/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape [Shape] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape for ONNX node: llama_model/layers.1/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape [Shape] outputs: [llama_model/layers.1/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant [Constant] outputs: [llama_model/layers.1/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather [Gather] inputs: [llama_model/layers.1/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.1/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather for ONNX node: llama_model/layers.1/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather [Gather] outputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_1 for ONNX node: llama_model/layers.1/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.1/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_1 for ONNX node: llama_model/layers.1/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2076\n",
      "[X] llama_model/layers.1/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2076 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2076 for ONNX node: onnx::MatMul_2076\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2077\n",
      "[X] llama_model/layers.1/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2077 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2077 for ONNX node: onnx::MatMul_2077\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2078\n",
      "[X] llama_model/layers.1/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.1/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2078 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2078 for ONNX node: onnx::MatMul_2078\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_348 [Constant]\n",
      "[X] Constant_348 [Constant] inputs: \n",
      "[X] Constant_348 [Constant] outputs: [onnx::Unsqueeze_461 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_461\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_461 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze for ONNX node: llama_model/layers.1/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_350 [Constant]\n",
      "[X] Constant_350 [Constant] inputs: \n",
      "[X] Constant_350 [Constant] outputs: [onnx::Unsqueeze_463 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_463\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_463 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat for ONNX node: llama_model/layers.1/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat [Concat] outputs: [llama_model/layers.1/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_355 [Constant]\n",
      "[X] Constant_355 [Constant] inputs: \n",
      "[X] Constant_355 [Constant] outputs: [onnx::Unsqueeze_470 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_470\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_470 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_357 [Constant]\n",
      "[X] Constant_357 [Constant] inputs: \n",
      "[X] Constant_357 [Constant] outputs: [onnx::Unsqueeze_472 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_472\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_472 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_1 for ONNX node: llama_model/layers.1/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_362 [Constant]\n",
      "[X] Constant_362 [Constant] inputs: \n",
      "[X] Constant_362 [Constant] outputs: [onnx::Unsqueeze_479 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_479\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_479 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_364 [Constant]\n",
      "[X] Constant_364 [Constant] inputs: \n",
      "[X] Constant_364 [Constant] outputs: [onnx::Unsqueeze_481 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_481\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_481 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_2 for ONNX node: llama_model/layers.1/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape [Reshape] inputs: [llama_model/layers.1/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape for ONNX node: llama_model/layers.1/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose [Transpose] inputs: [llama_model/layers.1/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose for ONNX node: llama_model/layers.1/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.1/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape_1 for ONNX node: llama_model/layers.1/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.1/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_1 for ONNX node: llama_model/layers.1/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.1/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape_2 for ONNX node: llama_model/layers.1/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.1/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_2 for ONNX node: llama_model/layers.1/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_2 for ONNX node: llama_model/layers.1/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.1/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_2 for ONNX node: llama_model/layers.1/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_264\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_264 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.1/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.1/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_274\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_274 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.1/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.1/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.1/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_3 for ONNX node: llama_model/layers.1/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.1/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_3 for ONNX node: llama_model/layers.1/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_395 [Constant]\n",
      "[X] Constant_395 [Constant] inputs: \n",
      "[X] Constant_395 [Constant] outputs: [onnx::Unsqueeze_518 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_518\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_518 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_10_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_10_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_11_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_3 for ONNX node: llama_model/layers.1/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_401 [Constant]\n",
      "[X] Constant_401 [Constant] inputs: \n",
      "[X] Constant_401 [Constant] outputs: [onnx::Unsqueeze_527 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_527\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_527 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_14_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_14_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_13_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_13_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_14_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_14_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_15_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_15_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_4 for ONNX node: llama_model/layers.1/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_407 [Constant]\n",
      "[X] Constant_407 [Constant] inputs: \n",
      "[X] Constant_407 [Constant] outputs: [onnx::Unsqueeze_536 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_536\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_536 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_16_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_16_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_17_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_17_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_18_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_18_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_5 for ONNX node: llama_model/layers.1/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_413 [Constant]\n",
      "[X] Constant_413 [Constant] inputs: \n",
      "[X] Constant_413 [Constant] outputs: [onnx::Unsqueeze_545 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_545\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_545 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_6 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_20_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_6 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_20_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_19_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_21_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_6 for ONNX node: llama_model/layers.1/self_attn/Concat_6\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_6_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_6 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.1/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_4 for ONNX node: llama_model/layers.1/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/ConstantOfShape [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/ConstantOfShape [ConstantOfShape] inputs: [llama_model/layers.1/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/ConstantOfShape_output_0 for ONNX tensor: llama_model/layers.1/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.1/self_attn/ConstantOfShape [ConstantOfShape] outputs: [llama_model/layers.1/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Expand [Expand]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.1/self_attn/Expand [Expand] inputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Expand for ONNX node: llama_model/layers.1/self_attn/Expand\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Expand_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Expand_output_0\n",
      "[X] llama_model/layers.1/self_attn/Expand [Expand] outputs: [llama_model/layers.1/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Tile [Tile]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Expand_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Tile [Tile] inputs: [llama_model/layers.1/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Tile for ONNX node: llama_model/layers.1/self_attn/Tile\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Tile_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Tile_output_0\n",
      "[X] llama_model/layers.1/self_attn/Tile [Tile] outputs: [llama_model/layers.1/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/GatherElements [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Tile_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.1/self_attn/GatherElements [GatherElements] inputs: [llama_model/layers.1/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/GatherElements for ONNX node: llama_model/layers.1/self_attn/GatherElements\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/GatherElements_output_0 for ONNX tensor: llama_model/layers.1/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.1/self_attn/GatherElements [GatherElements] outputs: [llama_model/layers.1/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_5 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_5 [Shape] inputs: [llama_model/layers.1/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_5 for ONNX node: llama_model/layers.1/self_attn/Shape_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_5 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/ConstantOfShape_1 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/ConstantOfShape_1 [ConstantOfShape] inputs: [llama_model/layers.1/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/ConstantOfShape_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/ConstantOfShape_1 [ConstantOfShape] outputs: [llama_model/layers.1/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Expand_1 [Expand]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Expand_1 [Expand] inputs: [llama_model/layers.1/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Expand_1 for ONNX node: llama_model/layers.1/self_attn/Expand_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Expand_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Expand_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Expand_1 [Expand] outputs: [llama_model/layers.1/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Tile_1 [Tile]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Expand_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.1/self_attn/Tile_1 [Tile] inputs: [llama_model/layers.1/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Tile_1 for ONNX node: llama_model/layers.1/self_attn/Tile_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Tile_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Tile_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Tile_1 [Tile] outputs: [llama_model/layers.1/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/GatherElements_1 [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Tile_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.1/self_attn/GatherElements_1 [GatherElements] inputs: [llama_model/layers.1/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/GatherElements_1 for ONNX node: llama_model/layers.1/self_attn/GatherElements_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/GatherElements_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/GatherElements_1 [GatherElements] outputs: [llama_model/layers.1/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul [Mul] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul for ONNX node: llama_model/layers.1/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul [Mul] outputs: [llama_model/layers.1/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_6 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_6 [Shape] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_6 for ONNX node: llama_model/layers.1/self_attn/Shape_6\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_6_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_6_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_6 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_6_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_6_output_0 -> (4)[INT32]], [llama_model/layers.1/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_22_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_4 for ONNX node: llama_model/layers.1/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_4_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_23_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div [Div] inputs: [llama_model/layers.1/self_attn/Gather_4_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_23_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_23_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Div for ONNX node: llama_model/layers.1/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Div_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div [Div] outputs: [llama_model/layers.1/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Div_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast [Cast] inputs: [llama_model/layers.1/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast for ONNX node: llama_model/layers.1/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast [Cast] outputs: [llama_model/layers.1/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.1/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_1 for ONNX node: llama_model/layers.1/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_24_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Constant_24_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice for ONNX node: llama_model/layers.1/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice [Slice] outputs: [llama_model/layers.1/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_28_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.1/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_30_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_30_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice_1 for ONNX node: llama_model/layers.1/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.1/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg [Neg] inputs: [llama_model/layers.1/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Neg for ONNX node: llama_model/layers.1/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg [Neg] outputs: [llama_model/layers.1/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_7 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_7 [Concat] inputs: [llama_model/layers.1/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_7 for ONNX node: llama_model/layers.1/self_attn/Concat_7\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_7_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_7 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_7_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.1/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul_1 for ONNX node: llama_model/layers.1/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.1/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add [Add] inputs: [llama_model/layers.1/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Add for ONNX node: llama_model/layers.1/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Add_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add [Add] outputs: [llama_model/layers.1/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul_2 for ONNX node: llama_model/layers.1/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.1/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Shape_7 [Shape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_7 [Shape] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Shape_7 for ONNX node: llama_model/layers.1/self_attn/Shape_7\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Shape_7_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Shape_7_output_0\n",
      "[X] llama_model/layers.1/self_attn/Shape_7 [Shape] outputs: [llama_model/layers.1/self_attn/Shape_7_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Shape_7_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.1/self_attn/Shape_7_output_0 -> (4)[INT32]], [llama_model/layers.1/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_32_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Gather_5 for ONNX node: llama_model/layers.1/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.1/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_33 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_33 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_33 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_33_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_1 [Div] inputs: [llama_model/layers.1/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_33_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_33_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Div_1 for ONNX node: llama_model/layers.1/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_1 [Div] outputs: [llama_model/layers.1/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.1/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_2 for ONNX node: llama_model/layers.1/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.1/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_3 for ONNX node: llama_model/layers.1/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_34 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_34 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_34 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_35 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_35 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_35 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_35_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_36 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_36 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_36 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_36_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_37 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_37 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_37 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_34_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_36_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_37_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Constant_34_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_36_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice_2 for ONNX node: llama_model/layers.1/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.1/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_38 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_38 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_38 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_38_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.1/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_39 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_39 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.1/self_attn/Constant_39 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_39_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_40 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_40 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_40 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_40_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_41 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_41 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_41 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_39_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_40_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_41_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.1/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_39_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_40_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Slice_3 for ONNX node: llama_model/layers.1/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.1/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.1/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Neg_1 for ONNX node: llama_model/layers.1/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.1/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_8 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_8 [Concat] inputs: [llama_model/layers.1/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.1/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_8 for ONNX node: llama_model/layers.1/self_attn/Concat_8\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_8_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_8_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_8 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_8_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.1/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Mul_3 for ONNX node: llama_model/layers.1/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.1/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_1 [Add] inputs: [llama_model/layers.1/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Add_1 for ONNX node: llama_model/layers.1/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_1 [Add] outputs: [llama_model/layers.1/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.1/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_3 for ONNX node: llama_model/layers.1/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul [MatMul] inputs: [llama_model/layers.1/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.1/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/MatMul for ONNX node: llama_model/layers.1/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_42 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_42 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_42 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_42_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_2 [Div] inputs: [llama_model/layers.1/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.1/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_42_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_42_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Div_2 for ONNX node: llama_model/layers.1/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Div_2 [Div] outputs: [llama_model/layers.1/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_2 [Add] inputs: [llama_model/layers.1/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Add_2 for ONNX node: llama_model/layers.1/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Add_2 [Add] outputs: [llama_model/layers.1/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/Softmax [Softmax] inputs: [llama_model/layers.1/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Softmax for ONNX node: llama_model/layers.1/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.1/self_attn/Softmax [Softmax] outputs: [llama_model/layers.1/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.1/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_4 for ONNX node: llama_model/layers.1/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.1/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Cast_5 for ONNX node: llama_model/layers.1/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.1/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.1/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.1/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.1/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/MatMul_1 for ONNX node: llama_model/layers.1/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.1/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.1/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.1/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Transpose_4 for ONNX node: llama_model/layers.1/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.1/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.1/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_487 [Constant]\n",
      "[X] Constant_487 [Constant] inputs: \n",
      "[X] Constant_487 [Constant] outputs: [onnx::Unsqueeze_630 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_14 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_630\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_14 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_630 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_14 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_14\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_14_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_14_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_14 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_489 [Constant]\n",
      "[X] Constant_489 [Constant] inputs: \n",
      "[X] Constant_489 [Constant] outputs: [onnx::Unsqueeze_632 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Unsqueeze_15 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_632\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_15 [Unsqueeze] inputs: [llama_model/layers.1/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_632 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Unsqueeze_15 for ONNX node: llama_model/layers.1/self_attn/Unsqueeze_15\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Unsqueeze_15_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Unsqueeze_15_output_0\n",
      "[X] llama_model/layers.1/self_attn/Unsqueeze_15 [Unsqueeze] outputs: [llama_model/layers.1/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Constant_43 [Constant]\n",
      "[X] llama_model/layers.1/self_attn/Constant_43 [Constant] inputs: \n",
      "[X] llama_model/layers.1/self_attn/Constant_43 [Constant] outputs: [llama_model/layers.1/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Concat_9 [Concat]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_14_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Unsqueeze_15_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Constant_43_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_9 [Concat] inputs: [llama_model/layers.1/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], [llama_model/layers.1/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Constant_43_output_0 for ONNX node: llama_model/layers.1/self_attn/Constant_43_output_0\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Concat_9 for ONNX node: llama_model/layers.1/self_attn/Concat_9\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Concat_9_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.1/self_attn/Concat_9 [Concat] outputs: [llama_model/layers.1/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.1/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.1/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.1/self_attn/Reshape_3 for ONNX node: llama_model/layers.1/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.1/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.1/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.1/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_2110\n",
      "[X] llama_model/layers.1/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.1/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2110 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2110 for ONNX node: onnx::MatMul_2110\n",
      "[X] Registering layer: llama_model/layers.1/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.1/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.1/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/Add [Add] inputs: [llama_model/layers.1/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/Add for ONNX node: llama_model/layers.1/Add\n",
      "[X] Registering tensor: llama_model/layers.1/Add_output_0 for ONNX tensor: llama_model/layers.1/Add_output_0\n",
      "[X] llama_model/layers.1/Add [Add] outputs: [llama_model/layers.1/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/Add_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.1/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Cast for ONNX node: llama_model/layers.1/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.1/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.1/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Pow for ONNX node: llama_model/layers.1/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.1/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.1/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.1/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.1/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.1/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Add for ONNX node: llama_model/layers.1/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.1/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.1/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.1/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.1/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.1/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.1/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Div for ONNX node: llama_model/layers.1/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.1/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Mul for ONNX node: llama_model/layers.1/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.1/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.1/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.1/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.1/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.1.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.1.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.1/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.1/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.1/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2111\n",
      "[X] llama_model/layers.1/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2111 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2111 for ONNX node: onnx::MatMul_2111\n",
      "[X] Registering layer: llama_model/layers.1/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.1/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.1/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.1/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.1/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.1/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.1/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.1/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.1/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/mlp/act_fn/Mul for ONNX node: llama_model/layers.1/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.1/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.1/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.1/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2112\n",
      "[X] llama_model/layers.1/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.1/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2112 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2112 for ONNX node: onnx::MatMul_2112\n",
      "[X] Registering layer: llama_model/layers.1/mlp/up_proj/MatMul for ONNX node: llama_model/layers.1/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.1/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.1/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/Mul [Mul] inputs: [llama_model/layers.1/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.1/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/mlp/Mul for ONNX node: llama_model/layers.1/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.1/mlp/Mul_output_0\n",
      "[X] llama_model/layers.1/mlp/Mul [Mul] outputs: [llama_model/layers.1/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.1/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_2113\n",
      "[X] llama_model/layers.1/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.1/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_2113 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2113 for ONNX node: onnx::MatMul_2113\n",
      "[X] Registering layer: llama_model/layers.1/mlp/down_proj/MatMul for ONNX node: llama_model/layers.1/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.1/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.1/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.1/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.1/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.1/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.1/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.1/Add_1 [Add] inputs: [llama_model/layers.1/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.1/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.1/Add_1 for ONNX node: llama_model/layers.1/Add_1\n",
      "[X] Registering tensor: llama_model/layers.1/Add_1_output_0 for ONNX tensor: llama_model/layers.1/Add_1_output_0\n",
      "[X] llama_model/layers.1/Add_1 [Add] outputs: [llama_model/layers.1/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.1/Add_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast [Cast] inputs: [llama_model/layers.1/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Cast for ONNX node: llama_model/layers.2/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast [Cast] outputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.2/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/input_layernorm/Constant [Constant] outputs: [llama_model/layers.2/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Pow [Pow] inputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.2/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Pow for ONNX node: llama_model/layers.2/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Pow [Pow] outputs: [llama_model/layers.2/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.2/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/ReduceMean for ONNX node: llama_model/layers.2/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.2/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.2/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Add [Add] inputs: [llama_model/layers.2/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.2/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.2/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Add for ONNX node: llama_model/layers.2/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Add [Add] outputs: [llama_model/layers.2/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.2/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Sqrt for ONNX node: llama_model/layers.2/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.2/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.2/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Div [Div] inputs: [llama_model/layers.2/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.2/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.2/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Div for ONNX node: llama_model/layers.2/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Div [Div] outputs: [llama_model/layers.2/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul [Mul] inputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Mul for ONNX node: llama_model/layers.2/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul [Mul] outputs: [llama_model/layers.2/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.2/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Cast_1 for ONNX node: llama_model/layers.2/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.2/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.2.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.2.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.2/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/input_layernorm/Mul_1 for ONNX node: llama_model/layers.2/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.2/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape [Shape] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape for ONNX node: llama_model/layers.2/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape [Shape] outputs: [llama_model/layers.2/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant [Constant] outputs: [llama_model/layers.2/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather [Gather] inputs: [llama_model/layers.2/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.2/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather for ONNX node: llama_model/layers.2/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather [Gather] outputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_1 for ONNX node: llama_model/layers.2/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.2/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_1 for ONNX node: llama_model/layers.2/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2114\n",
      "[X] llama_model/layers.2/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2114 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2114 for ONNX node: onnx::MatMul_2114\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2115\n",
      "[X] llama_model/layers.2/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2115 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2115 for ONNX node: onnx::MatMul_2115\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2116\n",
      "[X] llama_model/layers.2/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.2/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2116 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2116 for ONNX node: onnx::MatMul_2116\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_536 [Constant]\n",
      "[X] Constant_536 [Constant] inputs: \n",
      "[X] Constant_536 [Constant] outputs: [onnx::Unsqueeze_687 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_687\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_687 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze for ONNX node: llama_model/layers.2/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_538 [Constant]\n",
      "[X] Constant_538 [Constant] inputs: \n",
      "[X] Constant_538 [Constant] outputs: [onnx::Unsqueeze_689 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_689\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_689 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat for ONNX node: llama_model/layers.2/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat [Concat] outputs: [llama_model/layers.2/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_543 [Constant]\n",
      "[X] Constant_543 [Constant] inputs: \n",
      "[X] Constant_543 [Constant] outputs: [onnx::Unsqueeze_696 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_696\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_696 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_545 [Constant]\n",
      "[X] Constant_545 [Constant] inputs: \n",
      "[X] Constant_545 [Constant] outputs: [onnx::Unsqueeze_698 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_698\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_698 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_1 for ONNX node: llama_model/layers.2/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_550 [Constant]\n",
      "[X] Constant_550 [Constant] inputs: \n",
      "[X] Constant_550 [Constant] outputs: [onnx::Unsqueeze_705 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_705\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_705 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_552 [Constant]\n",
      "[X] Constant_552 [Constant] inputs: \n",
      "[X] Constant_552 [Constant] outputs: [onnx::Unsqueeze_707 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_707\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_707 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_2 for ONNX node: llama_model/layers.2/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape [Reshape] inputs: [llama_model/layers.2/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape for ONNX node: llama_model/layers.2/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose [Transpose] inputs: [llama_model/layers.2/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose for ONNX node: llama_model/layers.2/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.2/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape_1 for ONNX node: llama_model/layers.2/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.2/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_1 for ONNX node: llama_model/layers.2/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.2/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape_2 for ONNX node: llama_model/layers.2/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.2/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_2 for ONNX node: llama_model/layers.2/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_2 for ONNX node: llama_model/layers.2/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.2/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_2 for ONNX node: llama_model/layers.2/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_264\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_264 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.2/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.2/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_274\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_274 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.2/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.2/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.2/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_3 for ONNX node: llama_model/layers.2/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.2/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_3 for ONNX node: llama_model/layers.2/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_583 [Constant]\n",
      "[X] Constant_583 [Constant] inputs: \n",
      "[X] Constant_583 [Constant] outputs: [onnx::Unsqueeze_744 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_744\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_744 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_10_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_10_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_11_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_3 for ONNX node: llama_model/layers.2/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_589 [Constant]\n",
      "[X] Constant_589 [Constant] inputs: \n",
      "[X] Constant_589 [Constant] outputs: [onnx::Unsqueeze_753 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_753\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_753 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_14_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_14_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_13_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_13_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_14_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_14_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_15_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_15_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_4 for ONNX node: llama_model/layers.2/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_595 [Constant]\n",
      "[X] Constant_595 [Constant] inputs: \n",
      "[X] Constant_595 [Constant] outputs: [onnx::Unsqueeze_762 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_762\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_762 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_16_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_16_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_17_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_17_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_18_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_18_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_5 for ONNX node: llama_model/layers.2/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_601 [Constant]\n",
      "[X] Constant_601 [Constant] inputs: \n",
      "[X] Constant_601 [Constant] outputs: [onnx::Unsqueeze_771 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_771\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_771 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_6 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_20_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_6 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_20_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_19_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_21_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_6 for ONNX node: llama_model/layers.2/self_attn/Concat_6\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_6_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_6 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.2/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_4 for ONNX node: llama_model/layers.2/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/ConstantOfShape [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/ConstantOfShape [ConstantOfShape] inputs: [llama_model/layers.2/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/ConstantOfShape_output_0 for ONNX tensor: llama_model/layers.2/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.2/self_attn/ConstantOfShape [ConstantOfShape] outputs: [llama_model/layers.2/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Expand [Expand]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.2/self_attn/Expand [Expand] inputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Expand for ONNX node: llama_model/layers.2/self_attn/Expand\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Expand_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Expand_output_0\n",
      "[X] llama_model/layers.2/self_attn/Expand [Expand] outputs: [llama_model/layers.2/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Tile [Tile]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Expand_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Tile [Tile] inputs: [llama_model/layers.2/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Tile for ONNX node: llama_model/layers.2/self_attn/Tile\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Tile_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Tile_output_0\n",
      "[X] llama_model/layers.2/self_attn/Tile [Tile] outputs: [llama_model/layers.2/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/GatherElements [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Tile_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.2/self_attn/GatherElements [GatherElements] inputs: [llama_model/layers.2/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/GatherElements for ONNX node: llama_model/layers.2/self_attn/GatherElements\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/GatherElements_output_0 for ONNX tensor: llama_model/layers.2/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.2/self_attn/GatherElements [GatherElements] outputs: [llama_model/layers.2/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_5 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_5 [Shape] inputs: [llama_model/layers.2/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_5 for ONNX node: llama_model/layers.2/self_attn/Shape_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_5 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/ConstantOfShape_1 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/ConstantOfShape_1 [ConstantOfShape] inputs: [llama_model/layers.2/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/ConstantOfShape_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/ConstantOfShape_1 [ConstantOfShape] outputs: [llama_model/layers.2/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Expand_1 [Expand]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Expand_1 [Expand] inputs: [llama_model/layers.2/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Expand_1 for ONNX node: llama_model/layers.2/self_attn/Expand_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Expand_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Expand_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Expand_1 [Expand] outputs: [llama_model/layers.2/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Tile_1 [Tile]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Expand_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.2/self_attn/Tile_1 [Tile] inputs: [llama_model/layers.2/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Tile_1 for ONNX node: llama_model/layers.2/self_attn/Tile_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Tile_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Tile_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Tile_1 [Tile] outputs: [llama_model/layers.2/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/GatherElements_1 [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Tile_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.2/self_attn/GatherElements_1 [GatherElements] inputs: [llama_model/layers.2/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/GatherElements_1 for ONNX node: llama_model/layers.2/self_attn/GatherElements_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/GatherElements_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/GatherElements_1 [GatherElements] outputs: [llama_model/layers.2/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul [Mul] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul for ONNX node: llama_model/layers.2/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul [Mul] outputs: [llama_model/layers.2/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_6 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_6 [Shape] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_6 for ONNX node: llama_model/layers.2/self_attn/Shape_6\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_6_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_6_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_6 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_6_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_6_output_0 -> (4)[INT32]], [llama_model/layers.2/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_22_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_4 for ONNX node: llama_model/layers.2/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_4_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_23_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div [Div] inputs: [llama_model/layers.2/self_attn/Gather_4_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_23_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_23_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Div for ONNX node: llama_model/layers.2/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Div_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div [Div] outputs: [llama_model/layers.2/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Div_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast [Cast] inputs: [llama_model/layers.2/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast for ONNX node: llama_model/layers.2/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast [Cast] outputs: [llama_model/layers.2/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.2/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_1 for ONNX node: llama_model/layers.2/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_24_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Constant_24_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice for ONNX node: llama_model/layers.2/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice [Slice] outputs: [llama_model/layers.2/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_28_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.2/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_30_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_30_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice_1 for ONNX node: llama_model/layers.2/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.2/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg [Neg] inputs: [llama_model/layers.2/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Neg for ONNX node: llama_model/layers.2/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg [Neg] outputs: [llama_model/layers.2/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_7 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_7 [Concat] inputs: [llama_model/layers.2/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_7 for ONNX node: llama_model/layers.2/self_attn/Concat_7\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_7_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_7 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_7_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.2/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul_1 for ONNX node: llama_model/layers.2/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.2/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add [Add] inputs: [llama_model/layers.2/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Add for ONNX node: llama_model/layers.2/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Add_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add [Add] outputs: [llama_model/layers.2/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul_2 for ONNX node: llama_model/layers.2/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.2/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Shape_7 [Shape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_7 [Shape] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Shape_7 for ONNX node: llama_model/layers.2/self_attn/Shape_7\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Shape_7_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Shape_7_output_0\n",
      "[X] llama_model/layers.2/self_attn/Shape_7 [Shape] outputs: [llama_model/layers.2/self_attn/Shape_7_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Shape_7_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.2/self_attn/Shape_7_output_0 -> (4)[INT32]], [llama_model/layers.2/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_32_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Gather_5 for ONNX node: llama_model/layers.2/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.2/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_33 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_33 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_33 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_33_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_1 [Div] inputs: [llama_model/layers.2/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_33_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_33_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Div_1 for ONNX node: llama_model/layers.2/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_1 [Div] outputs: [llama_model/layers.2/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.2/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_2 for ONNX node: llama_model/layers.2/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.2/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_3 for ONNX node: llama_model/layers.2/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_34 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_34 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_34 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_35 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_35 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_35 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_35_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_36 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_36 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_36 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_36_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_37 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_37 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_37 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_34_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_36_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_37_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Constant_34_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_36_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice_2 for ONNX node: llama_model/layers.2/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.2/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_38 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_38 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_38 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_38_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.2/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_39 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_39 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.2/self_attn/Constant_39 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_39_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_40 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_40 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_40 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_40_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_41 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_41 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_41 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_39_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_40_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_41_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.2/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_39_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_40_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Slice_3 for ONNX node: llama_model/layers.2/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.2/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.2/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Neg_1 for ONNX node: llama_model/layers.2/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.2/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_8 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_8 [Concat] inputs: [llama_model/layers.2/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.2/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_8 for ONNX node: llama_model/layers.2/self_attn/Concat_8\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_8_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_8_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_8 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_8_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.2/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Mul_3 for ONNX node: llama_model/layers.2/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.2/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_1 [Add] inputs: [llama_model/layers.2/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Add_1 for ONNX node: llama_model/layers.2/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_1 [Add] outputs: [llama_model/layers.2/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.2/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_3 for ONNX node: llama_model/layers.2/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul [MatMul] inputs: [llama_model/layers.2/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.2/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/MatMul for ONNX node: llama_model/layers.2/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_42 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_42 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_42 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_42_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_2 [Div] inputs: [llama_model/layers.2/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.2/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_42_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_42_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Div_2 for ONNX node: llama_model/layers.2/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Div_2 [Div] outputs: [llama_model/layers.2/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_2 [Add] inputs: [llama_model/layers.2/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Add_2 for ONNX node: llama_model/layers.2/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Add_2 [Add] outputs: [llama_model/layers.2/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/Softmax [Softmax] inputs: [llama_model/layers.2/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Softmax for ONNX node: llama_model/layers.2/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.2/self_attn/Softmax [Softmax] outputs: [llama_model/layers.2/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.2/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_4 for ONNX node: llama_model/layers.2/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.2/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Cast_5 for ONNX node: llama_model/layers.2/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.2/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.2/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.2/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.2/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/MatMul_1 for ONNX node: llama_model/layers.2/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.2/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.2/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.2/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Transpose_4 for ONNX node: llama_model/layers.2/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.2/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.2/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_675 [Constant]\n",
      "[X] Constant_675 [Constant] inputs: \n",
      "[X] Constant_675 [Constant] outputs: [onnx::Unsqueeze_856 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_14 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_856\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_14 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_856 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_14 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_14\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_14_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_14_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_14 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_677 [Constant]\n",
      "[X] Constant_677 [Constant] inputs: \n",
      "[X] Constant_677 [Constant] outputs: [onnx::Unsqueeze_858 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Unsqueeze_15 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_858\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_15 [Unsqueeze] inputs: [llama_model/layers.2/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_858 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Unsqueeze_15 for ONNX node: llama_model/layers.2/self_attn/Unsqueeze_15\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Unsqueeze_15_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Unsqueeze_15_output_0\n",
      "[X] llama_model/layers.2/self_attn/Unsqueeze_15 [Unsqueeze] outputs: [llama_model/layers.2/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Constant_43 [Constant]\n",
      "[X] llama_model/layers.2/self_attn/Constant_43 [Constant] inputs: \n",
      "[X] llama_model/layers.2/self_attn/Constant_43 [Constant] outputs: [llama_model/layers.2/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Concat_9 [Concat]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_14_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Unsqueeze_15_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Constant_43_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_9 [Concat] inputs: [llama_model/layers.2/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], [llama_model/layers.2/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Constant_43_output_0 for ONNX node: llama_model/layers.2/self_attn/Constant_43_output_0\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Concat_9 for ONNX node: llama_model/layers.2/self_attn/Concat_9\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Concat_9_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.2/self_attn/Concat_9 [Concat] outputs: [llama_model/layers.2/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.2/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.2/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.2/self_attn/Reshape_3 for ONNX node: llama_model/layers.2/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.2/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.2/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.2/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_2148\n",
      "[X] llama_model/layers.2/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.2/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2148 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2148 for ONNX node: onnx::MatMul_2148\n",
      "[X] Registering layer: llama_model/layers.2/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.2/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.2/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/Add [Add] inputs: [llama_model/layers.2/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/Add for ONNX node: llama_model/layers.2/Add\n",
      "[X] Registering tensor: llama_model/layers.2/Add_output_0 for ONNX tensor: llama_model/layers.2/Add_output_0\n",
      "[X] llama_model/layers.2/Add [Add] outputs: [llama_model/layers.2/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/Add_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.2/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Cast for ONNX node: llama_model/layers.2/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.2/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.2/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Pow for ONNX node: llama_model/layers.2/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.2/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.2/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.2/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.2/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.2/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Add for ONNX node: llama_model/layers.2/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.2/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.2/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.2/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.2/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.2/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.2/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Div for ONNX node: llama_model/layers.2/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.2/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Mul for ONNX node: llama_model/layers.2/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.2/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.2/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.2/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.2/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.2.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.2.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.2/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.2/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.2/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2149\n",
      "[X] llama_model/layers.2/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2149 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2149 for ONNX node: onnx::MatMul_2149\n",
      "[X] Registering layer: llama_model/layers.2/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.2/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.2/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.2/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.2/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.2/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.2/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.2/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.2/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/mlp/act_fn/Mul for ONNX node: llama_model/layers.2/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.2/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.2/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.2/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2150\n",
      "[X] llama_model/layers.2/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.2/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2150 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2150 for ONNX node: onnx::MatMul_2150\n",
      "[X] Registering layer: llama_model/layers.2/mlp/up_proj/MatMul for ONNX node: llama_model/layers.2/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.2/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.2/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/Mul [Mul] inputs: [llama_model/layers.2/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.2/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/mlp/Mul for ONNX node: llama_model/layers.2/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.2/mlp/Mul_output_0\n",
      "[X] llama_model/layers.2/mlp/Mul [Mul] outputs: [llama_model/layers.2/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.2/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_2151\n",
      "[X] llama_model/layers.2/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.2/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_2151 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2151 for ONNX node: onnx::MatMul_2151\n",
      "[X] Registering layer: llama_model/layers.2/mlp/down_proj/MatMul for ONNX node: llama_model/layers.2/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.2/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.2/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.2/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.2/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.2/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.2/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.2/Add_1 [Add] inputs: [llama_model/layers.2/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.2/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.2/Add_1 for ONNX node: llama_model/layers.2/Add_1\n",
      "[X] Registering tensor: llama_model/layers.2/Add_1_output_0 for ONNX tensor: llama_model/layers.2/Add_1_output_0\n",
      "[X] llama_model/layers.2/Add_1 [Add] outputs: [llama_model/layers.2/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.2/Add_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast [Cast] inputs: [llama_model/layers.2/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Cast for ONNX node: llama_model/layers.3/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast [Cast] outputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.3/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/input_layernorm/Constant [Constant] outputs: [llama_model/layers.3/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Pow [Pow] inputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.3/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Pow for ONNX node: llama_model/layers.3/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Pow [Pow] outputs: [llama_model/layers.3/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.3/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/ReduceMean for ONNX node: llama_model/layers.3/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.3/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.3/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Add [Add] inputs: [llama_model/layers.3/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.3/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.3/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Add for ONNX node: llama_model/layers.3/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Add [Add] outputs: [llama_model/layers.3/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.3/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Sqrt for ONNX node: llama_model/layers.3/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.3/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.3/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Div [Div] inputs: [llama_model/layers.3/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.3/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.3/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Div for ONNX node: llama_model/layers.3/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Div [Div] outputs: [llama_model/layers.3/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul [Mul] inputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Mul for ONNX node: llama_model/layers.3/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul [Mul] outputs: [llama_model/layers.3/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.3/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Cast_1 for ONNX node: llama_model/layers.3/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.3/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.3.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.3.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.3/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/input_layernorm/Mul_1 for ONNX node: llama_model/layers.3/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.3/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape [Shape] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape for ONNX node: llama_model/layers.3/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape [Shape] outputs: [llama_model/layers.3/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant [Constant] outputs: [llama_model/layers.3/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather [Gather] inputs: [llama_model/layers.3/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.3/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather for ONNX node: llama_model/layers.3/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather [Gather] outputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_1 for ONNX node: llama_model/layers.3/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.3/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_1 for ONNX node: llama_model/layers.3/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2152\n",
      "[X] llama_model/layers.3/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2152 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2152 for ONNX node: onnx::MatMul_2152\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2153\n",
      "[X] llama_model/layers.3/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2153 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2153 for ONNX node: onnx::MatMul_2153\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2154\n",
      "[X] llama_model/layers.3/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.3/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2154 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2154 for ONNX node: onnx::MatMul_2154\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_724 [Constant]\n",
      "[X] Constant_724 [Constant] inputs: \n",
      "[X] Constant_724 [Constant] outputs: [onnx::Unsqueeze_913 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_913\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_913 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze for ONNX node: llama_model/layers.3/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_726 [Constant]\n",
      "[X] Constant_726 [Constant] inputs: \n",
      "[X] Constant_726 [Constant] outputs: [onnx::Unsqueeze_915 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_915\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_915 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat for ONNX node: llama_model/layers.3/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat [Concat] outputs: [llama_model/layers.3/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_731 [Constant]\n",
      "[X] Constant_731 [Constant] inputs: \n",
      "[X] Constant_731 [Constant] outputs: [onnx::Unsqueeze_922 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_922\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_922 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_733 [Constant]\n",
      "[X] Constant_733 [Constant] inputs: \n",
      "[X] Constant_733 [Constant] outputs: [onnx::Unsqueeze_924 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_924\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_924 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_1 for ONNX node: llama_model/layers.3/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_738 [Constant]\n",
      "[X] Constant_738 [Constant] inputs: \n",
      "[X] Constant_738 [Constant] outputs: [onnx::Unsqueeze_931 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_931\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_931 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_740 [Constant]\n",
      "[X] Constant_740 [Constant] inputs: \n",
      "[X] Constant_740 [Constant] outputs: [onnx::Unsqueeze_933 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_933\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_933 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_2 for ONNX node: llama_model/layers.3/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape [Reshape] inputs: [llama_model/layers.3/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape for ONNX node: llama_model/layers.3/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose [Transpose] inputs: [llama_model/layers.3/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose for ONNX node: llama_model/layers.3/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.3/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape_1 for ONNX node: llama_model/layers.3/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.3/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_1 for ONNX node: llama_model/layers.3/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.3/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape_2 for ONNX node: llama_model/layers.3/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.3/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_2 for ONNX node: llama_model/layers.3/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_2 for ONNX node: llama_model/layers.3/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.3/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_2 for ONNX node: llama_model/layers.3/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_264\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_264 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.3/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.3/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_274\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_274 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.3/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.3/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.3/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_3 for ONNX node: llama_model/layers.3/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.3/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_3 for ONNX node: llama_model/layers.3/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_771 [Constant]\n",
      "[X] Constant_771 [Constant] inputs: \n",
      "[X] Constant_771 [Constant] outputs: [onnx::Unsqueeze_970 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_970\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_970 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_10_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_10_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_11_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_3 for ONNX node: llama_model/layers.3/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_777 [Constant]\n",
      "[X] Constant_777 [Constant] inputs: \n",
      "[X] Constant_777 [Constant] outputs: [onnx::Unsqueeze_979 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_979\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_979 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_14_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_14_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_13_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_13_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_14_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_14_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_15_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_15_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_4 for ONNX node: llama_model/layers.3/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_783 [Constant]\n",
      "[X] Constant_783 [Constant] inputs: \n",
      "[X] Constant_783 [Constant] outputs: [onnx::Unsqueeze_988 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_988\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_988 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_16_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_16_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_17_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_17_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_18_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_18_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_5 for ONNX node: llama_model/layers.3/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_789 [Constant]\n",
      "[X] Constant_789 [Constant] inputs: \n",
      "[X] Constant_789 [Constant] outputs: [onnx::Unsqueeze_997 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_997\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_997 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_6 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_20_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_6 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_20_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_19_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_21_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_6 for ONNX node: llama_model/layers.3/self_attn/Concat_6\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_6_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_6 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.3/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_4 for ONNX node: llama_model/layers.3/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/ConstantOfShape [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/ConstantOfShape [ConstantOfShape] inputs: [llama_model/layers.3/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/ConstantOfShape_output_0 for ONNX tensor: llama_model/layers.3/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.3/self_attn/ConstantOfShape [ConstantOfShape] outputs: [llama_model/layers.3/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Expand [Expand]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.3/self_attn/Expand [Expand] inputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Expand for ONNX node: llama_model/layers.3/self_attn/Expand\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Expand_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Expand_output_0\n",
      "[X] llama_model/layers.3/self_attn/Expand [Expand] outputs: [llama_model/layers.3/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Tile [Tile]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Expand_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Tile [Tile] inputs: [llama_model/layers.3/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Tile for ONNX node: llama_model/layers.3/self_attn/Tile\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Tile_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Tile_output_0\n",
      "[X] llama_model/layers.3/self_attn/Tile [Tile] outputs: [llama_model/layers.3/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/GatherElements [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Tile_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.3/self_attn/GatherElements [GatherElements] inputs: [llama_model/layers.3/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/GatherElements for ONNX node: llama_model/layers.3/self_attn/GatherElements\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/GatherElements_output_0 for ONNX tensor: llama_model/layers.3/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.3/self_attn/GatherElements [GatherElements] outputs: [llama_model/layers.3/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_5 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_5 [Shape] inputs: [llama_model/layers.3/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_5 for ONNX node: llama_model/layers.3/self_attn/Shape_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_5 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/ConstantOfShape_1 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/ConstantOfShape_1 [ConstantOfShape] inputs: [llama_model/layers.3/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/ConstantOfShape_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/ConstantOfShape_1 [ConstantOfShape] outputs: [llama_model/layers.3/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Expand_1 [Expand]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Expand_1 [Expand] inputs: [llama_model/layers.3/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Expand_1 for ONNX node: llama_model/layers.3/self_attn/Expand_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Expand_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Expand_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Expand_1 [Expand] outputs: [llama_model/layers.3/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Tile_1 [Tile]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Expand_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.3/self_attn/Tile_1 [Tile] inputs: [llama_model/layers.3/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Tile_1 for ONNX node: llama_model/layers.3/self_attn/Tile_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Tile_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Tile_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Tile_1 [Tile] outputs: [llama_model/layers.3/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/GatherElements_1 [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Tile_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.3/self_attn/GatherElements_1 [GatherElements] inputs: [llama_model/layers.3/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/GatherElements_1 for ONNX node: llama_model/layers.3/self_attn/GatherElements_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/GatherElements_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/GatherElements_1 [GatherElements] outputs: [llama_model/layers.3/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul [Mul] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul for ONNX node: llama_model/layers.3/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul [Mul] outputs: [llama_model/layers.3/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_6 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_6 [Shape] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_6 for ONNX node: llama_model/layers.3/self_attn/Shape_6\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_6_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_6_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_6 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_6_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_6_output_0 -> (4)[INT32]], [llama_model/layers.3/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_22_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_4 for ONNX node: llama_model/layers.3/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_4_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_23_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div [Div] inputs: [llama_model/layers.3/self_attn/Gather_4_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_23_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_23_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Div for ONNX node: llama_model/layers.3/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Div_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div [Div] outputs: [llama_model/layers.3/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Div_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast [Cast] inputs: [llama_model/layers.3/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast for ONNX node: llama_model/layers.3/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast [Cast] outputs: [llama_model/layers.3/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.3/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_1 for ONNX node: llama_model/layers.3/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_24_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Constant_24_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice for ONNX node: llama_model/layers.3/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice [Slice] outputs: [llama_model/layers.3/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_28_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.3/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_30_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_30_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice_1 for ONNX node: llama_model/layers.3/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.3/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg [Neg] inputs: [llama_model/layers.3/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Neg for ONNX node: llama_model/layers.3/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg [Neg] outputs: [llama_model/layers.3/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_7 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_7 [Concat] inputs: [llama_model/layers.3/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_7 for ONNX node: llama_model/layers.3/self_attn/Concat_7\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_7_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_7 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_7_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.3/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul_1 for ONNX node: llama_model/layers.3/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.3/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add [Add] inputs: [llama_model/layers.3/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Add for ONNX node: llama_model/layers.3/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Add_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add [Add] outputs: [llama_model/layers.3/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul_2 for ONNX node: llama_model/layers.3/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.3/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Shape_7 [Shape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_7 [Shape] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Shape_7 for ONNX node: llama_model/layers.3/self_attn/Shape_7\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Shape_7_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Shape_7_output_0\n",
      "[X] llama_model/layers.3/self_attn/Shape_7 [Shape] outputs: [llama_model/layers.3/self_attn/Shape_7_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Shape_7_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.3/self_attn/Shape_7_output_0 -> (4)[INT32]], [llama_model/layers.3/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_32_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Gather_5 for ONNX node: llama_model/layers.3/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.3/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_33 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_33 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_33 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_33_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_1 [Div] inputs: [llama_model/layers.3/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_33_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_33_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Div_1 for ONNX node: llama_model/layers.3/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_1 [Div] outputs: [llama_model/layers.3/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.3/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_2 for ONNX node: llama_model/layers.3/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.3/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_3 for ONNX node: llama_model/layers.3/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_34 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_34 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_34 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_35 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_35 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_35 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_35_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_36 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_36 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_36 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_36_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_37 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_37 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_37 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_34_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_36_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_37_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Constant_34_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_36_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice_2 for ONNX node: llama_model/layers.3/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.3/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_38 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_38 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_38 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_38_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.3/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_39 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_39 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.3/self_attn/Constant_39 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_39_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_40 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_40 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_40 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_40_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_41 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_41 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_41 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_39_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_40_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_41_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.3/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_39_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_40_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Slice_3 for ONNX node: llama_model/layers.3/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.3/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.3/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Neg_1 for ONNX node: llama_model/layers.3/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.3/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_8 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_8 [Concat] inputs: [llama_model/layers.3/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.3/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_8 for ONNX node: llama_model/layers.3/self_attn/Concat_8\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_8_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_8_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_8 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_8_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.3/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Mul_3 for ONNX node: llama_model/layers.3/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.3/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_1 [Add] inputs: [llama_model/layers.3/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Add_1 for ONNX node: llama_model/layers.3/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_1 [Add] outputs: [llama_model/layers.3/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.3/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_3 for ONNX node: llama_model/layers.3/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul [MatMul] inputs: [llama_model/layers.3/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.3/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/MatMul for ONNX node: llama_model/layers.3/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_42 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_42 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_42 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_42_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_2 [Div] inputs: [llama_model/layers.3/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.3/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_42_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_42_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Div_2 for ONNX node: llama_model/layers.3/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Div_2 [Div] outputs: [llama_model/layers.3/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_2 [Add] inputs: [llama_model/layers.3/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Add_2 for ONNX node: llama_model/layers.3/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Add_2 [Add] outputs: [llama_model/layers.3/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/Softmax [Softmax] inputs: [llama_model/layers.3/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Softmax for ONNX node: llama_model/layers.3/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.3/self_attn/Softmax [Softmax] outputs: [llama_model/layers.3/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.3/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_4 for ONNX node: llama_model/layers.3/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.3/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Cast_5 for ONNX node: llama_model/layers.3/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.3/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.3/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.3/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.3/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/MatMul_1 for ONNX node: llama_model/layers.3/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.3/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.3/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.3/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Transpose_4 for ONNX node: llama_model/layers.3/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.3/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.3/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_863 [Constant]\n",
      "[X] Constant_863 [Constant] inputs: \n",
      "[X] Constant_863 [Constant] outputs: [onnx::Unsqueeze_1082 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_14 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1082\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_14 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1082 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_14 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_14\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_14_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_14_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_14 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_865 [Constant]\n",
      "[X] Constant_865 [Constant] inputs: \n",
      "[X] Constant_865 [Constant] outputs: [onnx::Unsqueeze_1084 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Unsqueeze_15 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1084\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_15 [Unsqueeze] inputs: [llama_model/layers.3/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1084 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Unsqueeze_15 for ONNX node: llama_model/layers.3/self_attn/Unsqueeze_15\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Unsqueeze_15_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Unsqueeze_15_output_0\n",
      "[X] llama_model/layers.3/self_attn/Unsqueeze_15 [Unsqueeze] outputs: [llama_model/layers.3/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Constant_43 [Constant]\n",
      "[X] llama_model/layers.3/self_attn/Constant_43 [Constant] inputs: \n",
      "[X] llama_model/layers.3/self_attn/Constant_43 [Constant] outputs: [llama_model/layers.3/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Concat_9 [Concat]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_14_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Unsqueeze_15_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Constant_43_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_9 [Concat] inputs: [llama_model/layers.3/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], [llama_model/layers.3/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Constant_43_output_0 for ONNX node: llama_model/layers.3/self_attn/Constant_43_output_0\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Concat_9 for ONNX node: llama_model/layers.3/self_attn/Concat_9\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Concat_9_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.3/self_attn/Concat_9 [Concat] outputs: [llama_model/layers.3/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.3/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.3/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.3/self_attn/Reshape_3 for ONNX node: llama_model/layers.3/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.3/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.3/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.3/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_2186\n",
      "[X] llama_model/layers.3/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.3/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2186 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2186 for ONNX node: onnx::MatMul_2186\n",
      "[X] Registering layer: llama_model/layers.3/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.3/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.3/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/Add [Add] inputs: [llama_model/layers.3/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/Add for ONNX node: llama_model/layers.3/Add\n",
      "[X] Registering tensor: llama_model/layers.3/Add_output_0 for ONNX tensor: llama_model/layers.3/Add_output_0\n",
      "[X] llama_model/layers.3/Add [Add] outputs: [llama_model/layers.3/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/Add_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.3/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Cast for ONNX node: llama_model/layers.3/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.3/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.3/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Pow for ONNX node: llama_model/layers.3/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.3/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.3/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.3/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.3/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.3/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Add for ONNX node: llama_model/layers.3/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.3/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.3/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.3/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.3/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.3/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.3/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Div for ONNX node: llama_model/layers.3/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.3/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Mul for ONNX node: llama_model/layers.3/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.3/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.3/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.3/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.3/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.3.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.3.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.3/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.3/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.3/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2187\n",
      "[X] llama_model/layers.3/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2187 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2187 for ONNX node: onnx::MatMul_2187\n",
      "[X] Registering layer: llama_model/layers.3/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.3/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.3/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.3/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.3/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.3/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.3/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.3/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.3/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/mlp/act_fn/Mul for ONNX node: llama_model/layers.3/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.3/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.3/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.3/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2188\n",
      "[X] llama_model/layers.3/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.3/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2188 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2188 for ONNX node: onnx::MatMul_2188\n",
      "[X] Registering layer: llama_model/layers.3/mlp/up_proj/MatMul for ONNX node: llama_model/layers.3/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.3/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.3/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/Mul [Mul] inputs: [llama_model/layers.3/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.3/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/mlp/Mul for ONNX node: llama_model/layers.3/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.3/mlp/Mul_output_0\n",
      "[X] llama_model/layers.3/mlp/Mul [Mul] outputs: [llama_model/layers.3/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.3/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_2189\n",
      "[X] llama_model/layers.3/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.3/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_2189 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2189 for ONNX node: onnx::MatMul_2189\n",
      "[X] Registering layer: llama_model/layers.3/mlp/down_proj/MatMul for ONNX node: llama_model/layers.3/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.3/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.3/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.3/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.3/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.3/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.3/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.3/Add_1 [Add] inputs: [llama_model/layers.3/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.3/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.3/Add_1 for ONNX node: llama_model/layers.3/Add_1\n",
      "[X] Registering tensor: llama_model/layers.3/Add_1_output_0 for ONNX tensor: llama_model/layers.3/Add_1_output_0\n",
      "[X] llama_model/layers.3/Add_1 [Add] outputs: [llama_model/layers.3/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.3/Add_1_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Cast [Cast] inputs: [llama_model/layers.3/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Cast for ONNX node: llama_model/layers.4/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Cast [Cast] outputs: [llama_model/layers.4/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.4/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.4/input_layernorm/Constant [Constant] outputs: [llama_model/layers.4/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Pow [Pow] inputs: [llama_model/layers.4/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.4/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Pow for ONNX node: llama_model/layers.4/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Pow [Pow] outputs: [llama_model/layers.4/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.4/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/ReduceMean for ONNX node: llama_model/layers.4/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.4/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.4/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.4/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.4/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Add [Add] inputs: [llama_model/layers.4/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.4/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.4/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Add for ONNX node: llama_model/layers.4/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Add [Add] outputs: [llama_model/layers.4/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.4/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Sqrt for ONNX node: llama_model/layers.4/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.4/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.4/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.4/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.4/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Div [Div] inputs: [llama_model/layers.4/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.4/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.4/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Div for ONNX node: llama_model/layers.4/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Div [Div] outputs: [llama_model/layers.4/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Mul [Mul] inputs: [llama_model/layers.4/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Mul for ONNX node: llama_model/layers.4/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Mul [Mul] outputs: [llama_model/layers.4/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.4/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Cast_1 for ONNX node: llama_model/layers.4/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.4/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.4.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.4.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.4/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/input_layernorm/Mul_1 for ONNX node: llama_model/layers.4/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.4/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.4/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape [Shape] inputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape for ONNX node: llama_model/layers.4/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape [Shape] outputs: [llama_model/layers.4/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant [Constant] outputs: [llama_model/layers.4/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather [Gather] inputs: [llama_model/layers.4/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.4/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather for ONNX node: llama_model/layers.4/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather [Gather] outputs: [llama_model/layers.4/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape_1 for ONNX node: llama_model/layers.4/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.4/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.4/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.4/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather_1 for ONNX node: llama_model/layers.4/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.4/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2190\n",
      "[X] llama_model/layers.4/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2190 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2190 for ONNX node: onnx::MatMul_2190\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.4/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.4/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2191\n",
      "[X] llama_model/layers.4/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2191 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2191 for ONNX node: onnx::MatMul_2191\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.4/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.4/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2192\n",
      "[X] llama_model/layers.4/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.4/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2192 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2192 for ONNX node: onnx::MatMul_2192\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.4/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.4/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_912 [Constant]\n",
      "[X] Constant_912 [Constant] inputs: \n",
      "[X] Constant_912 [Constant] outputs: [onnx::Unsqueeze_1139 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1139\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1139 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze for ONNX node: llama_model/layers.4/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_914 [Constant]\n",
      "[X] Constant_914 [Constant] inputs: \n",
      "[X] Constant_914 [Constant] outputs: [onnx::Unsqueeze_1141 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1141\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1141 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat for ONNX node: llama_model/layers.4/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat [Concat] outputs: [llama_model/layers.4/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_919 [Constant]\n",
      "[X] Constant_919 [Constant] inputs: \n",
      "[X] Constant_919 [Constant] outputs: [onnx::Unsqueeze_1148 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1148\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1148 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_921 [Constant]\n",
      "[X] Constant_921 [Constant] inputs: \n",
      "[X] Constant_921 [Constant] outputs: [onnx::Unsqueeze_1150 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1150\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1150 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_1 for ONNX node: llama_model/layers.4/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_926 [Constant]\n",
      "[X] Constant_926 [Constant] inputs: \n",
      "[X] Constant_926 [Constant] outputs: [onnx::Unsqueeze_1157 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1157\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1157 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_928 [Constant]\n",
      "[X] Constant_928 [Constant] inputs: \n",
      "[X] Constant_928 [Constant] outputs: [onnx::Unsqueeze_1159 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1159\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1159 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_2 for ONNX node: llama_model/layers.4/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape [Reshape] inputs: [llama_model/layers.4/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Reshape for ONNX node: llama_model/layers.4/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape [Reshape] outputs: [llama_model/layers.4/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose [Transpose] inputs: [llama_model/layers.4/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Transpose for ONNX node: llama_model/layers.4/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose [Transpose] outputs: [llama_model/layers.4/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.4/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Reshape_1 for ONNX node: llama_model/layers.4/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.4/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.4/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Transpose_1 for ONNX node: llama_model/layers.4/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.4/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Reshape_2 for ONNX node: llama_model/layers.4/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.4/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.4/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Transpose_2 for ONNX node: llama_model/layers.4/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.4/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape_2 for ONNX node: llama_model/layers.4/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.4/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.4/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.4/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather_2 for ONNX node: llama_model/layers.4/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.4/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_264\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_264 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.4/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.4/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.4/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.4/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.4/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_274\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_274 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.4/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.4/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.4/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.4/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.4/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.4/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape_3 for ONNX node: llama_model/layers.4/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.4/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.4/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.4/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather_3 for ONNX node: llama_model/layers.4/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.4/self_attn/Gather_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_959 [Constant]\n",
      "[X] Constant_959 [Constant] inputs: \n",
      "[X] Constant_959 [Constant] outputs: [onnx::Unsqueeze_1196 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1196\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1196 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_10_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_10_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_11_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_3 for ONNX node: llama_model/layers.4/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_965 [Constant]\n",
      "[X] Constant_965 [Constant] inputs: \n",
      "[X] Constant_965 [Constant] outputs: [onnx::Unsqueeze_1205 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1205\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1205 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_14_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_14_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_13_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_13_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_14_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_14_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_15_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_15_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_4 for ONNX node: llama_model/layers.4/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_971 [Constant]\n",
      "[X] Constant_971 [Constant] inputs: \n",
      "[X] Constant_971 [Constant] outputs: [onnx::Unsqueeze_1214 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1214\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1214 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_16_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_16_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_17_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_17_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_18_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_18_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_5 for ONNX node: llama_model/layers.4/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_977 [Constant]\n",
      "[X] Constant_977 [Constant] inputs: \n",
      "[X] Constant_977 [Constant] outputs: [onnx::Unsqueeze_1223 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1223\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1223 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_6 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_20_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_6 [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_20_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_19_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_21_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_6 for ONNX node: llama_model/layers.4/self_attn/Concat_6\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_6_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_6 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.4/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape_4 for ONNX node: llama_model/layers.4/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.4/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/ConstantOfShape [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/ConstantOfShape [ConstantOfShape] inputs: [llama_model/layers.4/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/ConstantOfShape_output_0 for ONNX tensor: llama_model/layers.4/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.4/self_attn/ConstantOfShape [ConstantOfShape] outputs: [llama_model/layers.4/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Expand [Expand]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.4/self_attn/Expand [Expand] inputs: [llama_model/layers.4/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Expand for ONNX node: llama_model/layers.4/self_attn/Expand\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Expand_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Expand_output_0\n",
      "[X] llama_model/layers.4/self_attn/Expand [Expand] outputs: [llama_model/layers.4/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Tile [Tile]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Expand_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Tile [Tile] inputs: [llama_model/layers.4/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Tile for ONNX node: llama_model/layers.4/self_attn/Tile\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Tile_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Tile_output_0\n",
      "[X] llama_model/layers.4/self_attn/Tile [Tile] outputs: [llama_model/layers.4/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/GatherElements [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Tile_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.4/self_attn/GatherElements [GatherElements] inputs: [llama_model/layers.4/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/GatherElements for ONNX node: llama_model/layers.4/self_attn/GatherElements\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/GatherElements_output_0 for ONNX tensor: llama_model/layers.4/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.4/self_attn/GatherElements [GatherElements] outputs: [llama_model/layers.4/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape_5 [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_5 [Shape] inputs: [llama_model/layers.4/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape_5 for ONNX node: llama_model/layers.4/self_attn/Shape_5\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_5_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_5 [Shape] outputs: [llama_model/layers.4/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/ConstantOfShape_1 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/ConstantOfShape_1 [ConstantOfShape] inputs: [llama_model/layers.4/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/ConstantOfShape_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/ConstantOfShape_1 [ConstantOfShape] outputs: [llama_model/layers.4/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Expand_1 [Expand]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Expand_1 [Expand] inputs: [llama_model/layers.4/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Expand_1 for ONNX node: llama_model/layers.4/self_attn/Expand_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Expand_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Expand_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Expand_1 [Expand] outputs: [llama_model/layers.4/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Tile_1 [Tile]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Expand_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.4/self_attn/Tile_1 [Tile] inputs: [llama_model/layers.4/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Tile_1 for ONNX node: llama_model/layers.4/self_attn/Tile_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Tile_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Tile_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Tile_1 [Tile] outputs: [llama_model/layers.4/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/GatherElements_1 [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Tile_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.4/self_attn/GatherElements_1 [GatherElements] inputs: [llama_model/layers.4/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/GatherElements_1 for ONNX node: llama_model/layers.4/self_attn/GatherElements_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/GatherElements_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/GatherElements_1 [GatherElements] outputs: [llama_model/layers.4/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul [Mul] inputs: [llama_model/layers.4/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Mul for ONNX node: llama_model/layers.4/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul [Mul] outputs: [llama_model/layers.4/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape_6 [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_6 [Shape] inputs: [llama_model/layers.4/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape_6 for ONNX node: llama_model/layers.4/self_attn/Shape_6\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_6_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_6_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_6 [Shape] outputs: [llama_model/layers.4/self_attn/Shape_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_6_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.4/self_attn/Shape_6_output_0 -> (4)[INT32]], [llama_model/layers.4/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_22_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather_4 for ONNX node: llama_model/layers.4/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.4/self_attn/Gather_4_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_23_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div [Div] inputs: [llama_model/layers.4/self_attn/Gather_4_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_23_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_23_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Div for ONNX node: llama_model/layers.4/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Div_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div [Div] outputs: [llama_model/layers.4/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Div_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast [Cast] inputs: [llama_model/layers.4/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast for ONNX node: llama_model/layers.4/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast [Cast] outputs: [llama_model/layers.4/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.4/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast_1 for ONNX node: llama_model/layers.4/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.4/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_24_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice [Slice] inputs: [llama_model/layers.4/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Constant_24_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Slice for ONNX node: llama_model/layers.4/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice [Slice] outputs: [llama_model/layers.4/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_28_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.4/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_30_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.4/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_30_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Slice_1 for ONNX node: llama_model/layers.4/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.4/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Neg [Neg] inputs: [llama_model/layers.4/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Neg for ONNX node: llama_model/layers.4/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.4/self_attn/Neg [Neg] outputs: [llama_model/layers.4/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_7 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_7 [Concat] inputs: [llama_model/layers.4/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.4/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_7 for ONNX node: llama_model/layers.4/self_attn/Concat_7\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_7_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_7_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_7 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_7_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.4/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Mul_1 for ONNX node: llama_model/layers.4/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.4/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add [Add] inputs: [llama_model/layers.4/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Add for ONNX node: llama_model/layers.4/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Add_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add [Add] outputs: [llama_model/layers.4/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Mul_2 for ONNX node: llama_model/layers.4/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.4/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Shape_7 [Shape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_7 [Shape] inputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Shape_7 for ONNX node: llama_model/layers.4/self_attn/Shape_7\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Shape_7_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Shape_7_output_0\n",
      "[X] llama_model/layers.4/self_attn/Shape_7 [Shape] outputs: [llama_model/layers.4/self_attn/Shape_7_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Shape_7_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.4/self_attn/Shape_7_output_0 -> (4)[INT32]], [llama_model/layers.4/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_32_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Gather_5 for ONNX node: llama_model/layers.4/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.4/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_33 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_33 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_33 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_33_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div_1 [Div] inputs: [llama_model/layers.4/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_33_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_33_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Div_1 for ONNX node: llama_model/layers.4/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div_1 [Div] outputs: [llama_model/layers.4/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.4/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast_2 for ONNX node: llama_model/layers.4/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.4/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.4/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast_3 for ONNX node: llama_model/layers.4/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.4/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_34 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_34 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_34 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_35 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_35 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_35 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_35_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_36 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_36 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_36 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_36_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_37 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_37 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_37 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_34_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_36_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_37_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Constant_34_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_36_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Slice_2 for ONNX node: llama_model/layers.4/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.4/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_38 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_38 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_38 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_38_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.4/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_39 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_39 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.4/self_attn/Constant_39 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_39_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_40 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_40 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_40 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_40_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_41 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_41 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_41 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_39_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_40_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_41_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.4/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_39_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_40_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Slice_3 for ONNX node: llama_model/layers.4/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.4/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.4/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Neg_1 for ONNX node: llama_model/layers.4/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.4/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_8 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_8 [Concat] inputs: [llama_model/layers.4/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.4/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_8 for ONNX node: llama_model/layers.4/self_attn/Concat_8\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_8_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_8_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_8 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_8_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.4/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Mul_3 for ONNX node: llama_model/layers.4/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.4/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add_1 [Add] inputs: [llama_model/layers.4/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Add_1 for ONNX node: llama_model/layers.4/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add_1 [Add] outputs: [llama_model/layers.4/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.4/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Transpose_3 for ONNX node: llama_model/layers.4/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.4/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/MatMul [MatMul] inputs: [llama_model/layers.4/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.4/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/MatMul for ONNX node: llama_model/layers.4/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.4/self_attn/MatMul [MatMul] outputs: [llama_model/layers.4/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_42 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_42 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_42 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_42_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div_2 [Div] inputs: [llama_model/layers.4/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.4/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_42_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_42_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Div_2 for ONNX node: llama_model/layers.4/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Div_2 [Div] outputs: [llama_model/layers.4/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add_2 [Add] inputs: [llama_model/layers.4/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Add_2 for ONNX node: llama_model/layers.4/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Add_2 [Add] outputs: [llama_model/layers.4/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/Softmax [Softmax] inputs: [llama_model/layers.4/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Softmax for ONNX node: llama_model/layers.4/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.4/self_attn/Softmax [Softmax] outputs: [llama_model/layers.4/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.4/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast_4 for ONNX node: llama_model/layers.4/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.4/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.4/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Cast_5 for ONNX node: llama_model/layers.4/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.4/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.4/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.4/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.4/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.4/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/MatMul_1 for ONNX node: llama_model/layers.4/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.4/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.4/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.4/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Transpose_4 for ONNX node: llama_model/layers.4/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.4/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.4/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_1051 [Constant]\n",
      "[X] Constant_1051 [Constant] inputs: \n",
      "[X] Constant_1051 [Constant] outputs: [onnx::Unsqueeze_1308 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_14 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1308\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_14 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1308 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_14 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_14\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_14_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_14_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_14 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1053 [Constant]\n",
      "[X] Constant_1053 [Constant] inputs: \n",
      "[X] Constant_1053 [Constant] outputs: [onnx::Unsqueeze_1310 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Unsqueeze_15 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1310\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_15 [Unsqueeze] inputs: [llama_model/layers.4/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1310 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Unsqueeze_15 for ONNX node: llama_model/layers.4/self_attn/Unsqueeze_15\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Unsqueeze_15_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Unsqueeze_15_output_0\n",
      "[X] llama_model/layers.4/self_attn/Unsqueeze_15 [Unsqueeze] outputs: [llama_model/layers.4/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Constant_43 [Constant]\n",
      "[X] llama_model/layers.4/self_attn/Constant_43 [Constant] inputs: \n",
      "[X] llama_model/layers.4/self_attn/Constant_43 [Constant] outputs: [llama_model/layers.4/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Concat_9 [Concat]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_14_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Unsqueeze_15_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Constant_43_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_9 [Concat] inputs: [llama_model/layers.4/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], [llama_model/layers.4/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Constant_43_output_0 for ONNX node: llama_model/layers.4/self_attn/Constant_43_output_0\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Concat_9 for ONNX node: llama_model/layers.4/self_attn/Concat_9\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Concat_9_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.4/self_attn/Concat_9 [Concat] outputs: [llama_model/layers.4/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.4/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.4/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.4/self_attn/Reshape_3 for ONNX node: llama_model/layers.4/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.4/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.4/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.4/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_2224\n",
      "[X] llama_model/layers.4/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.4/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2224 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2224 for ONNX node: onnx::MatMul_2224\n",
      "[X] Registering layer: llama_model/layers.4/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.4/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.4/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.4/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/Add [Add] inputs: [llama_model/layers.4/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/Add for ONNX node: llama_model/layers.4/Add\n",
      "[X] Registering tensor: llama_model/layers.4/Add_output_0 for ONNX tensor: llama_model/layers.4/Add_output_0\n",
      "[X] llama_model/layers.4/Add [Add] outputs: [llama_model/layers.4/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/Add_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.4/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Cast for ONNX node: llama_model/layers.4/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.4/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.4/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.4/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.4/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Pow for ONNX node: llama_model/layers.4/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.4/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.4/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.4/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.4/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.4/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.4/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.4/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.4/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Add for ONNX node: llama_model/layers.4/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.4/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.4/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.4/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.4/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.4/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.4/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.4/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.4/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.4/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Div for ONNX node: llama_model/layers.4/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.4/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.4/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Mul for ONNX node: llama_model/layers.4/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.4/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.4/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.4/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.4/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.4.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.4.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.4/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.4/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.4/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.4/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.4/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.4/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2225\n",
      "[X] llama_model/layers.4/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.4/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2225 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2225 for ONNX node: onnx::MatMul_2225\n",
      "[X] Registering layer: llama_model/layers.4/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.4/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.4/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.4/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.4/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.4/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.4/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.4/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.4/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.4/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.4/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.4/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/mlp/act_fn/Mul for ONNX node: llama_model/layers.4/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.4/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.4/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.4/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2226\n",
      "[X] llama_model/layers.4/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.4/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2226 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2226 for ONNX node: onnx::MatMul_2226\n",
      "[X] Registering layer: llama_model/layers.4/mlp/up_proj/MatMul for ONNX node: llama_model/layers.4/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.4/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.4/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.4/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/mlp/Mul [Mul] inputs: [llama_model/layers.4/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.4/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/mlp/Mul for ONNX node: llama_model/layers.4/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.4/mlp/Mul_output_0\n",
      "[X] llama_model/layers.4/mlp/Mul [Mul] outputs: [llama_model/layers.4/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.4/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_2227\n",
      "[X] llama_model/layers.4/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.4/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_2227 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2227 for ONNX node: onnx::MatMul_2227\n",
      "[X] Registering layer: llama_model/layers.4/mlp/down_proj/MatMul for ONNX node: llama_model/layers.4/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.4/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.4/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.4/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.4/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.4/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.4/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.4/Add_1 [Add] inputs: [llama_model/layers.4/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.4/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.4/Add_1 for ONNX node: llama_model/layers.4/Add_1\n",
      "[X] Registering tensor: llama_model/layers.4/Add_1_output_0 for ONNX tensor: llama_model/layers.4/Add_1_output_0\n",
      "[X] llama_model/layers.4/Add_1 [Add] outputs: [llama_model/layers.4/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.4/Add_1_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Cast [Cast] inputs: [llama_model/layers.4/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Cast for ONNX node: llama_model/layers.5/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Cast [Cast] outputs: [llama_model/layers.5/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.5/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.5/input_layernorm/Constant [Constant] outputs: [llama_model/layers.5/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Pow [Pow] inputs: [llama_model/layers.5/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.5/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Pow for ONNX node: llama_model/layers.5/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Pow [Pow] outputs: [llama_model/layers.5/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.5/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/ReduceMean for ONNX node: llama_model/layers.5/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.5/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.5/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.5/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.5/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Add [Add] inputs: [llama_model/layers.5/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.5/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.5/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Add for ONNX node: llama_model/layers.5/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Add [Add] outputs: [llama_model/layers.5/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.5/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Sqrt for ONNX node: llama_model/layers.5/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.5/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.5/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.5/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.5/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Div [Div] inputs: [llama_model/layers.5/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.5/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.5/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Div for ONNX node: llama_model/layers.5/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Div [Div] outputs: [llama_model/layers.5/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Mul [Mul] inputs: [llama_model/layers.5/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Mul for ONNX node: llama_model/layers.5/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Mul [Mul] outputs: [llama_model/layers.5/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.5/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Cast_1 for ONNX node: llama_model/layers.5/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.5/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.5.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.5.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.5/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/input_layernorm/Mul_1 for ONNX node: llama_model/layers.5/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.5/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.5/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape [Shape] inputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape for ONNX node: llama_model/layers.5/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape [Shape] outputs: [llama_model/layers.5/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant [Constant] outputs: [llama_model/layers.5/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather [Gather] inputs: [llama_model/layers.5/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.5/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather for ONNX node: llama_model/layers.5/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather [Gather] outputs: [llama_model/layers.5/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape_1 for ONNX node: llama_model/layers.5/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.5/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.5/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.5/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather_1 for ONNX node: llama_model/layers.5/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.5/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2228\n",
      "[X] llama_model/layers.5/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2228 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2228 for ONNX node: onnx::MatMul_2228\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.5/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.5/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2229\n",
      "[X] llama_model/layers.5/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2229 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2229 for ONNX node: onnx::MatMul_2229\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.5/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.5/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2230\n",
      "[X] llama_model/layers.5/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.5/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2230 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2230 for ONNX node: onnx::MatMul_2230\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.5/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.5/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_1100 [Constant]\n",
      "[X] Constant_1100 [Constant] inputs: \n",
      "[X] Constant_1100 [Constant] outputs: [onnx::Unsqueeze_1365 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1365\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1365 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze for ONNX node: llama_model/layers.5/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1102 [Constant]\n",
      "[X] Constant_1102 [Constant] inputs: \n",
      "[X] Constant_1102 [Constant] outputs: [onnx::Unsqueeze_1367 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1367\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1367 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat for ONNX node: llama_model/layers.5/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat [Concat] outputs: [llama_model/layers.5/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1107 [Constant]\n",
      "[X] Constant_1107 [Constant] inputs: \n",
      "[X] Constant_1107 [Constant] outputs: [onnx::Unsqueeze_1374 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1374\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1374 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1109 [Constant]\n",
      "[X] Constant_1109 [Constant] inputs: \n",
      "[X] Constant_1109 [Constant] outputs: [onnx::Unsqueeze_1376 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1376\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1376 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_1 for ONNX node: llama_model/layers.5/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1114 [Constant]\n",
      "[X] Constant_1114 [Constant] inputs: \n",
      "[X] Constant_1114 [Constant] outputs: [onnx::Unsqueeze_1383 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1383\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1383 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1116 [Constant]\n",
      "[X] Constant_1116 [Constant] inputs: \n",
      "[X] Constant_1116 [Constant] outputs: [onnx::Unsqueeze_1385 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1385\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1385 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_2 for ONNX node: llama_model/layers.5/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape [Reshape] inputs: [llama_model/layers.5/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Reshape for ONNX node: llama_model/layers.5/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape [Reshape] outputs: [llama_model/layers.5/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose [Transpose] inputs: [llama_model/layers.5/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Transpose for ONNX node: llama_model/layers.5/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose [Transpose] outputs: [llama_model/layers.5/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.5/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Reshape_1 for ONNX node: llama_model/layers.5/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.5/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.5/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Transpose_1 for ONNX node: llama_model/layers.5/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.5/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Reshape_2 for ONNX node: llama_model/layers.5/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.5/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.5/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Transpose_2 for ONNX node: llama_model/layers.5/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.5/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape_2 for ONNX node: llama_model/layers.5/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.5/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.5/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.5/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather_2 for ONNX node: llama_model/layers.5/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.5/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_264\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_264 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.5/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.5/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.5/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.5/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.5/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_274\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_274 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.5/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.5/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.5/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.5/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.5/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.5/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape_3 for ONNX node: llama_model/layers.5/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.5/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.5/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.5/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather_3 for ONNX node: llama_model/layers.5/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.5/self_attn/Gather_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_1147 [Constant]\n",
      "[X] Constant_1147 [Constant] inputs: \n",
      "[X] Constant_1147 [Constant] outputs: [onnx::Unsqueeze_1422 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1422\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1422 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_10_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_10_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_11_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_3 for ONNX node: llama_model/layers.5/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1153 [Constant]\n",
      "[X] Constant_1153 [Constant] inputs: \n",
      "[X] Constant_1153 [Constant] outputs: [onnx::Unsqueeze_1431 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1431\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1431 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_14_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_14_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_13_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_13_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_14_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_14_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_15_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_15_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_4 for ONNX node: llama_model/layers.5/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1159 [Constant]\n",
      "[X] Constant_1159 [Constant] inputs: \n",
      "[X] Constant_1159 [Constant] outputs: [onnx::Unsqueeze_1440 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1440\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1440 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_16_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_16_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_17_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_17_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_18_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_18_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_5 for ONNX node: llama_model/layers.5/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1165 [Constant]\n",
      "[X] Constant_1165 [Constant] inputs: \n",
      "[X] Constant_1165 [Constant] outputs: [onnx::Unsqueeze_1449 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1449\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1449 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_6 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_20_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_6 [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_20_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_19_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_21_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_6 for ONNX node: llama_model/layers.5/self_attn/Concat_6\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_6_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_6 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.5/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape_4 for ONNX node: llama_model/layers.5/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.5/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/ConstantOfShape [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/ConstantOfShape [ConstantOfShape] inputs: [llama_model/layers.5/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/ConstantOfShape_output_0 for ONNX tensor: llama_model/layers.5/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.5/self_attn/ConstantOfShape [ConstantOfShape] outputs: [llama_model/layers.5/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Expand [Expand]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.5/self_attn/Expand [Expand] inputs: [llama_model/layers.5/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Expand for ONNX node: llama_model/layers.5/self_attn/Expand\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Expand_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Expand_output_0\n",
      "[X] llama_model/layers.5/self_attn/Expand [Expand] outputs: [llama_model/layers.5/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Tile [Tile]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Expand_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Tile [Tile] inputs: [llama_model/layers.5/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Tile for ONNX node: llama_model/layers.5/self_attn/Tile\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Tile_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Tile_output_0\n",
      "[X] llama_model/layers.5/self_attn/Tile [Tile] outputs: [llama_model/layers.5/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/GatherElements [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Tile_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.5/self_attn/GatherElements [GatherElements] inputs: [llama_model/layers.5/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/GatherElements for ONNX node: llama_model/layers.5/self_attn/GatherElements\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/GatherElements_output_0 for ONNX tensor: llama_model/layers.5/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.5/self_attn/GatherElements [GatherElements] outputs: [llama_model/layers.5/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape_5 [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_5 [Shape] inputs: [llama_model/layers.5/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape_5 for ONNX node: llama_model/layers.5/self_attn/Shape_5\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_5_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_5 [Shape] outputs: [llama_model/layers.5/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/ConstantOfShape_1 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/ConstantOfShape_1 [ConstantOfShape] inputs: [llama_model/layers.5/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/ConstantOfShape_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/ConstantOfShape_1 [ConstantOfShape] outputs: [llama_model/layers.5/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Expand_1 [Expand]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Expand_1 [Expand] inputs: [llama_model/layers.5/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Expand_1 for ONNX node: llama_model/layers.5/self_attn/Expand_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Expand_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Expand_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Expand_1 [Expand] outputs: [llama_model/layers.5/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Tile_1 [Tile]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Expand_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.5/self_attn/Tile_1 [Tile] inputs: [llama_model/layers.5/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Tile_1 for ONNX node: llama_model/layers.5/self_attn/Tile_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Tile_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Tile_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Tile_1 [Tile] outputs: [llama_model/layers.5/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/GatherElements_1 [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Tile_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.5/self_attn/GatherElements_1 [GatherElements] inputs: [llama_model/layers.5/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/GatherElements_1 for ONNX node: llama_model/layers.5/self_attn/GatherElements_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/GatherElements_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/GatherElements_1 [GatherElements] outputs: [llama_model/layers.5/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul [Mul] inputs: [llama_model/layers.5/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Mul for ONNX node: llama_model/layers.5/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul [Mul] outputs: [llama_model/layers.5/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape_6 [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_6 [Shape] inputs: [llama_model/layers.5/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape_6 for ONNX node: llama_model/layers.5/self_attn/Shape_6\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_6_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_6_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_6 [Shape] outputs: [llama_model/layers.5/self_attn/Shape_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_6_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.5/self_attn/Shape_6_output_0 -> (4)[INT32]], [llama_model/layers.5/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_22_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather_4 for ONNX node: llama_model/layers.5/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.5/self_attn/Gather_4_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_23_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div [Div] inputs: [llama_model/layers.5/self_attn/Gather_4_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_23_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_23_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Div for ONNX node: llama_model/layers.5/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Div_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div [Div] outputs: [llama_model/layers.5/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Div_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast [Cast] inputs: [llama_model/layers.5/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast for ONNX node: llama_model/layers.5/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast [Cast] outputs: [llama_model/layers.5/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.5/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast_1 for ONNX node: llama_model/layers.5/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.5/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_24_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice [Slice] inputs: [llama_model/layers.5/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Constant_24_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Slice for ONNX node: llama_model/layers.5/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice [Slice] outputs: [llama_model/layers.5/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_28_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.5/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_30_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.5/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_30_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Slice_1 for ONNX node: llama_model/layers.5/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.5/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Neg [Neg] inputs: [llama_model/layers.5/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Neg for ONNX node: llama_model/layers.5/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.5/self_attn/Neg [Neg] outputs: [llama_model/layers.5/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_7 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_7 [Concat] inputs: [llama_model/layers.5/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.5/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_7 for ONNX node: llama_model/layers.5/self_attn/Concat_7\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_7_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_7_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_7 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_7_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.5/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Mul_1 for ONNX node: llama_model/layers.5/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.5/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add [Add] inputs: [llama_model/layers.5/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Add for ONNX node: llama_model/layers.5/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Add_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add [Add] outputs: [llama_model/layers.5/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Mul_2 for ONNX node: llama_model/layers.5/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.5/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Shape_7 [Shape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_7 [Shape] inputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Shape_7 for ONNX node: llama_model/layers.5/self_attn/Shape_7\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Shape_7_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Shape_7_output_0\n",
      "[X] llama_model/layers.5/self_attn/Shape_7 [Shape] outputs: [llama_model/layers.5/self_attn/Shape_7_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Shape_7_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.5/self_attn/Shape_7_output_0 -> (4)[INT32]], [llama_model/layers.5/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_32_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Gather_5 for ONNX node: llama_model/layers.5/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.5/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_33 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_33 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_33 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_33_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div_1 [Div] inputs: [llama_model/layers.5/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_33_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_33_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Div_1 for ONNX node: llama_model/layers.5/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div_1 [Div] outputs: [llama_model/layers.5/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.5/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast_2 for ONNX node: llama_model/layers.5/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.5/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.5/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast_3 for ONNX node: llama_model/layers.5/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.5/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_34 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_34 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_34 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_35 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_35 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_35 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_35_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_36 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_36 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_36 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_36_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_37 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_37 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_37 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_34_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_36_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_37_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Constant_34_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_36_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Slice_2 for ONNX node: llama_model/layers.5/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.5/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_38 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_38 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_38 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_38_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.5/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_39 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_39 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.5/self_attn/Constant_39 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_39_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_40 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_40 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_40 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_40_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_41 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_41 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_41 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_39_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_40_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_41_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.5/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_39_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_40_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Slice_3 for ONNX node: llama_model/layers.5/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.5/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.5/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Neg_1 for ONNX node: llama_model/layers.5/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.5/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_8 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_8 [Concat] inputs: [llama_model/layers.5/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.5/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_8 for ONNX node: llama_model/layers.5/self_attn/Concat_8\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_8_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_8_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_8 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_8_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.5/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Mul_3 for ONNX node: llama_model/layers.5/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.5/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add_1 [Add] inputs: [llama_model/layers.5/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Add_1 for ONNX node: llama_model/layers.5/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add_1 [Add] outputs: [llama_model/layers.5/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.5/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Transpose_3 for ONNX node: llama_model/layers.5/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.5/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/MatMul [MatMul] inputs: [llama_model/layers.5/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.5/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/MatMul for ONNX node: llama_model/layers.5/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.5/self_attn/MatMul [MatMul] outputs: [llama_model/layers.5/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_42 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_42 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_42 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_42_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div_2 [Div] inputs: [llama_model/layers.5/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.5/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_42_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_42_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Div_2 for ONNX node: llama_model/layers.5/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Div_2 [Div] outputs: [llama_model/layers.5/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add_2 [Add] inputs: [llama_model/layers.5/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Add_2 for ONNX node: llama_model/layers.5/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Add_2 [Add] outputs: [llama_model/layers.5/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/Softmax [Softmax] inputs: [llama_model/layers.5/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Softmax for ONNX node: llama_model/layers.5/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.5/self_attn/Softmax [Softmax] outputs: [llama_model/layers.5/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.5/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast_4 for ONNX node: llama_model/layers.5/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.5/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.5/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Cast_5 for ONNX node: llama_model/layers.5/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.5/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.5/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.5/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.5/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.5/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/MatMul_1 for ONNX node: llama_model/layers.5/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.5/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.5/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.5/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Transpose_4 for ONNX node: llama_model/layers.5/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.5/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.5/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_1239 [Constant]\n",
      "[X] Constant_1239 [Constant] inputs: \n",
      "[X] Constant_1239 [Constant] outputs: [onnx::Unsqueeze_1534 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_14 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1534\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_14 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1534 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_14 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_14\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_14_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_14_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_14 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1241 [Constant]\n",
      "[X] Constant_1241 [Constant] inputs: \n",
      "[X] Constant_1241 [Constant] outputs: [onnx::Unsqueeze_1536 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Unsqueeze_15 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1536\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_15 [Unsqueeze] inputs: [llama_model/layers.5/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1536 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Unsqueeze_15 for ONNX node: llama_model/layers.5/self_attn/Unsqueeze_15\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Unsqueeze_15_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Unsqueeze_15_output_0\n",
      "[X] llama_model/layers.5/self_attn/Unsqueeze_15 [Unsqueeze] outputs: [llama_model/layers.5/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Constant_43 [Constant]\n",
      "[X] llama_model/layers.5/self_attn/Constant_43 [Constant] inputs: \n",
      "[X] llama_model/layers.5/self_attn/Constant_43 [Constant] outputs: [llama_model/layers.5/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Concat_9 [Concat]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_14_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Unsqueeze_15_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Constant_43_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_9 [Concat] inputs: [llama_model/layers.5/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], [llama_model/layers.5/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Constant_43_output_0 for ONNX node: llama_model/layers.5/self_attn/Constant_43_output_0\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Concat_9 for ONNX node: llama_model/layers.5/self_attn/Concat_9\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Concat_9_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.5/self_attn/Concat_9 [Concat] outputs: [llama_model/layers.5/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.5/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.5/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.5/self_attn/Reshape_3 for ONNX node: llama_model/layers.5/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.5/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.5/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.5/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_2262\n",
      "[X] llama_model/layers.5/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.5/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2262 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2262 for ONNX node: onnx::MatMul_2262\n",
      "[X] Registering layer: llama_model/layers.5/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.5/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.5/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.5/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/Add [Add] inputs: [llama_model/layers.5/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/Add for ONNX node: llama_model/layers.5/Add\n",
      "[X] Registering tensor: llama_model/layers.5/Add_output_0 for ONNX tensor: llama_model/layers.5/Add_output_0\n",
      "[X] llama_model/layers.5/Add [Add] outputs: [llama_model/layers.5/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/Add_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.5/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Cast for ONNX node: llama_model/layers.5/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.5/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.5/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.5/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.5/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Pow for ONNX node: llama_model/layers.5/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.5/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.5/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.5/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.5/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.5/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.5/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.5/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.5/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Add for ONNX node: llama_model/layers.5/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.5/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.5/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.5/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.5/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.5/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.5/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.5/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.5/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.5/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Div for ONNX node: llama_model/layers.5/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.5/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.5/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Mul for ONNX node: llama_model/layers.5/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.5/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.5/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.5/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.5/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.5.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.5.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.5/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.5/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.5/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.5/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.5/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.5/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2263\n",
      "[X] llama_model/layers.5/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.5/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2263 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2263 for ONNX node: onnx::MatMul_2263\n",
      "[X] Registering layer: llama_model/layers.5/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.5/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.5/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.5/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.5/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.5/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.5/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.5/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.5/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.5/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.5/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.5/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/mlp/act_fn/Mul for ONNX node: llama_model/layers.5/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.5/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.5/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.5/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2264\n",
      "[X] llama_model/layers.5/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.5/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2264 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2264 for ONNX node: onnx::MatMul_2264\n",
      "[X] Registering layer: llama_model/layers.5/mlp/up_proj/MatMul for ONNX node: llama_model/layers.5/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.5/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.5/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.5/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/mlp/Mul [Mul] inputs: [llama_model/layers.5/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.5/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/mlp/Mul for ONNX node: llama_model/layers.5/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.5/mlp/Mul_output_0\n",
      "[X] llama_model/layers.5/mlp/Mul [Mul] outputs: [llama_model/layers.5/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.5/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_2265\n",
      "[X] llama_model/layers.5/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.5/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_2265 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2265 for ONNX node: onnx::MatMul_2265\n",
      "[X] Registering layer: llama_model/layers.5/mlp/down_proj/MatMul for ONNX node: llama_model/layers.5/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.5/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.5/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.5/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.5/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.5/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.5/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.5/Add_1 [Add] inputs: [llama_model/layers.5/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.5/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.5/Add_1 for ONNX node: llama_model/layers.5/Add_1\n",
      "[X] Registering tensor: llama_model/layers.5/Add_1_output_0 for ONNX tensor: llama_model/layers.5/Add_1_output_0\n",
      "[X] llama_model/layers.5/Add_1 [Add] outputs: [llama_model/layers.5/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.5/Add_1_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Cast [Cast] inputs: [llama_model/layers.5/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Cast for ONNX node: llama_model/layers.6/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Cast [Cast] outputs: [llama_model/layers.6/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.6/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.6/input_layernorm/Constant [Constant] outputs: [llama_model/layers.6/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Pow [Pow] inputs: [llama_model/layers.6/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.6/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Pow for ONNX node: llama_model/layers.6/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Pow [Pow] outputs: [llama_model/layers.6/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.6/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/ReduceMean for ONNX node: llama_model/layers.6/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.6/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.6/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.6/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.6/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Add [Add] inputs: [llama_model/layers.6/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.6/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.6/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Add for ONNX node: llama_model/layers.6/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Add [Add] outputs: [llama_model/layers.6/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.6/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Sqrt for ONNX node: llama_model/layers.6/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.6/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.6/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.6/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.6/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Div [Div] inputs: [llama_model/layers.6/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.6/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.6/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Div for ONNX node: llama_model/layers.6/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Div [Div] outputs: [llama_model/layers.6/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Mul [Mul] inputs: [llama_model/layers.6/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Mul for ONNX node: llama_model/layers.6/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Mul [Mul] outputs: [llama_model/layers.6/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.6/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Cast_1 for ONNX node: llama_model/layers.6/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.6/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.6.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.6.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.6/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/input_layernorm/Mul_1 for ONNX node: llama_model/layers.6/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.6/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.6/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape [Shape] inputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape for ONNX node: llama_model/layers.6/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape [Shape] outputs: [llama_model/layers.6/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant [Constant] outputs: [llama_model/layers.6/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather [Gather] inputs: [llama_model/layers.6/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.6/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather for ONNX node: llama_model/layers.6/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather [Gather] outputs: [llama_model/layers.6/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape_1 for ONNX node: llama_model/layers.6/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.6/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.6/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.6/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather_1 for ONNX node: llama_model/layers.6/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.6/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2266\n",
      "[X] llama_model/layers.6/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2266 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2266 for ONNX node: onnx::MatMul_2266\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.6/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.6/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2267\n",
      "[X] llama_model/layers.6/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2267 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2267 for ONNX node: onnx::MatMul_2267\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.6/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.6/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2268\n",
      "[X] llama_model/layers.6/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.6/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2268 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2268 for ONNX node: onnx::MatMul_2268\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.6/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.6/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_1288 [Constant]\n",
      "[X] Constant_1288 [Constant] inputs: \n",
      "[X] Constant_1288 [Constant] outputs: [onnx::Unsqueeze_1591 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1591\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1591 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze for ONNX node: llama_model/layers.6/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1290 [Constant]\n",
      "[X] Constant_1290 [Constant] inputs: \n",
      "[X] Constant_1290 [Constant] outputs: [onnx::Unsqueeze_1593 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1593\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1593 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat for ONNX node: llama_model/layers.6/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat [Concat] outputs: [llama_model/layers.6/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1295 [Constant]\n",
      "[X] Constant_1295 [Constant] inputs: \n",
      "[X] Constant_1295 [Constant] outputs: [onnx::Unsqueeze_1600 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1600\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1600 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1297 [Constant]\n",
      "[X] Constant_1297 [Constant] inputs: \n",
      "[X] Constant_1297 [Constant] outputs: [onnx::Unsqueeze_1602 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1602\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1602 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_1 for ONNX node: llama_model/layers.6/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1302 [Constant]\n",
      "[X] Constant_1302 [Constant] inputs: \n",
      "[X] Constant_1302 [Constant] outputs: [onnx::Unsqueeze_1609 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1609\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1609 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1304 [Constant]\n",
      "[X] Constant_1304 [Constant] inputs: \n",
      "[X] Constant_1304 [Constant] outputs: [onnx::Unsqueeze_1611 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1611\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1611 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_2 for ONNX node: llama_model/layers.6/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape [Reshape] inputs: [llama_model/layers.6/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Reshape for ONNX node: llama_model/layers.6/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape [Reshape] outputs: [llama_model/layers.6/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose [Transpose] inputs: [llama_model/layers.6/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Transpose for ONNX node: llama_model/layers.6/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose [Transpose] outputs: [llama_model/layers.6/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.6/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Reshape_1 for ONNX node: llama_model/layers.6/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.6/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.6/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Transpose_1 for ONNX node: llama_model/layers.6/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.6/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Reshape_2 for ONNX node: llama_model/layers.6/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.6/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.6/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Transpose_2 for ONNX node: llama_model/layers.6/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.6/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape_2 for ONNX node: llama_model/layers.6/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.6/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.6/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.6/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather_2 for ONNX node: llama_model/layers.6/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.6/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_264\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_264 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.6/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.6/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.6/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.6/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.6/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_274\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_274 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.6/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.6/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.6/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.6/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.6/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.6/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape_3 for ONNX node: llama_model/layers.6/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.6/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.6/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.6/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather_3 for ONNX node: llama_model/layers.6/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.6/self_attn/Gather_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_1335 [Constant]\n",
      "[X] Constant_1335 [Constant] inputs: \n",
      "[X] Constant_1335 [Constant] outputs: [onnx::Unsqueeze_1648 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1648\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1648 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_10_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_10_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_11_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_3 for ONNX node: llama_model/layers.6/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1341 [Constant]\n",
      "[X] Constant_1341 [Constant] inputs: \n",
      "[X] Constant_1341 [Constant] outputs: [onnx::Unsqueeze_1657 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1657\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1657 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_14_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_14_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_13_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_13_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_14_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_14_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_15_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_15_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_4 for ONNX node: llama_model/layers.6/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1347 [Constant]\n",
      "[X] Constant_1347 [Constant] inputs: \n",
      "[X] Constant_1347 [Constant] outputs: [onnx::Unsqueeze_1666 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1666\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1666 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_16_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_16_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_17_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_17_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_18_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_18_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_5 for ONNX node: llama_model/layers.6/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1353 [Constant]\n",
      "[X] Constant_1353 [Constant] inputs: \n",
      "[X] Constant_1353 [Constant] outputs: [onnx::Unsqueeze_1675 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1675\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1675 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_6 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_20_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_6 [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_20_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_19_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_21_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_6 for ONNX node: llama_model/layers.6/self_attn/Concat_6\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_6_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_6 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.6/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape_4 for ONNX node: llama_model/layers.6/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.6/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/ConstantOfShape [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/ConstantOfShape [ConstantOfShape] inputs: [llama_model/layers.6/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/ConstantOfShape_output_0 for ONNX tensor: llama_model/layers.6/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.6/self_attn/ConstantOfShape [ConstantOfShape] outputs: [llama_model/layers.6/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Expand [Expand]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.6/self_attn/Expand [Expand] inputs: [llama_model/layers.6/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Expand for ONNX node: llama_model/layers.6/self_attn/Expand\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Expand_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Expand_output_0\n",
      "[X] llama_model/layers.6/self_attn/Expand [Expand] outputs: [llama_model/layers.6/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Tile [Tile]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Expand_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Tile [Tile] inputs: [llama_model/layers.6/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Tile for ONNX node: llama_model/layers.6/self_attn/Tile\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Tile_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Tile_output_0\n",
      "[X] llama_model/layers.6/self_attn/Tile [Tile] outputs: [llama_model/layers.6/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/GatherElements [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Tile_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.6/self_attn/GatherElements [GatherElements] inputs: [llama_model/layers.6/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/GatherElements for ONNX node: llama_model/layers.6/self_attn/GatherElements\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/GatherElements_output_0 for ONNX tensor: llama_model/layers.6/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.6/self_attn/GatherElements [GatherElements] outputs: [llama_model/layers.6/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape_5 [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_5 [Shape] inputs: [llama_model/layers.6/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape_5 for ONNX node: llama_model/layers.6/self_attn/Shape_5\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_5_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_5 [Shape] outputs: [llama_model/layers.6/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/ConstantOfShape_1 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/ConstantOfShape_1 [ConstantOfShape] inputs: [llama_model/layers.6/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/ConstantOfShape_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/ConstantOfShape_1 [ConstantOfShape] outputs: [llama_model/layers.6/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Expand_1 [Expand]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Expand_1 [Expand] inputs: [llama_model/layers.6/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Expand_1 for ONNX node: llama_model/layers.6/self_attn/Expand_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Expand_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Expand_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Expand_1 [Expand] outputs: [llama_model/layers.6/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Tile_1 [Tile]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Expand_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.6/self_attn/Tile_1 [Tile] inputs: [llama_model/layers.6/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Tile_1 for ONNX node: llama_model/layers.6/self_attn/Tile_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Tile_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Tile_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Tile_1 [Tile] outputs: [llama_model/layers.6/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/GatherElements_1 [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Tile_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.6/self_attn/GatherElements_1 [GatherElements] inputs: [llama_model/layers.6/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/GatherElements_1 for ONNX node: llama_model/layers.6/self_attn/GatherElements_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/GatherElements_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/GatherElements_1 [GatherElements] outputs: [llama_model/layers.6/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul [Mul] inputs: [llama_model/layers.6/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Mul for ONNX node: llama_model/layers.6/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul [Mul] outputs: [llama_model/layers.6/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape_6 [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_6 [Shape] inputs: [llama_model/layers.6/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape_6 for ONNX node: llama_model/layers.6/self_attn/Shape_6\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_6_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_6_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_6 [Shape] outputs: [llama_model/layers.6/self_attn/Shape_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_6_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.6/self_attn/Shape_6_output_0 -> (4)[INT32]], [llama_model/layers.6/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_22_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather_4 for ONNX node: llama_model/layers.6/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.6/self_attn/Gather_4_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_23_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div [Div] inputs: [llama_model/layers.6/self_attn/Gather_4_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_23_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_23_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Div for ONNX node: llama_model/layers.6/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Div_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div [Div] outputs: [llama_model/layers.6/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Div_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast [Cast] inputs: [llama_model/layers.6/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast for ONNX node: llama_model/layers.6/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast [Cast] outputs: [llama_model/layers.6/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.6/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast_1 for ONNX node: llama_model/layers.6/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.6/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_24_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice [Slice] inputs: [llama_model/layers.6/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Constant_24_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Slice for ONNX node: llama_model/layers.6/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice [Slice] outputs: [llama_model/layers.6/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_28_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.6/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_30_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.6/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_30_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Slice_1 for ONNX node: llama_model/layers.6/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.6/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Neg [Neg] inputs: [llama_model/layers.6/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Neg for ONNX node: llama_model/layers.6/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.6/self_attn/Neg [Neg] outputs: [llama_model/layers.6/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_7 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_7 [Concat] inputs: [llama_model/layers.6/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.6/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_7 for ONNX node: llama_model/layers.6/self_attn/Concat_7\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_7_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_7_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_7 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_7_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.6/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Mul_1 for ONNX node: llama_model/layers.6/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.6/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add [Add] inputs: [llama_model/layers.6/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Add for ONNX node: llama_model/layers.6/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Add_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add [Add] outputs: [llama_model/layers.6/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Mul_2 for ONNX node: llama_model/layers.6/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.6/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Shape_7 [Shape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_7 [Shape] inputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Shape_7 for ONNX node: llama_model/layers.6/self_attn/Shape_7\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Shape_7_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Shape_7_output_0\n",
      "[X] llama_model/layers.6/self_attn/Shape_7 [Shape] outputs: [llama_model/layers.6/self_attn/Shape_7_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Shape_7_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.6/self_attn/Shape_7_output_0 -> (4)[INT32]], [llama_model/layers.6/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_32_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Gather_5 for ONNX node: llama_model/layers.6/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.6/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_33 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_33 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_33 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_33_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div_1 [Div] inputs: [llama_model/layers.6/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_33_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_33_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Div_1 for ONNX node: llama_model/layers.6/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div_1 [Div] outputs: [llama_model/layers.6/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.6/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast_2 for ONNX node: llama_model/layers.6/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.6/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.6/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast_3 for ONNX node: llama_model/layers.6/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.6/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_34 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_34 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_34 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_35 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_35 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_35 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_35_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_36 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_36 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_36 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_36_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_37 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_37 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_37 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_34_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_36_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_37_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Constant_34_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_36_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Slice_2 for ONNX node: llama_model/layers.6/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.6/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_38 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_38 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_38 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_38_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.6/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_39 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_39 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.6/self_attn/Constant_39 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_39_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_40 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_40 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_40 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_40_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_41 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_41 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_41 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_39_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_40_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_41_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.6/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_39_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_40_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Slice_3 for ONNX node: llama_model/layers.6/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.6/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.6/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Neg_1 for ONNX node: llama_model/layers.6/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.6/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_8 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_8 [Concat] inputs: [llama_model/layers.6/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.6/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_8 for ONNX node: llama_model/layers.6/self_attn/Concat_8\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_8_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_8_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_8 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_8_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.6/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Mul_3 for ONNX node: llama_model/layers.6/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.6/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add_1 [Add] inputs: [llama_model/layers.6/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Add_1 for ONNX node: llama_model/layers.6/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add_1 [Add] outputs: [llama_model/layers.6/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.6/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Transpose_3 for ONNX node: llama_model/layers.6/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.6/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/MatMul [MatMul] inputs: [llama_model/layers.6/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.6/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/MatMul for ONNX node: llama_model/layers.6/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.6/self_attn/MatMul [MatMul] outputs: [llama_model/layers.6/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_42 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_42 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_42 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_42_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div_2 [Div] inputs: [llama_model/layers.6/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.6/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_42_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_42_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Div_2 for ONNX node: llama_model/layers.6/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Div_2 [Div] outputs: [llama_model/layers.6/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add_2 [Add] inputs: [llama_model/layers.6/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Add_2 for ONNX node: llama_model/layers.6/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Add_2 [Add] outputs: [llama_model/layers.6/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/Softmax [Softmax] inputs: [llama_model/layers.6/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Softmax for ONNX node: llama_model/layers.6/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.6/self_attn/Softmax [Softmax] outputs: [llama_model/layers.6/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.6/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast_4 for ONNX node: llama_model/layers.6/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.6/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.6/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Cast_5 for ONNX node: llama_model/layers.6/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.6/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.6/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.6/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.6/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.6/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/MatMul_1 for ONNX node: llama_model/layers.6/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.6/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.6/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.6/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Transpose_4 for ONNX node: llama_model/layers.6/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.6/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.6/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_1427 [Constant]\n",
      "[X] Constant_1427 [Constant] inputs: \n",
      "[X] Constant_1427 [Constant] outputs: [onnx::Unsqueeze_1760 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_14 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1760\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_14 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1760 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_14 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_14\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_14_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_14_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_14 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1429 [Constant]\n",
      "[X] Constant_1429 [Constant] inputs: \n",
      "[X] Constant_1429 [Constant] outputs: [onnx::Unsqueeze_1762 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Unsqueeze_15 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1762\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_15 [Unsqueeze] inputs: [llama_model/layers.6/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1762 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Unsqueeze_15 for ONNX node: llama_model/layers.6/self_attn/Unsqueeze_15\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Unsqueeze_15_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Unsqueeze_15_output_0\n",
      "[X] llama_model/layers.6/self_attn/Unsqueeze_15 [Unsqueeze] outputs: [llama_model/layers.6/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Constant_43 [Constant]\n",
      "[X] llama_model/layers.6/self_attn/Constant_43 [Constant] inputs: \n",
      "[X] llama_model/layers.6/self_attn/Constant_43 [Constant] outputs: [llama_model/layers.6/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Concat_9 [Concat]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_14_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Unsqueeze_15_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Constant_43_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_9 [Concat] inputs: [llama_model/layers.6/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], [llama_model/layers.6/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Constant_43_output_0 for ONNX node: llama_model/layers.6/self_attn/Constant_43_output_0\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Concat_9 for ONNX node: llama_model/layers.6/self_attn/Concat_9\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Concat_9_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.6/self_attn/Concat_9 [Concat] outputs: [llama_model/layers.6/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.6/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.6/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.6/self_attn/Reshape_3 for ONNX node: llama_model/layers.6/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.6/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.6/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.6/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_2300\n",
      "[X] llama_model/layers.6/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.6/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2300 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2300 for ONNX node: onnx::MatMul_2300\n",
      "[X] Registering layer: llama_model/layers.6/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.6/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.6/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.6/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/Add [Add] inputs: [llama_model/layers.6/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/Add for ONNX node: llama_model/layers.6/Add\n",
      "[X] Registering tensor: llama_model/layers.6/Add_output_0 for ONNX tensor: llama_model/layers.6/Add_output_0\n",
      "[X] llama_model/layers.6/Add [Add] outputs: [llama_model/layers.6/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/Add_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.6/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Cast for ONNX node: llama_model/layers.6/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.6/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.6/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.6/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.6/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Pow for ONNX node: llama_model/layers.6/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.6/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.6/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.6/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.6/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.6/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.6/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.6/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.6/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Add for ONNX node: llama_model/layers.6/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.6/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.6/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.6/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.6/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.6/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.6/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.6/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.6/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.6/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Div for ONNX node: llama_model/layers.6/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.6/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.6/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Mul for ONNX node: llama_model/layers.6/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.6/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.6/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.6/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.6/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.6.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.6.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.6/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.6/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.6/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.6/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.6/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.6/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2301\n",
      "[X] llama_model/layers.6/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.6/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2301 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2301 for ONNX node: onnx::MatMul_2301\n",
      "[X] Registering layer: llama_model/layers.6/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.6/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.6/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.6/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.6/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.6/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.6/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.6/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.6/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.6/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.6/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.6/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/mlp/act_fn/Mul for ONNX node: llama_model/layers.6/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.6/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.6/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.6/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2302\n",
      "[X] llama_model/layers.6/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.6/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2302 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2302 for ONNX node: onnx::MatMul_2302\n",
      "[X] Registering layer: llama_model/layers.6/mlp/up_proj/MatMul for ONNX node: llama_model/layers.6/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.6/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.6/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.6/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/mlp/Mul [Mul] inputs: [llama_model/layers.6/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.6/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/mlp/Mul for ONNX node: llama_model/layers.6/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.6/mlp/Mul_output_0\n",
      "[X] llama_model/layers.6/mlp/Mul [Mul] outputs: [llama_model/layers.6/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.6/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_2303\n",
      "[X] llama_model/layers.6/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.6/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_2303 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2303 for ONNX node: onnx::MatMul_2303\n",
      "[X] Registering layer: llama_model/layers.6/mlp/down_proj/MatMul for ONNX node: llama_model/layers.6/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.6/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.6/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.6/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.6/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.6/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.6/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.6/Add_1 [Add] inputs: [llama_model/layers.6/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.6/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.6/Add_1 for ONNX node: llama_model/layers.6/Add_1\n",
      "[X] Registering tensor: llama_model/layers.6/Add_1_output_0 for ONNX tensor: llama_model/layers.6/Add_1_output_0\n",
      "[X] llama_model/layers.6/Add_1 [Add] outputs: [llama_model/layers.6/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.6/Add_1_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Cast [Cast] inputs: [llama_model/layers.6/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Cast for ONNX node: llama_model/layers.7/input_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Cast [Cast] outputs: [llama_model/layers.7/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.7/input_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.7/input_layernorm/Constant [Constant] outputs: [llama_model/layers.7/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Pow [Pow] inputs: [llama_model/layers.7/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/input_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Constant_output_0 for ONNX node: llama_model/layers.7/input_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Pow for ONNX node: llama_model/layers.7/input_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Pow [Pow] outputs: [llama_model/layers.7/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.7/input_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/ReduceMean for ONNX node: llama_model/layers.7/input_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.7/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.7/input_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.7/input_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.7/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Add [Add] inputs: [llama_model/layers.7/input_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.7/input_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.7/input_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Add for ONNX node: llama_model/layers.7/input_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Add [Add] outputs: [llama_model/layers.7/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Add_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.7/input_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Sqrt for ONNX node: llama_model/layers.7/input_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.7/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.7/input_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.7/input_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.7/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Div [Div] inputs: [llama_model/layers.7/input_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.7/input_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.7/input_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Div for ONNX node: llama_model/layers.7/input_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Div [Div] outputs: [llama_model/layers.7/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Div_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Mul [Mul] inputs: [llama_model/layers.7/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/input_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Mul for ONNX node: llama_model/layers.7/input_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Mul [Mul] outputs: [llama_model/layers.7/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.7/input_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Cast_1 for ONNX node: llama_model/layers.7/input_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.7/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/input_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.7.input_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.7.input_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.7/input_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/input_layernorm/Mul_1 for ONNX node: llama_model/layers.7/input_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.7/input_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.7/input_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape [Shape] inputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape for ONNX node: llama_model/layers.7/self_attn/Shape\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape [Shape] outputs: [llama_model/layers.7/self_attn/Shape_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant [Constant] outputs: [llama_model/layers.7/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather [Gather] inputs: [llama_model/layers.7/self_attn/Shape_output_0 -> (3)[INT32]], [llama_model/layers.7/self_attn/Constant_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather for ONNX node: llama_model/layers.7/self_attn/Gather\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather [Gather] outputs: [llama_model/layers.7/self_attn/Gather_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape_1 [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_1 [Shape] inputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape_1 for ONNX node: llama_model/layers.7/self_attn/Shape_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_1 [Shape] outputs: [llama_model/layers.7/self_attn/Shape_1_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_1 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_1 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather_1 [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_1 [Gather] inputs: [llama_model/layers.7/self_attn/Shape_1_output_0 -> (3)[INT32]], [llama_model/layers.7/self_attn/Constant_1_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_1_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_1_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather_1 for ONNX node: llama_model/layers.7/self_attn/Gather_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_1 [Gather] outputs: [llama_model/layers.7/self_attn/Gather_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/q_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2304\n",
      "[X] llama_model/layers.7/self_attn/q_proj/MatMul [MatMul] inputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2304 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2304 for ONNX node: onnx::MatMul_2304\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/q_proj/MatMul for ONNX node: llama_model/layers.7/self_attn/q_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/q_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/q_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/self_attn/q_proj/MatMul [MatMul] outputs: [llama_model/layers.7/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/k_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2305\n",
      "[X] llama_model/layers.7/self_attn/k_proj/MatMul [MatMul] inputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2305 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2305 for ONNX node: onnx::MatMul_2305\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/k_proj/MatMul for ONNX node: llama_model/layers.7/self_attn/k_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/k_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/k_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/self_attn/k_proj/MatMul [MatMul] outputs: [llama_model/layers.7/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/v_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2306\n",
      "[X] llama_model/layers.7/self_attn/v_proj/MatMul [MatMul] inputs: [llama_model/layers.7/input_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2306 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2306 for ONNX node: onnx::MatMul_2306\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/v_proj/MatMul for ONNX node: llama_model/layers.7/self_attn/v_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/v_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/v_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/self_attn/v_proj/MatMul [MatMul] outputs: [llama_model/layers.7/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: Constant_1476 [Constant]\n",
      "[X] Constant_1476 [Constant] inputs: \n",
      "[X] Constant_1476 [Constant] outputs: [onnx::Unsqueeze_1817 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1817\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1817 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze for ONNX node: llama_model/layers.7/self_attn/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1478 [Constant]\n",
      "[X] Constant_1478 [Constant] inputs: \n",
      "[X] Constant_1478 [Constant] outputs: [onnx::Unsqueeze_1819 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1819\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1819 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_1 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_2 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_2 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_3 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_3 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_2_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_3_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_3_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat for ONNX node: llama_model/layers.7/self_attn/Concat\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat [Concat] outputs: [llama_model/layers.7/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1483 [Constant]\n",
      "[X] Constant_1483 [Constant] inputs: \n",
      "[X] Constant_1483 [Constant] outputs: [onnx::Unsqueeze_1826 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_2 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1826\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_2 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1826 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_2 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_2 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1485 [Constant]\n",
      "[X] Constant_1485 [Constant] inputs: \n",
      "[X] Constant_1485 [Constant] outputs: [onnx::Unsqueeze_1828 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_3 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1828\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_3 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1828 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_3 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_3 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_4 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_4 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_5 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_5 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_1 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_3_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_1 [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_2_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_3_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_4_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_4_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_5_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_5_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_1 for ONNX node: llama_model/layers.7/self_attn/Concat_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_1 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1490 [Constant]\n",
      "[X] Constant_1490 [Constant] inputs: \n",
      "[X] Constant_1490 [Constant] outputs: [onnx::Unsqueeze_1835 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_4 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1835\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_4 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1835 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_4 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_4 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1492 [Constant]\n",
      "[X] Constant_1492 [Constant] inputs: \n",
      "[X] Constant_1492 [Constant] outputs: [onnx::Unsqueeze_1837 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_5 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1837\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_5 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1837 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_5 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_5\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_5_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_5 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_6 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_6 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_7 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_7 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_2 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_5_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_7_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_2 [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_4_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_5_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_6_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_6_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_7_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_7_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_2 for ONNX node: llama_model/layers.7/self_attn/Concat_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_2 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Reshape [Reshape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/q_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape [Reshape] inputs: [llama_model/layers.7/self_attn/q_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/self_attn/Concat_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Reshape for ONNX node: llama_model/layers.7/self_attn/Reshape\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Reshape_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape [Reshape] outputs: [llama_model/layers.7/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Transpose [Transpose]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Reshape_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose [Transpose] inputs: [llama_model/layers.7/self_attn/Reshape_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Transpose for ONNX node: llama_model/layers.7/self_attn/Transpose\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Transpose_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose [Transpose] outputs: [llama_model/layers.7/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Reshape_1 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/k_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_1 [Reshape] inputs: [llama_model/layers.7/self_attn/k_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/self_attn/Concat_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Reshape_1 for ONNX node: llama_model/layers.7/self_attn/Reshape_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Reshape_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_1 [Reshape] outputs: [llama_model/layers.7/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Transpose_1 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Reshape_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_1 [Transpose] inputs: [llama_model/layers.7/self_attn/Reshape_1_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Transpose_1 for ONNX node: llama_model/layers.7/self_attn/Transpose_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Transpose_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_1 [Transpose] outputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Reshape_2 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/v_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_2 [Reshape] inputs: [llama_model/layers.7/self_attn/v_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/self_attn/Concat_2_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Reshape_2 for ONNX node: llama_model/layers.7/self_attn/Reshape_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Reshape_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_2 [Reshape] outputs: [llama_model/layers.7/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Transpose_2 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Reshape_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_2 [Transpose] inputs: [llama_model/layers.7/self_attn/Reshape_2_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Transpose_2 for ONNX node: llama_model/layers.7/self_attn/Transpose_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Transpose_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_2 [Transpose] outputs: [llama_model/layers.7/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape_2 [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_2 [Shape] inputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape_2 for ONNX node: llama_model/layers.7/self_attn/Shape_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_2 [Shape] outputs: [llama_model/layers.7/self_attn/Shape_2_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_8 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_8 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_8 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather_2 [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_8_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_2 [Gather] inputs: [llama_model/layers.7/self_attn/Shape_2_output_0 -> (4)[INT32]], [llama_model/layers.7/self_attn/Constant_8_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_8_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_8_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather_2 for ONNX node: llama_model/layers.7/self_attn/Gather_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_2 [Gather] outputs: [llama_model/layers.7/self_attn/Gather_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_1 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_1 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Unsqueeze [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_1_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Unsqueeze [Unsqueeze] outputs: [llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_2 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_2 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_3 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_3 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_3 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Slice [Slice]\n",
      "[X] Searching for input: onnx::Slice_264\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Slice [Slice] inputs: [onnx::Slice_264 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.7/self_attn/rotary_emb/Constant_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_2_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_3_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Slice for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Slice\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Slice_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Slice [Slice] outputs: [llama_model/layers.7/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Slice_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Cast [Cast] inputs: [llama_model/layers.7/self_attn/rotary_emb/Slice_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Cast for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Cast\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Cast_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Cast_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Cast [Cast] outputs: [llama_model/layers.7/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_4 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_4 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_4 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_5 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_5 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_5 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_2_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_5_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1 for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_6 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_6 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_6 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Constant_7 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_7 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Constant_7 [Constant] outputs: [llama_model/layers.7/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Slice_1 [Slice]\n",
      "[X] Searching for input: onnx::Slice_274\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_6_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Constant_7_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Slice_1 [Slice] inputs: [onnx::Slice_274 -> (1, 1, 128, 8)[FLOAT]], [llama_model/layers.7/self_attn/rotary_emb/Constant_4_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Unsqueeze_1_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_6_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/rotary_emb/Constant_7_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Slice_1 for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Slice_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Slice_1 [Slice] outputs: [llama_model/layers.7/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/rotary_emb/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Slice_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Cast_1 [Cast] inputs: [llama_model/layers.7/self_attn/rotary_emb/Slice_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/rotary_emb/Cast_1 for ONNX node: llama_model/layers.7/self_attn/rotary_emb/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/rotary_emb/Cast_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/rotary_emb/Cast_1 [Cast] outputs: [llama_model/layers.7/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape_3 [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_3 [Shape] inputs: [llama_model/layers.7/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape_3 for ONNX node: llama_model/layers.7/self_attn/Shape_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_3 [Shape] outputs: [llama_model/layers.7/self_attn/Shape_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_9 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_9 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_9 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather_3 [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_3_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_9_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_3 [Gather] inputs: [llama_model/layers.7/self_attn/Shape_3_output_0 -> (4)[INT32]], [llama_model/layers.7/self_attn/Constant_9_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_9_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_9_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather_3 for ONNX node: llama_model/layers.7/self_attn/Gather_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_3 [Gather] outputs: [llama_model/layers.7/self_attn/Gather_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: Constant_1523 [Constant]\n",
      "[X] Constant_1523 [Constant] inputs: \n",
      "[X] Constant_1523 [Constant] outputs: [onnx::Unsqueeze_1874 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_6 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1874\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_6 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1874 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_6 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_6\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_6_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_6_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_6 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_10 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_10 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_10 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_11 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_11 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_11 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_12 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_12 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_12 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_3 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_6_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_10_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_11_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_12_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_3 [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_6_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_10_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_11_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_12_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_10_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_10_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_11_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_11_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_12_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_12_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_3 for ONNX node: llama_model/layers.7/self_attn/Concat_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_3 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1529 [Constant]\n",
      "[X] Constant_1529 [Constant] inputs: \n",
      "[X] Constant_1529 [Constant] outputs: [onnx::Unsqueeze_1883 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_7 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1883\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_7 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1883 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_7 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_7\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_7_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_7_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_7 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_13 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_13 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_13 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_14 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_14 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_14 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_15 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_15 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_15 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_4 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_7_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_13_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_14_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_15_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_4 [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_7_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_13_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_14_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_15_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_13_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_13_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_14_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_14_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_15_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_15_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_4 for ONNX node: llama_model/layers.7/self_attn/Concat_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_4 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1535 [Constant]\n",
      "[X] Constant_1535 [Constant] inputs: \n",
      "[X] Constant_1535 [Constant] outputs: [onnx::Unsqueeze_1892 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_8 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1892\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_8 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1892 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_8 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_8\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_8_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_8_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_8 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_16 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_16 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_16 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_16_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_17 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_17 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_17 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_17_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_18 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_18 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_18 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_5 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_8_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_16_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_17_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_18_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_5 [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_8_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_16_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_17_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_18_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_16_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_16_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_17_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_17_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_18_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_18_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_5 for ONNX node: llama_model/layers.7/self_attn/Concat_5\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_5_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_5 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: Constant_1541 [Constant]\n",
      "[X] Constant_1541 [Constant] inputs: \n",
      "[X] Constant_1541 [Constant] outputs: [onnx::Unsqueeze_1901 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_9 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_3_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1901\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_9 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_3_output_0 -> ()[INT32]], [onnx::Unsqueeze_1901 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_9 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_9\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_9_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_9_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_9 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_19 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_19 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_19 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_19_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_20 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_20 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_20 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_20_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_21 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_21 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_21 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_6 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_9_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_19_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_20_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_21_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_6 [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_9_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_19_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_20_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_21_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_19_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_19_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_20_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_20_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_21_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_21_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_6 for ONNX node: llama_model/layers.7/self_attn/Concat_6\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_6_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_6 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape_4 [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_4 [Shape] inputs: [llama_model/layers.7/self_attn/Concat_3_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape_4 for ONNX node: llama_model/layers.7/self_attn/Shape_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_4 [Shape] outputs: [llama_model/layers.7/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/ConstantOfShape [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/ConstantOfShape [ConstantOfShape] inputs: [llama_model/layers.7/self_attn/Shape_4_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/ConstantOfShape_output_0 for ONNX tensor: llama_model/layers.7/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.7/self_attn/ConstantOfShape [ConstantOfShape] outputs: [llama_model/layers.7/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Expand [Expand]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/ConstantOfShape_output_0\n",
      "[X] llama_model/layers.7/self_attn/Expand [Expand] inputs: [llama_model/layers.7/self_attn/rotary_emb/Cast_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/ConstantOfShape_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Expand for ONNX node: llama_model/layers.7/self_attn/Expand\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Expand_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Expand_output_0\n",
      "[X] llama_model/layers.7/self_attn/Expand [Expand] outputs: [llama_model/layers.7/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Tile [Tile]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Expand_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Tile [Tile] inputs: [llama_model/layers.7/self_attn/Expand_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Concat_4_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Tile for ONNX node: llama_model/layers.7/self_attn/Tile\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Tile_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Tile_output_0\n",
      "[X] llama_model/layers.7/self_attn/Tile [Tile] outputs: [llama_model/layers.7/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/GatherElements [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Tile_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.7/self_attn/GatherElements [GatherElements] inputs: [llama_model/layers.7/self_attn/Tile_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/GatherElements for ONNX node: llama_model/layers.7/self_attn/GatherElements\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/GatherElements_output_0 for ONNX tensor: llama_model/layers.7/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.7/self_attn/GatherElements [GatherElements] outputs: [llama_model/layers.7/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape_5 [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_5 [Shape] inputs: [llama_model/layers.7/self_attn/Concat_5_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape_5 for ONNX node: llama_model/layers.7/self_attn/Shape_5\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_5_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_5 [Shape] outputs: [llama_model/layers.7/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/ConstantOfShape_1 [ConstantOfShape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/ConstantOfShape_1 [ConstantOfShape] inputs: [llama_model/layers.7/self_attn/Shape_5_output_0 -> (1)[INT32]], \n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/ConstantOfShape_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/ConstantOfShape_1 [ConstantOfShape] outputs: [llama_model/layers.7/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Expand_1 [Expand]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/rotary_emb/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/ConstantOfShape_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Expand_1 [Expand] inputs: [llama_model/layers.7/self_attn/rotary_emb/Cast_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/ConstantOfShape_1_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Expand_1 for ONNX node: llama_model/layers.7/self_attn/Expand_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Expand_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Expand_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Expand_1 [Expand] outputs: [llama_model/layers.7/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Tile_1 [Tile]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Expand_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_6_output_0\n",
      "[X] llama_model/layers.7/self_attn/Tile_1 [Tile] inputs: [llama_model/layers.7/self_attn/Expand_1_output_0 -> (1, 1, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Concat_6_output_0 -> (4)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Tile_1 for ONNX node: llama_model/layers.7/self_attn/Tile_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Tile_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Tile_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Tile_1 [Tile] outputs: [llama_model/layers.7/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/GatherElements_1 [GatherElements]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Tile_1_output_0\n",
      "[X] Searching for input: llama_model/layers.0/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.7/self_attn/GatherElements_1 [GatherElements] inputs: [llama_model/layers.7/self_attn/Tile_1_output_0 -> (-1, 1, -1, 8)[FLOAT]], [llama_model/layers.0/self_attn/Unsqueeze_11_output_0 -> (1, 1, -1, 1)[INT32]], \n",
      "[X] Using Gather axis: 2\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/GatherElements_1 for ONNX node: llama_model/layers.7/self_attn/GatherElements_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/GatherElements_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/GatherElements_1 [GatherElements] outputs: [llama_model/layers.7/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul [Mul] inputs: [llama_model/layers.7/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Mul for ONNX node: llama_model/layers.7/self_attn/Mul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Mul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Mul_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul [Mul] outputs: [llama_model/layers.7/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape_6 [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_6 [Shape] inputs: [llama_model/layers.7/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape_6 for ONNX node: llama_model/layers.7/self_attn/Shape_6\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_6_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_6_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_6 [Shape] outputs: [llama_model/layers.7/self_attn/Shape_6_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_22 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_22 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_22 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather_4 [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_6_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_22_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_4 [Gather] inputs: [llama_model/layers.7/self_attn/Shape_6_output_0 -> (4)[INT32]], [llama_model/layers.7/self_attn/Constant_22_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_22_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_22_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather_4 for ONNX node: llama_model/layers.7/self_attn/Gather_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_4 [Gather] outputs: [llama_model/layers.7/self_attn/Gather_4_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_23 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_23 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_23 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_23_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div [Div] inputs: [llama_model/layers.7/self_attn/Gather_4_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_23_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_23_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_23_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Div for ONNX node: llama_model/layers.7/self_attn/Div\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Div_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Div_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div [Div] outputs: [llama_model/layers.7/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Div_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast [Cast] inputs: [llama_model/layers.7/self_attn/Div_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast for ONNX node: llama_model/layers.7/self_attn/Cast\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast [Cast] outputs: [llama_model/layers.7/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_1 [Cast] inputs: [llama_model/layers.7/self_attn/Cast_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast_1 for ONNX node: llama_model/layers.7/self_attn/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_1 [Cast] outputs: [llama_model/layers.7/self_attn/Cast_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_24 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_24 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_24 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_24_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_25 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_25 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_25 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_10 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_25_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_10 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_25_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_10 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_10\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_10_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_10_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_10 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_26 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_26 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_26 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_26_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_27 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_27 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_27 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Slice [Slice]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_24_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_10_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_26_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_27_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice [Slice] inputs: [llama_model/layers.7/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Constant_24_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_10_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_26_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_27_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Slice for ONNX node: llama_model/layers.7/self_attn/Slice\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Slice_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice [Slice] outputs: [llama_model/layers.7/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_28 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_28 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_28 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_11 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_28_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_11 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Cast_1_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_28_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_11 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_11\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_11_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_11_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_11 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_29 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_29 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.7/self_attn/Constant_29 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_29_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_30 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_30 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_30 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_30_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_31 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_31 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_31 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Slice_1 [Slice]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_11_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_29_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_30_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_31_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_1 [Slice] inputs: [llama_model/layers.7/self_attn/Transpose_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Unsqueeze_11_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_29_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_30_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_31_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Slice_1 for ONNX node: llama_model/layers.7/self_attn/Slice_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Slice_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_1 [Slice] outputs: [llama_model/layers.7/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Neg [Neg]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Slice_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Neg [Neg] inputs: [llama_model/layers.7/self_attn/Slice_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Neg for ONNX node: llama_model/layers.7/self_attn/Neg\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Neg_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Neg_output_0\n",
      "[X] llama_model/layers.7/self_attn/Neg [Neg] outputs: [llama_model/layers.7/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_7 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Neg_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Slice_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_7 [Concat] inputs: [llama_model/layers.7/self_attn/Neg_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.7/self_attn/Slice_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_7 for ONNX node: llama_model/layers.7/self_attn/Concat_7\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_7_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_7_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_7 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_7_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_1 [Mul] inputs: [llama_model/layers.7/self_attn/Concat_7_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Mul_1 for ONNX node: llama_model/layers.7/self_attn/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Mul_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_1 [Mul] outputs: [llama_model/layers.7/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Mul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add [Add] inputs: [llama_model/layers.7/self_attn/Mul_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Mul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Add for ONNX node: llama_model/layers.7/self_attn/Add\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Add_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Add_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add [Add] outputs: [llama_model/layers.7/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Mul_2 [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/GatherElements_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_2 [Mul] inputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/GatherElements_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Mul_2 for ONNX node: llama_model/layers.7/self_attn/Mul_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Mul_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Mul_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_2 [Mul] outputs: [llama_model/layers.7/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Shape_7 [Shape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_7 [Shape] inputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Shape_7 for ONNX node: llama_model/layers.7/self_attn/Shape_7\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Shape_7_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Shape_7_output_0\n",
      "[X] llama_model/layers.7/self_attn/Shape_7 [Shape] outputs: [llama_model/layers.7/self_attn/Shape_7_output_0 -> (4)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_32 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_32 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_32 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Gather_5 [Gather]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Shape_7_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_32_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_5 [Gather] inputs: [llama_model/layers.7/self_attn/Shape_7_output_0 -> (4)[INT32]], [llama_model/layers.7/self_attn/Constant_32_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_32_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_32_output_0\n",
      "[X] Using Gather axis: 0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Gather_5 for ONNX node: llama_model/layers.7/self_attn/Gather_5\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Gather_5_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Gather_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Gather_5 [Gather] outputs: [llama_model/layers.7/self_attn/Gather_5_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_33 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_33 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_33 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Div_1 [Div]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_5_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_33_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div_1 [Div] inputs: [llama_model/layers.7/self_attn/Gather_5_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_33_output_0 -> ()[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_33_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_33_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Div_1 for ONNX node: llama_model/layers.7/self_attn/Div_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Div_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div_1 [Div] outputs: [llama_model/layers.7/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast_2 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Div_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_2 [Cast] inputs: [llama_model/layers.7/self_attn/Div_1_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast_2 for ONNX node: llama_model/layers.7/self_attn/Cast_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_2 [Cast] outputs: [llama_model/layers.7/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast_3 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_3 [Cast] inputs: [llama_model/layers.7/self_attn/Cast_2_output_0 -> ()[INT32]], \n",
      "[X] Casting to type: int32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast_3 for ONNX node: llama_model/layers.7/self_attn/Cast_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_3 [Cast] outputs: [llama_model/layers.7/self_attn/Cast_3_output_0 -> ()[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_34 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_34 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_34 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_34_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_35 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_35 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_35 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_12 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_35_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_12 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_35_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_12 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_12\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_12_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_12_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_12 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_36 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_36 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_36 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_36_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_37 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_37 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_37 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Slice_2 [Slice]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_34_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_12_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_36_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_37_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_2 [Slice] inputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Constant_34_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_12_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_36_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_37_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Slice_2 for ONNX node: llama_model/layers.7/self_attn/Slice_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Slice_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_2 [Slice] outputs: [llama_model/layers.7/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_38 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_38 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_38 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_13 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_3_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_38_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_13 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Cast_3_output_0 -> ()[INT32]], [llama_model/layers.7/self_attn/Constant_38_output_0 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_13 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_13\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_13_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_13_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_13 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_39 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_39 [Constant] inputs: \n",
      "[X] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647\n",
      "[X] llama_model/layers.7/self_attn/Constant_39 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_39_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_40 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_40 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_40 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_40_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_41 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_41 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_41 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Slice_3 [Slice]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_13_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_39_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_40_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_41_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_3 [Slice] inputs: [llama_model/layers.7/self_attn/Transpose_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Unsqueeze_13_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_39_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_40_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_41_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Slice_3 for ONNX node: llama_model/layers.7/self_attn/Slice_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Slice_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Slice_3 [Slice] outputs: [llama_model/layers.7/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Neg_1 [Neg]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Slice_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Neg_1 [Neg] inputs: [llama_model/layers.7/self_attn/Slice_3_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Neg_1 for ONNX node: llama_model/layers.7/self_attn/Neg_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Neg_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Neg_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Neg_1 [Neg] outputs: [llama_model/layers.7/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_8 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Neg_1_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Slice_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_8 [Concat] inputs: [llama_model/layers.7/self_attn/Neg_1_output_0 -> (-1, 8, -1, 4)[FLOAT]], [llama_model/layers.7/self_attn/Slice_2_output_0 -> (-1, 8, -1, 4)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_8 for ONNX node: llama_model/layers.7/self_attn/Concat_8\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_8_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_8_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_8 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Mul_3 [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_8_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/GatherElements_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_3 [Mul] inputs: [llama_model/layers.7/self_attn/Concat_8_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/GatherElements_1_output_0 -> (1, 1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Mul_3 for ONNX node: llama_model/layers.7/self_attn/Mul_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Mul_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Mul_3 [Mul] outputs: [llama_model/layers.7/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Mul_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Mul_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add_1 [Add] inputs: [llama_model/layers.7/self_attn/Mul_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Mul_3_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Add_1 for ONNX node: llama_model/layers.7/self_attn/Add_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Add_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add_1 [Add] outputs: [llama_model/layers.7/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Transpose_3 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Add_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_3 [Transpose] inputs: [llama_model/layers.7/self_attn/Add_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Transpose_3 for ONNX node: llama_model/layers.7/self_attn/Transpose_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Transpose_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_3 [Transpose] outputs: [llama_model/layers.7/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Add_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/MatMul [MatMul] inputs: [llama_model/layers.7/self_attn/Add_output_0 -> (-1, 8, -1, 8)[FLOAT]], [llama_model/layers.7/self_attn/Transpose_3_output_0 -> (-1, 8, 8, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/MatMul for ONNX node: llama_model/layers.7/self_attn/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/MatMul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/MatMul_output_0\n",
      "[X] llama_model/layers.7/self_attn/MatMul [MatMul] outputs: [llama_model/layers.7/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_42 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_42 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_42 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Div_2 [Div]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_42_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div_2 [Div] inputs: [llama_model/layers.7/self_attn/MatMul_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.7/self_attn/Constant_42_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_42_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_42_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Div_2 for ONNX node: llama_model/layers.7/self_attn/Div_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Div_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Div_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Div_2 [Div] outputs: [llama_model/layers.7/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Add_2 [Add]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Div_2_output_0\n",
      "[X] Searching for input: llama_model/Add_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add_2 [Add] inputs: [llama_model/layers.7/self_attn/Div_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/Add_1_output_0 -> (-1, 1, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Add_2 for ONNX node: llama_model/layers.7/self_attn/Add_2\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Add_2_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Add_2 [Add] outputs: [llama_model/layers.7/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Softmax [Softmax]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Add_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/Softmax [Softmax] inputs: [llama_model/layers.7/self_attn/Add_2_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Softmax for ONNX node: llama_model/layers.7/self_attn/Softmax\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Softmax_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.7/self_attn/Softmax [Softmax] outputs: [llama_model/layers.7/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast_4 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Softmax_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_4 [Cast] inputs: [llama_model/layers.7/self_attn/Softmax_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast_4 for ONNX node: llama_model/layers.7/self_attn/Cast_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_4 [Cast] outputs: [llama_model/layers.7/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Cast_5 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_5 [Cast] inputs: [llama_model/layers.7/self_attn/Cast_4_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Cast_5 for ONNX node: llama_model/layers.7/self_attn/Cast_5\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Cast_5_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Cast_5_output_0\n",
      "[X] llama_model/layers.7/self_attn/Cast_5 [Cast] outputs: [llama_model/layers.7/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/MatMul_1 [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Cast_5_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_2_output_0\n",
      "[X] llama_model/layers.7/self_attn/MatMul_1 [MatMul] inputs: [llama_model/layers.7/self_attn/Cast_5_output_0 -> (-1, 8, -1, -1)[FLOAT]], [llama_model/layers.7/self_attn/Transpose_2_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/MatMul_1 for ONNX node: llama_model/layers.7/self_attn/MatMul_1\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/MatMul_1_output_0 for ONNX tensor: llama_model/layers.7/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/MatMul_1 [MatMul] outputs: [llama_model/layers.7/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Transpose_4 [Transpose]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/MatMul_1_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_4 [Transpose] inputs: [llama_model/layers.7/self_attn/MatMul_1_output_0 -> (-1, 8, -1, 8)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Transpose_4 for ONNX node: llama_model/layers.7/self_attn/Transpose_4\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Transpose_4_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Transpose_4_output_0\n",
      "[X] llama_model/layers.7/self_attn/Transpose_4 [Transpose] outputs: [llama_model/layers.7/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], \n",
      "[X] Parsing node: Constant_1615 [Constant]\n",
      "[X] Constant_1615 [Constant] inputs: \n",
      "[X] Constant_1615 [Constant] outputs: [onnx::Unsqueeze_1986 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_14 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1986\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_14 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_output_0 -> ()[INT32]], [onnx::Unsqueeze_1986 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_14 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_14\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_14_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_14_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_14 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: Constant_1617 [Constant]\n",
      "[X] Constant_1617 [Constant] inputs: \n",
      "[X] Constant_1617 [Constant] outputs: [onnx::Unsqueeze_1988 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Unsqueeze_15 [Unsqueeze]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Gather_1_output_0\n",
      "[X] Searching for input: onnx::Unsqueeze_1988\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_15 [Unsqueeze] inputs: [llama_model/layers.7/self_attn/Gather_1_output_0 -> ()[INT32]], [onnx::Unsqueeze_1988 -> (1)[INT32]], \n",
      "[X] Original shape: (), unsqueezing to: (1,)\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Unsqueeze_15 for ONNX node: llama_model/layers.7/self_attn/Unsqueeze_15\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Unsqueeze_15_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Unsqueeze_15_output_0\n",
      "[X] llama_model/layers.7/self_attn/Unsqueeze_15 [Unsqueeze] outputs: [llama_model/layers.7/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Constant_43 [Constant]\n",
      "[X] llama_model/layers.7/self_attn/Constant_43 [Constant] inputs: \n",
      "[X] llama_model/layers.7/self_attn/Constant_43 [Constant] outputs: [llama_model/layers.7/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Concat_9 [Concat]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_14_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Unsqueeze_15_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Constant_43_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_9 [Concat] inputs: [llama_model/layers.7/self_attn/Unsqueeze_14_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Unsqueeze_15_output_0 -> (1)[INT32]], [llama_model/layers.7/self_attn/Constant_43_output_0 -> (1)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Constant_43_output_0 for ONNX node: llama_model/layers.7/self_attn/Constant_43_output_0\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Concat_9 for ONNX node: llama_model/layers.7/self_attn/Concat_9\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Concat_9_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.7/self_attn/Concat_9 [Concat] outputs: [llama_model/layers.7/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/Reshape_3 [Reshape]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Transpose_4_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Concat_9_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_3 [Reshape] inputs: [llama_model/layers.7/self_attn/Transpose_4_output_0 -> (-1, -1, 8, 8)[FLOAT]], [llama_model/layers.7/self_attn/Concat_9_output_0 -> (3)[INT32]], \n",
      "[X] Registering layer: llama_model/layers.7/self_attn/Reshape_3 for ONNX node: llama_model/layers.7/self_attn/Reshape_3\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/Reshape_3_output_0 for ONNX tensor: llama_model/layers.7/self_attn/Reshape_3_output_0\n",
      "[X] llama_model/layers.7/self_attn/Reshape_3 [Reshape] outputs: [llama_model/layers.7/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/self_attn/o_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/Reshape_3_output_0\n",
      "[X] Searching for input: onnx::MatMul_2338\n",
      "[X] llama_model/layers.7/self_attn/o_proj/MatMul [MatMul] inputs: [llama_model/layers.7/self_attn/Reshape_3_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2338 -> (64, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2338 for ONNX node: onnx::MatMul_2338\n",
      "[X] Registering layer: llama_model/layers.7/self_attn/o_proj/MatMul for ONNX node: llama_model/layers.7/self_attn/o_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/self_attn/o_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/self_attn/o_proj/MatMul [MatMul] outputs: [llama_model/layers.7/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.7/input_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/self_attn/o_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/Add [Add] inputs: [llama_model/layers.7/input_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/self_attn/o_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/Add for ONNX node: llama_model/layers.7/Add\n",
      "[X] Registering tensor: llama_model/layers.7/Add_output_0 for ONNX tensor: llama_model/layers.7/Add_output_0\n",
      "[X] llama_model/layers.7/Add [Add] outputs: [llama_model/layers.7/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/Add_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Cast [Cast] inputs: [llama_model/layers.7/Add_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Cast for ONNX node: llama_model/layers.7/post_attention_layernorm/Cast\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Cast_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Cast_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Cast [Cast] outputs: [llama_model/layers.7/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Constant [Constant]\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant [Constant] inputs: \n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant [Constant] outputs: [llama_model/layers.7/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Constant_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Pow [Pow] inputs: [llama_model/layers.7/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/post_attention_layernorm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Constant_output_0 for ONNX node: llama_model/layers.7/post_attention_layernorm/Constant_output_0\n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Pow for ONNX node: llama_model/layers.7/post_attention_layernorm/Pow\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Pow_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Pow [Pow] outputs: [llama_model/layers.7/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Pow_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/ReduceMean [ReduceMean] inputs: [llama_model/layers.7/post_attention_layernorm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/ReduceMean for ONNX node: llama_model/layers.7/post_attention_layernorm/ReduceMean\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/ReduceMean_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/ReduceMean [ReduceMean] outputs: [llama_model/layers.7/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Constant_1 [Constant]\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant_1 [Constant] outputs: [llama_model/layers.7/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Add [Add]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Constant_1_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Add [Add] inputs: [llama_model/layers.7/post_attention_layernorm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/layers.7/post_attention_layernorm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Constant_1_output_0 for ONNX node: llama_model/layers.7/post_attention_layernorm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Add for ONNX node: llama_model/layers.7/post_attention_layernorm/Add\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Add_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Add [Add] outputs: [llama_model/layers.7/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Add_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Sqrt [Sqrt] inputs: [llama_model/layers.7/post_attention_layernorm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Sqrt for ONNX node: llama_model/layers.7/post_attention_layernorm/Sqrt\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Sqrt_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Sqrt [Sqrt] outputs: [llama_model/layers.7/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Constant_2 [Constant]\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/layers.7/post_attention_layernorm/Constant_2 [Constant] outputs: [llama_model/layers.7/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Div [Div]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Sqrt_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Div [Div] inputs: [llama_model/layers.7/post_attention_layernorm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/layers.7/post_attention_layernorm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Constant_2_output_0 for ONNX node: llama_model/layers.7/post_attention_layernorm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Div for ONNX node: llama_model/layers.7/post_attention_layernorm/Div\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Div_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Div [Div] outputs: [llama_model/layers.7/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Div_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Mul [Mul] inputs: [llama_model/layers.7/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/post_attention_layernorm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Mul for ONNX node: llama_model/layers.7/post_attention_layernorm/Mul\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Mul_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Mul [Mul] outputs: [llama_model/layers.7/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Mul_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Cast_1 [Cast] inputs: [llama_model/layers.7/post_attention_layernorm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Cast_1 for ONNX node: llama_model/layers.7/post_attention_layernorm/Cast_1\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Cast_1_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Cast_1 [Cast] outputs: [llama_model/layers.7/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/post_attention_layernorm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.layers.7.post_attention_layernorm.weight\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Cast_1_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Mul_1 [Mul] inputs: [llama_model.layers.7.post_attention_layernorm.weight -> (64)[FLOAT]], [llama_model/layers.7/post_attention_layernorm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/post_attention_layernorm/Mul_1 for ONNX node: llama_model/layers.7/post_attention_layernorm/Mul_1\n",
      "[X] Registering tensor: llama_model/layers.7/post_attention_layernorm/Mul_1_output_0 for ONNX tensor: llama_model/layers.7/post_attention_layernorm/Mul_1_output_0\n",
      "[X] llama_model/layers.7/post_attention_layernorm/Mul_1 [Mul] outputs: [llama_model/layers.7/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/gate_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2339\n",
      "[X] llama_model/layers.7/mlp/gate_proj/MatMul [MatMul] inputs: [llama_model/layers.7/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2339 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2339 for ONNX node: onnx::MatMul_2339\n",
      "[X] Registering layer: llama_model/layers.7/mlp/gate_proj/MatMul for ONNX node: llama_model/layers.7/mlp/gate_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/gate_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/mlp/gate_proj/MatMul [MatMul] outputs: [llama_model/layers.7/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/act_fn/Sigmoid [Sigmoid]\n",
      "[X] Searching for input: llama_model/layers.7/mlp/gate_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/mlp/act_fn/Sigmoid [Sigmoid] inputs: [llama_model/layers.7/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/mlp/act_fn/Sigmoid for ONNX node: llama_model/layers.7/mlp/act_fn/Sigmoid\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/act_fn/Sigmoid_output_0 for ONNX tensor: llama_model/layers.7/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.7/mlp/act_fn/Sigmoid [Sigmoid] outputs: [llama_model/layers.7/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/act_fn/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/mlp/gate_proj/MatMul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/mlp/act_fn/Sigmoid_output_0\n",
      "[X] llama_model/layers.7/mlp/act_fn/Mul [Mul] inputs: [llama_model/layers.7/mlp/gate_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.7/mlp/act_fn/Sigmoid_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/mlp/act_fn/Mul for ONNX node: llama_model/layers.7/mlp/act_fn/Mul\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/act_fn/Mul_output_0 for ONNX tensor: llama_model/layers.7/mlp/act_fn/Mul_output_0\n",
      "[X] llama_model/layers.7/mlp/act_fn/Mul [Mul] outputs: [llama_model/layers.7/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/up_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2340\n",
      "[X] llama_model/layers.7/mlp/up_proj/MatMul [MatMul] inputs: [llama_model/layers.7/post_attention_layernorm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2340 -> (64, 128)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2340 for ONNX node: onnx::MatMul_2340\n",
      "[X] Registering layer: llama_model/layers.7/mlp/up_proj/MatMul for ONNX node: llama_model/layers.7/mlp/up_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/up_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/mlp/up_proj/MatMul [MatMul] outputs: [llama_model/layers.7/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/Mul [Mul]\n",
      "[X] Searching for input: llama_model/layers.7/mlp/act_fn/Mul_output_0\n",
      "[X] Searching for input: llama_model/layers.7/mlp/up_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/mlp/Mul [Mul] inputs: [llama_model/layers.7/mlp/act_fn/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [llama_model/layers.7/mlp/up_proj/MatMul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/mlp/Mul for ONNX node: llama_model/layers.7/mlp/Mul\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/Mul_output_0 for ONNX tensor: llama_model/layers.7/mlp/Mul_output_0\n",
      "[X] llama_model/layers.7/mlp/Mul [Mul] outputs: [llama_model/layers.7/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/mlp/down_proj/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/layers.7/mlp/Mul_output_0\n",
      "[X] Searching for input: onnx::MatMul_2341\n",
      "[X] llama_model/layers.7/mlp/down_proj/MatMul [MatMul] inputs: [llama_model/layers.7/mlp/Mul_output_0 -> (-1, -1, 128)[FLOAT]], [onnx::MatMul_2341 -> (128, 64)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2341 for ONNX node: onnx::MatMul_2341\n",
      "[X] Registering layer: llama_model/layers.7/mlp/down_proj/MatMul for ONNX node: llama_model/layers.7/mlp/down_proj/MatMul\n",
      "[X] Registering tensor: llama_model/layers.7/mlp/down_proj/MatMul_output_0 for ONNX tensor: llama_model/layers.7/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/mlp/down_proj/MatMul [MatMul] outputs: [llama_model/layers.7/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/layers.7/Add_1 [Add]\n",
      "[X] Searching for input: llama_model/layers.7/post_attention_layernorm/Cast_output_0\n",
      "[X] Searching for input: llama_model/layers.7/mlp/down_proj/MatMul_output_0\n",
      "[X] llama_model/layers.7/Add_1 [Add] inputs: [llama_model/layers.7/post_attention_layernorm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/layers.7/mlp/down_proj/MatMul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/layers.7/Add_1 for ONNX node: llama_model/layers.7/Add_1\n",
      "[X] Registering tensor: llama_model/layers.7/Add_1_output_0 for ONNX tensor: llama_model/layers.7/Add_1_output_0\n",
      "[X] llama_model/layers.7/Add_1 [Add] outputs: [llama_model/layers.7/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Cast [Cast]\n",
      "[X] Searching for input: llama_model/layers.7/Add_1_output_0\n",
      "[X] llama_model/norm/Cast [Cast] inputs: [llama_model/layers.7/Add_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/norm/Cast for ONNX node: llama_model/norm/Cast\n",
      "[X] Registering tensor: llama_model/norm/Cast_output_0 for ONNX tensor: llama_model/norm/Cast_output_0\n",
      "[X] llama_model/norm/Cast [Cast] outputs: [llama_model/norm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Constant [Constant]\n",
      "[X] llama_model/norm/Constant [Constant] inputs: \n",
      "[X] llama_model/norm/Constant [Constant] outputs: [llama_model/norm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Pow [Pow]\n",
      "[X] Searching for input: llama_model/norm/Cast_output_0\n",
      "[X] Searching for input: llama_model/norm/Constant_output_0\n",
      "[X] llama_model/norm/Pow [Pow] inputs: [llama_model/norm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/norm/Constant_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Constant_output_0 for ONNX node: llama_model/norm/Constant_output_0\n",
      "[X] Registering layer: llama_model/norm/Pow for ONNX node: llama_model/norm/Pow\n",
      "[X] Registering tensor: llama_model/norm/Pow_output_0 for ONNX tensor: llama_model/norm/Pow_output_0\n",
      "[X] llama_model/norm/Pow [Pow] outputs: [llama_model/norm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/ReduceMean [ReduceMean]\n",
      "[X] Searching for input: llama_model/norm/Pow_output_0\n",
      "[X] llama_model/norm/ReduceMean [ReduceMean] inputs: [llama_model/norm/Pow_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/ReduceMean for ONNX node: llama_model/norm/ReduceMean\n",
      "[X] Registering tensor: llama_model/norm/ReduceMean_output_0 for ONNX tensor: llama_model/norm/ReduceMean_output_0\n",
      "[X] llama_model/norm/ReduceMean [ReduceMean] outputs: [llama_model/norm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Constant_1 [Constant]\n",
      "[X] llama_model/norm/Constant_1 [Constant] inputs: \n",
      "[X] llama_model/norm/Constant_1 [Constant] outputs: [llama_model/norm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Add [Add]\n",
      "[X] Searching for input: llama_model/norm/ReduceMean_output_0\n",
      "[X] Searching for input: llama_model/norm/Constant_1_output_0\n",
      "[X] llama_model/norm/Add [Add] inputs: [llama_model/norm/ReduceMean_output_0 -> (-1, -1, 1)[FLOAT]], [llama_model/norm/Constant_1_output_0 -> ()[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Constant_1_output_0 for ONNX node: llama_model/norm/Constant_1_output_0\n",
      "[X] Registering layer: llama_model/norm/Add for ONNX node: llama_model/norm/Add\n",
      "[X] Registering tensor: llama_model/norm/Add_output_0 for ONNX tensor: llama_model/norm/Add_output_0\n",
      "[X] llama_model/norm/Add [Add] outputs: [llama_model/norm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Sqrt [Sqrt]\n",
      "[X] Searching for input: llama_model/norm/Add_output_0\n",
      "[X] llama_model/norm/Sqrt [Sqrt] inputs: [llama_model/norm/Add_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Sqrt for ONNX node: llama_model/norm/Sqrt\n",
      "[X] Registering tensor: llama_model/norm/Sqrt_output_0 for ONNX tensor: llama_model/norm/Sqrt_output_0\n",
      "[X] llama_model/norm/Sqrt [Sqrt] outputs: [llama_model/norm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Constant_2 [Constant]\n",
      "[X] llama_model/norm/Constant_2 [Constant] inputs: \n",
      "[X] llama_model/norm/Constant_2 [Constant] outputs: [llama_model/norm/Constant_2_output_0 -> ()[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Div [Div]\n",
      "[X] Searching for input: llama_model/norm/Constant_2_output_0\n",
      "[X] Searching for input: llama_model/norm/Sqrt_output_0\n",
      "[X] llama_model/norm/Div [Div] inputs: [llama_model/norm/Constant_2_output_0 -> ()[FLOAT]], [llama_model/norm/Sqrt_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Constant_2_output_0 for ONNX node: llama_model/norm/Constant_2_output_0\n",
      "[X] Registering layer: llama_model/norm/Div for ONNX node: llama_model/norm/Div\n",
      "[X] Registering tensor: llama_model/norm/Div_output_0 for ONNX tensor: llama_model/norm/Div_output_0\n",
      "[X] llama_model/norm/Div [Div] outputs: [llama_model/norm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Mul [Mul]\n",
      "[X] Searching for input: llama_model/norm/Cast_output_0\n",
      "[X] Searching for input: llama_model/norm/Div_output_0\n",
      "[X] llama_model/norm/Mul [Mul] inputs: [llama_model/norm/Cast_output_0 -> (-1, -1, 64)[FLOAT]], [llama_model/norm/Div_output_0 -> (-1, -1, 1)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Mul for ONNX node: llama_model/norm/Mul\n",
      "[X] Registering tensor: llama_model/norm/Mul_output_0 for ONNX tensor: llama_model/norm/Mul_output_0\n",
      "[X] llama_model/norm/Mul [Mul] outputs: [llama_model/norm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Cast_1 [Cast]\n",
      "[X] Searching for input: llama_model/norm/Mul_output_0\n",
      "[X] llama_model/norm/Cast_1 [Cast] inputs: [llama_model/norm/Mul_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Casting to type: float32\n",
      "[X] Registering layer: llama_model/norm/Cast_1 for ONNX node: llama_model/norm/Cast_1\n",
      "[X] Registering tensor: llama_model/norm/Cast_1_output_0 for ONNX tensor: llama_model/norm/Cast_1_output_0\n",
      "[X] llama_model/norm/Cast_1 [Cast] outputs: [llama_model/norm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: llama_model/norm/Mul_1 [Mul]\n",
      "[X] Searching for input: llama_model.norm.weight\n",
      "[X] Searching for input: llama_model/norm/Cast_1_output_0\n",
      "[X] llama_model/norm/Mul_1 [Mul] inputs: [llama_model.norm.weight -> (64)[FLOAT]], [llama_model/norm/Cast_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Registering layer: llama_model/norm/Mul_1 for ONNX node: llama_model/norm/Mul_1\n",
      "[X] Registering tensor: llama_model/norm/Mul_1_output_0 for ONNX tensor: llama_model/norm/Mul_1_output_0\n",
      "[X] llama_model/norm/Mul_1 [Mul] outputs: [llama_model/norm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], \n",
      "[X] Parsing node: lm_head/MatMul [MatMul]\n",
      "[X] Searching for input: llama_model/norm/Mul_1_output_0\n",
      "[X] Searching for input: onnx::MatMul_2342\n",
      "[X] lm_head/MatMul [MatMul] inputs: [llama_model/norm/Mul_1_output_0 -> (-1, -1, 64)[FLOAT]], [onnx::MatMul_2342 -> (64, 32000)[FLOAT]], \n",
      "[X] Registering layer: onnx::MatMul_2342 for ONNX node: onnx::MatMul_2342\n",
      "[X] Registering layer: lm_head/MatMul for ONNX node: lm_head/MatMul\n",
      "[X] Registering tensor: logits_1024 for ONNX tensor: logits\n",
      "[X] lm_head/MatMul [MatMul] outputs: [logits -> (-1, -1, 32000)[FLOAT]], \n",
      "[X] Marking logits_1024 as output: logits\n",
      "[V] Setting TensorRT Optimization Profiles\n",
      "[V] Input tensor: input_ids (dtype=DataType.INT32, shape=(-1, -1)) | Setting input tensor shapes to: (min=(1, 1), opt=(1, 10), max=(1, 20))\n",
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {input_ids [min=(1, 1), opt=(1, 10), max=(1, 20)]}\n",
      "    ]\n",
      "[I] Building engine with configuration:\n",
      "    Flags                  | [FP16, TF32, OBEY_PRECISION_CONSTRAINTS]\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 3072.00 MiB, TACTIC_DRAM: 15102.06 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
      "[V] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[X] Original: 887 layers\n",
      "[X] After dead-layer removal: 887 layers\n",
      "[X] Graph construction completed in 0.0314579 seconds.\n",
      "[X] Running: IdentityToCastTransform on Identity_0\n",
      "[X] Swap the layer type of Identity_0 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_1\n",
      "[X] Swap the layer type of Identity_1 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_2\n",
      "[X] Swap the layer type of Identity_2 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_3\n",
      "[X] Swap the layer type of Identity_3 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_4\n",
      "[X] Swap the layer type of Identity_4 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_5\n",
      "[X] Swap the layer type of Identity_5 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_6\n",
      "[X] Swap the layer type of Identity_6 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_7\n",
      "[X] Swap the layer type of Identity_7 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_8\n",
      "[X] Swap the layer type of Identity_8 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_9\n",
      "[X] Swap the layer type of Identity_9 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_10\n",
      "[X] Swap the layer type of Identity_10 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_11\n",
      "[X] Swap the layer type of Identity_11 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_12\n",
      "[X] Swap the layer type of Identity_12 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_13\n",
      "[X] Swap the layer type of Identity_13 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_14\n",
      "[X] Swap the layer type of Identity_14 from IDENTITY to CAST\n",
      "[X] Running: IdentityToCastTransform on Identity_15\n",
      "[X] Swap the layer type of Identity_15 from IDENTITY to CAST\n",
      "[X] Running: ConstShuffleFusion on (Unnamed Layer* 46) [Constant]\n",
      "[X] ConstShuffleFusion: Fusing (Unnamed Layer* 46) [Constant] with (Unnamed Layer* 47) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on (Unnamed Layer* 52) [Constant]\n",
      "[X] ConstShuffleFusion: Fusing (Unnamed Layer* 52) [Constant] with (Unnamed Layer* 53) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/Constant_18_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/Constant_18_output_0 with (Unnamed Layer* 83) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/Constant_37_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/Constant_37_output_0 with (Unnamed Layer* 166) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/Constant_38_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/Constant_38_output_0 with (Unnamed Layer* 171) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/input_layernorm/Constant_output_0 with (Unnamed Layer* 177) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 181) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 185) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2038\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2038 with (Unnamed Layer* 198) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2039\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2039 with (Unnamed Layer* 201) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2040\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2040 with (Unnamed Layer* 204) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/self_attn/Constant_44_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/self_attn/Constant_44_output_0 with (Unnamed Layer* 476) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2072\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2072 with (Unnamed Layer* 492) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 497) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 501) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.0/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.0/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 505) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2073\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2073 with (Unnamed Layer* 512) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2074\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2074 with (Unnamed Layer* 517) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2075\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2075 with (Unnamed Layer* 521) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/input_layernorm/Constant_output_0 with (Unnamed Layer* 526) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 530) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 534) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2076\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2076 with (Unnamed Layer* 547) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2077\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2077 with (Unnamed Layer* 550) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2078\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2078 with (Unnamed Layer* 553) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/self_attn/Constant_42_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/self_attn/Constant_42_output_0 with (Unnamed Layer* 811) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2110\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2110 with (Unnamed Layer* 827) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 832) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 836) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.1/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.1/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 840) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2111\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2111 with (Unnamed Layer* 847) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2112\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2112 with (Unnamed Layer* 852) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2113\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2113 with (Unnamed Layer* 856) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/input_layernorm/Constant_output_0 with (Unnamed Layer* 861) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 865) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 869) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2114\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2114 with (Unnamed Layer* 882) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2115\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2115 with (Unnamed Layer* 885) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2116\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2116 with (Unnamed Layer* 888) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/self_attn/Constant_42_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/self_attn/Constant_42_output_0 with (Unnamed Layer* 1146) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2148\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2148 with (Unnamed Layer* 1162) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 1167) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 1171) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.2/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.2/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 1175) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2149\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2149 with (Unnamed Layer* 1182) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2150\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2150 with (Unnamed Layer* 1187) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2151\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2151 with (Unnamed Layer* 1191) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/input_layernorm/Constant_output_0 with (Unnamed Layer* 1196) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 1200) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 1204) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2152\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2152 with (Unnamed Layer* 1217) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2153\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2153 with (Unnamed Layer* 1220) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2154\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2154 with (Unnamed Layer* 1223) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/self_attn/Constant_42_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/self_attn/Constant_42_output_0 with (Unnamed Layer* 1481) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2186\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2186 with (Unnamed Layer* 1497) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 1502) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 1506) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.3/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.3/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 1510) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2187\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2187 with (Unnamed Layer* 1517) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2188\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2188 with (Unnamed Layer* 1522) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2189\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2189 with (Unnamed Layer* 1526) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/input_layernorm/Constant_output_0 with (Unnamed Layer* 1531) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 1535) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 1539) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2190\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2190 with (Unnamed Layer* 1552) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2191\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2191 with (Unnamed Layer* 1555) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2192\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2192 with (Unnamed Layer* 1558) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/self_attn/Constant_42_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/self_attn/Constant_42_output_0 with (Unnamed Layer* 1816) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2224\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2224 with (Unnamed Layer* 1832) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 1837) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 1841) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.4/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.4/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 1845) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2225\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2225 with (Unnamed Layer* 1852) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2226\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2226 with (Unnamed Layer* 1857) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2227\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2227 with (Unnamed Layer* 1861) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/input_layernorm/Constant_output_0 with (Unnamed Layer* 1866) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 1870) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 1874) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2228\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2228 with (Unnamed Layer* 1887) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2229\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2229 with (Unnamed Layer* 1890) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2230\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2230 with (Unnamed Layer* 1893) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/self_attn/Constant_42_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/self_attn/Constant_42_output_0 with (Unnamed Layer* 2151) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2262\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2262 with (Unnamed Layer* 2167) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 2172) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 2176) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.5/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.5/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 2180) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2263\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2263 with (Unnamed Layer* 2187) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2264\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2264 with (Unnamed Layer* 2192) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2265\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2265 with (Unnamed Layer* 2196) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/input_layernorm/Constant_output_0 with (Unnamed Layer* 2201) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 2205) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 2209) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2266\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2266 with (Unnamed Layer* 2222) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2267\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2267 with (Unnamed Layer* 2225) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2268\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2268 with (Unnamed Layer* 2228) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/self_attn/Constant_42_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/self_attn/Constant_42_output_0 with (Unnamed Layer* 2486) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2300\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2300 with (Unnamed Layer* 2502) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 2507) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 2511) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.6/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.6/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 2515) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2301\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2301 with (Unnamed Layer* 2522) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2302\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2302 with (Unnamed Layer* 2527) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2303\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2303 with (Unnamed Layer* 2531) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/input_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/input_layernorm/Constant_output_0 with (Unnamed Layer* 2536) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/input_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/input_layernorm/Constant_1_output_0 with (Unnamed Layer* 2540) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/input_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/input_layernorm/Constant_2_output_0 with (Unnamed Layer* 2544) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2304\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2304 with (Unnamed Layer* 2557) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2305\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2305 with (Unnamed Layer* 2560) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2306\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2306 with (Unnamed Layer* 2563) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/self_attn/Constant_42_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/self_attn/Constant_42_output_0 with (Unnamed Layer* 2821) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2338\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2338 with (Unnamed Layer* 2837) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/post_attention_layernorm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/post_attention_layernorm/Constant_output_0 with (Unnamed Layer* 2842) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/post_attention_layernorm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/post_attention_layernorm/Constant_1_output_0 with (Unnamed Layer* 2846) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/layers.7/post_attention_layernorm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/layers.7/post_attention_layernorm/Constant_2_output_0 with (Unnamed Layer* 2850) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2339\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2339 with (Unnamed Layer* 2857) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2340\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2340 with (Unnamed Layer* 2862) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2341\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2341 with (Unnamed Layer* 2866) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/norm/Constant_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/norm/Constant_output_0 with (Unnamed Layer* 2871) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/norm/Constant_1_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/norm/Constant_1_output_0 with (Unnamed Layer* 2875) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on llama_model/norm/Constant_2_output_0\n",
      "[X] ConstShuffleFusion: Fusing llama_model/norm/Constant_2_output_0 with (Unnamed Layer* 2879) [Shuffle]\n",
      "[X] Running: ConstShuffleFusion on onnx::MatMul_2342\n",
      "[X] ConstShuffleFusion: Fusing onnx::MatMul_2342 with (Unnamed Layer* 2886) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/Unsqueeze_12\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/Unsqueeze_12 with llama_model/Unsqueeze_13\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/Unsqueeze\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/Unsqueeze with llama_model/Reshape\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/Unsqueeze + llama_model/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/Unsqueeze + llama_model/Reshape with llama_model/layers.0/self_attn/Unsqueeze_10\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Reshape with llama_model/layers.0/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Reshape_1 with llama_model/layers.0/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Reshape_2 with llama_model/layers.0/self_attn/Transpose_2\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/Unsqueeze + llama_model/Reshape + llama_model/layers.0/self_attn/Unsqueeze_10\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/Unsqueeze + llama_model/Reshape + llama_model/layers.0/self_attn/Unsqueeze_10 with llama_model/layers.0/self_attn/Unsqueeze_11\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/Unsqueeze_7\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/Unsqueeze_7 with llama_model/Unsqueeze_8\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 480) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 480) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.0/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.0/self_attn/Transpose_4 with llama_model/layers.0/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Reshape with llama_model/layers.1/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Reshape_1 with llama_model/layers.1/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Reshape_2 with llama_model/layers.1/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 815) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 815) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.1/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.1/self_attn/Transpose_4 with llama_model/layers.1/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Reshape with llama_model/layers.2/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Reshape_1 with llama_model/layers.2/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Reshape_2 with llama_model/layers.2/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 1150) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 1150) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.2/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.2/self_attn/Transpose_4 with llama_model/layers.2/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Reshape with llama_model/layers.3/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Reshape_1 with llama_model/layers.3/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Reshape_2 with llama_model/layers.3/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 1485) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 1485) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.3/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.3/self_attn/Transpose_4 with llama_model/layers.3/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.4/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.4/self_attn/Reshape with llama_model/layers.4/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.4/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.4/self_attn/Reshape_1 with llama_model/layers.4/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.4/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.4/self_attn/Reshape_2 with llama_model/layers.4/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 1820) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 1820) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.4/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.4/self_attn/Transpose_4 with llama_model/layers.4/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.5/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.5/self_attn/Reshape with llama_model/layers.5/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.5/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.5/self_attn/Reshape_1 with llama_model/layers.5/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.5/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.5/self_attn/Reshape_2 with llama_model/layers.5/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 2155) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 2155) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.5/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.5/self_attn/Transpose_4 with llama_model/layers.5/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.6/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.6/self_attn/Reshape with llama_model/layers.6/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.6/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.6/self_attn/Reshape_1 with llama_model/layers.6/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.6/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.6/self_attn/Reshape_2 with llama_model/layers.6/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 2490) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 2490) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.6/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.6/self_attn/Transpose_4 with llama_model/layers.6/self_attn/Reshape_3\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.7/self_attn/Reshape\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.7/self_attn/Reshape with llama_model/layers.7/self_attn/Transpose\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.7/self_attn/Reshape_1\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.7/self_attn/Reshape_1 with llama_model/layers.7/self_attn/Transpose_1\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.7/self_attn/Reshape_2\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.7/self_attn/Reshape_2 with llama_model/layers.7/self_attn/Transpose_2\n",
      "[X] Running: ShuffleErasure on (Unnamed Layer* 2825) [Shuffle]\n",
      "[X] Removing (Unnamed Layer* 2825) [Shuffle]\n",
      "[X] Running: ShuffleShuffleFusion on llama_model/layers.7/self_attn/Transpose_4\n",
      "[X] ShuffleShuffleFusion: Fusing llama_model/layers.7/self_attn/Transpose_4 with llama_model/layers.7/self_attn/Reshape_3\n",
      "[X] After Myelin optimization: 1 layers\n",
      "[X] Applying ScaleNodes fusions.\n",
      "[X] After scale fusion: 1 layers\n",
      "[X] After dupe layer removal: 1 layers\n",
      "[X] After final dead-layer removal: 1 layers\n",
      "[X] After tensor merging: 1 layers\n",
      "[X] After vertical fusions: 1 layers\n",
      "[X] After dupe layer removal: 1 layers\n",
      "[X] After final dead-layer removal: 1 layers\n",
      "[X] After tensor merging: 1 layers\n",
      "[X] After slice removal: 1 layers\n",
      "[X] After concat removal: 1 layers\n",
      "[X] Trying to split Reshape and strided tensor\n",
      "[V] Graph optimization time: 0.075985 seconds.\n",
      "[V] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[X] Building graph using backend strategy 2\n",
      "[V] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[X] Constructing optimization profile number 0 [1/1].\n",
      "[X] Applying generic optimizations to the graph for inference.\n",
      "[X] 0 requirements combinations were removed due to type constraints.\n",
      "[X] Reserving memory for host IO tensors. Host: 0 bytes\n",
      "[X] =============== Computing costs for {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]}\n",
      "[X] *************** Autotuning format combination: Int32(sequence,1) -> Float((* 32000 sequence),32000,1) ***************\n",
      "[X] --------------- Timing Runner: {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023])\n",
      "[X]  (foreignNode) Set user's cuda kernel library\n",
      "[X] Tactic: 0x0000000000000000 Time: 0.492395\n",
      "[X] {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023]) profiling completed in 17.0946 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.492395\n",
      "[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000\n",
      "[X] *************** Autotuning format combination: Int32(sequence,1) -> Half((* 32000 sequence),32000,1) ***************\n",
      "[X] --------------- Timing Runner: {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023])\n",
      "[X]  (foreignNode) Set user's cuda kernel library\n",
      "[X] Tactic: 0x0000000000000000 Time: 0.426421\n",
      "[X] {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]} (Myelin[0x80000023]) profiling completed in 16.621 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.426421\n",
      "[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000\n",
      "[X] =============== Computing reformatting costs\n",
      "[X] =============== Computing reformatting costs\n",
      "[X] =============== Computing reformatting costs: \n",
      "[X] *************** Autotuning Reformat: Float((* 32000 sequence),32000,1) -> Half((* 32000 sequence),32000,1) ***************\n",
      "[X] --------------- Timing Runner: Optimizer Reformat(<in> -> logits) (Reformat[0x80000006])\n",
      "[X] Tactic: 0x00000000000003e8 Time: 0.00404961\n",
      "[X] Tactic: 0x00000000000003ea Time: 0.0127344\n",
      "[X] Tactic: 0x0000000000000000 Time: 0.0096003\n",
      "[X] Optimizer Reformat(<in> -> logits) (Reformat[0x80000006]) profiling completed in 0.0161862 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00404961\n",
      "[X] Formats and tactics selection completed in 33.7703 seconds.\n",
      "[X] After reformat layers: 1 layers\n",
      "[X] Total number of blocks in pre-optimized block assignment: 1\n",
      "[V] Detected 1 inputs and 1 output network tensors.\n",
      "[X] Layer: {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]} Host Persistent: 32 Device Persistent: 0 Scratch Memory: 40448\n",
      "[X] Skipped printing memory information for 0 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.\n",
      "[V] Total Host Persistent Memory: 32\n",
      "[V] Total Device Persistent Memory: 0\n",
      "[V] Total Scratch Memory: 40448\n",
      "[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 8 MiB, GPU 20 MiB\n",
      "[V] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.\n",
      "[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.02407ms to assign 1 blocks to 1 nodes requiring 40448 bytes.\n",
      "[X] Total number of blocks in optimized block assignment: 1\n",
      "[V] Total Activation Memory: 40448\n",
      "[X] Total number of generated kernels selected for the engine: 0\n",
      "[X] Disabling unused tactic source: EDGE_MASK_CONVOLUTIONS\n",
      "[X] Disabling unused tactic source: JIT_CONVOLUTIONS\n",
      "[X] Engine generation completed in 34.2658 seconds.\n",
      "[W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[W] Check verbose logs for the list of affected weights.\n",
      "[W] - 75 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[X]   List of affected weights: llama_model/layers.0/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 181) [Shuffle], llama_model/layers.0/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 501) [Shuffle], llama_model/layers.1/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 530) [Shuffle], llama_model/layers.1/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 836) [Shuffle], llama_model/layers.2/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 865) [Shuffle], llama_model/layers.2/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 1171) [Shuffle], llama_model/layers.3/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 1200) [Shuffle], llama_model/layers.3/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 1506) [Shuffle], llama_model/layers.4/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 1535) [Shuffle], llama_model/layers.4/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 1841) [Shuffle], llama_model/layers.5/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 1870) [Shuffle], llama_model/layers.5/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 2176) [Shuffle], llama_model/layers.6/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 2205) [Shuffle], llama_model/layers.6/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 2511) [Shuffle], llama_model/layers.7/input_layernorm/Constant_1_output_0 + (Unnamed Layer* 2540) [Shuffle], llama_model/layers.7/post_attention_layernorm/Constant_1_output_0 + (Unnamed Layer* 2846) [Shuffle], llama_model/norm/Constant_1_output_0 + (Unnamed Layer* 2875) [Shuffle], llama_model_embed_tokens_weight_constant, onnx__MatMul_2038 _ (Unnamed Layer_ 198) [Shuffle]_constant, onnx__MatMul_2039 _ (Unnamed Layer_ 201) [Shuffle]_constant, onnx__MatMul_2040 _ (Unnamed Layer_ 204) [Shuffle]_constant, onnx__MatMul_2072 _ (Unnamed Layer_ 492) [Shuffle]_constant, onnx__MatMul_2073 _ (Unnamed Layer_ 512) [Shuffle]_constant, onnx__MatMul_2074 _ (Unnamed Layer_ 517) [Shuffle]_constant, onnx__MatMul_2075 _ (Unnamed Layer_ 521) [Shuffle]_constant, onnx__MatMul_2076 _ (Unnamed Layer_ 547) [Shuffle]_constant, onnx__MatMul_2077 _ (Unnamed Layer_ 550) [Shuffle]_constant, onnx__MatMul_2078 _ (Unnamed Layer_ 553) [Shuffle]_constant, onnx__MatMul_2110 _ (Unnamed Layer_ 827) [Shuffle]_constant, onnx__MatMul_2111 _ (Unnamed Layer_ 847) [Shuffle]_constant, onnx__MatMul_2112 _ (Unnamed Layer_ 852) [Shuffle]_constant, onnx__MatMul_2113 _ (Unnamed Layer_ 856) [Shuffle]_constant, onnx__MatMul_2114 _ (Unnamed Layer_ 882) [Shuffle]_constant, onnx__MatMul_2115 _ (Unnamed Layer_ 885) [Shuffle]_constant, onnx__MatMul_2116 _ (Unnamed Layer_ 888) [Shuffle]_constant, onnx__MatMul_2148 _ (Unnamed Layer_ 1162) [Shuffle]_constant, onnx__MatMul_2149 _ (Unnamed Layer_ 1182) [Shuffle]_constant, onnx__MatMul_2150 _ (Unnamed Layer_ 1187) [Shuffle]_constant, onnx__MatMul_2151 _ (Unnamed Layer_ 1191) [Shuffle]_constant, onnx__MatMul_2152 _ (Unnamed Layer_ 1217) [Shuffle]_constant, onnx__MatMul_2153 _ (Unnamed Layer_ 1220) [Shuffle]_constant, onnx__MatMul_2154 _ (Unnamed Layer_ 1223) [Shuffle]_constant, onnx__MatMul_2186 _ (Unnamed Layer_ 1497) [Shuffle]_constant, onnx__MatMul_2187 _ (Unnamed Layer_ 1517) [Shuffle]_constant, onnx__MatMul_2188 _ (Unnamed Layer_ 1522) [Shuffle]_constant, onnx__MatMul_2189 _ (Unnamed Layer_ 1526) [Shuffle]_constant, onnx__MatMul_2190 _ (Unnamed Layer_ 1552) [Shuffle]_constant, onnx__MatMul_2191 _ (Unnamed Layer_ 1555) [Shuffle]_constant, onnx__MatMul_2192 _ (Unnamed Layer_ 1558) [Shuffle]_constant, onnx__MatMul_2224 _ (Unnamed Layer_ 1832) [Shuffle]_constant, onnx__MatMul_2225 _ (Unnamed Layer_ 1852) [Shuffle]_constant, onnx__MatMul_2226 _ (Unnamed Layer_ 1857) [Shuffle]_constant, onnx__MatMul_2227 _ (Unnamed Layer_ 1861) [Shuffle]_constant, onnx__MatMul_2228 _ (Unnamed Layer_ 1887) [Shuffle]_constant, onnx__MatMul_2229 _ (Unnamed Layer_ 1890) [Shuffle]_constant, onnx__MatMul_2230 _ (Unnamed Layer_ 1893) [Shuffle]_constant, onnx__MatMul_2262 _ (Unnamed Layer_ 2167) [Shuffle]_constant, onnx__MatMul_2263 _ (Unnamed Layer_ 2187) [Shuffle]_constant, onnx__MatMul_2264 _ (Unnamed Layer_ 2192) [Shuffle]_constant, onnx__MatMul_2265 _ (Unnamed Layer_ 2196) [Shuffle]_constant, onnx__MatMul_2266 _ (Unnamed Layer_ 2222) [Shuffle]_constant, onnx__MatMul_2267 _ (Unnamed Layer_ 2225) [Shuffle]_constant, onnx__MatMul_2268 _ (Unnamed Layer_ 2228) [Shuffle]_constant, onnx__MatMul_2300 _ (Unnamed Layer_ 2502) [Shuffle]_constant, onnx__MatMul_2301 _ (Unnamed Layer_ 2522) [Shuffle]_constant, onnx__MatMul_2302 _ (Unnamed Layer_ 2527) [Shuffle]_constant, onnx__MatMul_2303 _ (Unnamed Layer_ 2531) [Shuffle]_constant, onnx__MatMul_2304 _ (Unnamed Layer_ 2557) [Shuffle]_constant, onnx__MatMul_2305 _ (Unnamed Layer_ 2560) [Shuffle]_constant, onnx__MatMul_2306 _ (Unnamed Layer_ 2563) [Shuffle]_constant, onnx__MatMul_2338 _ (Unnamed Layer_ 2837) [Shuffle]_constant, onnx__MatMul_2339 _ (Unnamed Layer_ 2857) [Shuffle]_constant, onnx__MatMul_2340 _ (Unnamed Layer_ 2862) [Shuffle]_constant, onnx__MatMul_2341 _ (Unnamed Layer_ 2866) [Shuffle]_constant, onnx__MatMul_2342 _ (Unnamed Layer_ 2886) [Shuffle]_constant\n",
      "[W] - 3 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "[X]   List of affected weights: llama_model_embed_tokens_weight_constant, onnx__MatMul_2152 _ (Unnamed Layer_ 1217) [Shuffle]_constant, onnx__MatMul_2342 _ (Unnamed Layer_ 2886) [Shuffle]_constant\n",
      "[W] - 2 weights are affected by this issue: Detected finite FP32 values which would overflow in FP16 and converted them to the closest finite FP16 value.\n",
      "[X]   List of affected weights: (Unnamed Layer* 52) [Constant] + (Unnamed Layer* 53) [Shuffle], llama_model/Constant_38_output_0 + (Unnamed Layer* 171) [Shuffle]\n",
      "[X] Engine Layer Information:\n",
      "    Layer(Myelin): {ForeignNode[(Unnamed Layer* 46) [Constant] + (Unnamed Layer* 47) [Shuffle]...lm_head/MatMul]}, Tactic: 0x0000000000000000, input_ids (Int32[1,-1]) -> logits (Half[1,-1,32000])\n",
      "[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +16, now: CPU 0, GPU 16 (MiB)\n",
      "[X] Adding 1 engine(s) to plan file.\n",
      "[I] Finished engine building in 34.395 seconds\n",
      "[X] Deleting timing cache: 1 entries, served 0 hits since creation.\n",
      "[X] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1\n",
      "[X] Plugin creator already registered - ::BatchedNMS_TRT version 1\n",
      "[X] Plugin creator already registered - ::BatchTilePlugin_TRT version 1\n",
      "[X] Plugin creator already registered - ::Clip_TRT version 1\n",
      "[X] Plugin creator already registered - ::CoordConvAC version 1\n",
      "[X] Plugin creator already registered - ::CropAndResizeDynamic version 1\n",
      "[X] Plugin creator already registered - ::CropAndResize version 1\n",
      "[X] Plugin creator already registered - ::DecodeBbox3DPlugin version 1\n",
      "[X] Plugin creator already registered - ::DetectionLayer_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1\n",
      "[X] Plugin creator already registered - ::EfficientNMS_TRT version 1\n",
      "[X] Plugin creator already registered - ::FlattenConcat_TRT version 1\n",
      "[X] Plugin creator already registered - ::GenerateDetection_TRT version 1\n",
      "[X] Plugin creator already registered - ::GridAnchor_TRT version 1\n",
      "[X] Plugin creator already registered - ::GridAnchorRect_TRT version 1\n",
      "[X] Plugin creator already registered - ::InstanceNormalization_TRT version 1\n",
      "[X] Plugin creator already registered - ::InstanceNormalization_TRT version 2\n",
      "[X] Plugin creator already registered - ::LReLU_TRT version 1\n",
      "[X] Plugin creator already registered - ::ModulatedDeformConv2d version 1\n",
      "[X] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1\n",
      "[X] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1\n",
      "[X] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[X] Plugin creator already registered - ::NMSDynamic_TRT version 1\n",
      "[X] Plugin creator already registered - ::NMS_TRT version 1\n",
      "[X] Plugin creator already registered - ::Normalize_TRT version 1\n",
      "[X] Plugin creator already registered - ::PillarScatterPlugin version 1\n",
      "[X] Plugin creator already registered - ::PriorBox_TRT version 1\n",
      "[X] Plugin creator already registered - ::ProposalDynamic version 1\n",
      "[X] Plugin creator already registered - ::ProposalLayer_TRT version 1\n",
      "[X] Plugin creator already registered - ::Proposal version 1\n",
      "[X] Plugin creator already registered - ::PyramidROIAlign_TRT version 1\n",
      "[X] Plugin creator already registered - ::Region_TRT version 1\n",
      "[X] Plugin creator already registered - ::Reorg_TRT version 1\n",
      "[X] Plugin creator already registered - ::ResizeNearest_TRT version 1\n",
      "[X] Plugin creator already registered - ::ROIAlign_TRT version 1\n",
      "[X] Plugin creator already registered - ::RPROI_TRT version 1\n",
      "[X] Plugin creator already registered - ::ScatterND version 1\n",
      "[X] Plugin creator already registered - ::SpecialSlice_TRT version 1\n",
      "[X] Plugin creator already registered - ::Split version 1\n",
      "[X] Plugin creator already registered - ::VoxelGeneratorPlugin version 1\n",
      "[V] Loaded engine size: 10 MiB\n",
      "[X] Deserialization required 6181 microseconds.\n",
      "[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +8, now: CPU 0, GPU 8 (MiB)\n",
      "[X] Adding 1 engine(s) to plan file.\n",
      "[I] Saving engine to ./models/llama-xs/trt-engine/llama-xs-bs1.engine\n"
     ]
    }
   ],
   "source": [
    "engine_path = os.path.join(trt_engine_folder, f\"{model_name}-{engine_tag}.engine\")\n",
    "\n",
    "if os.path.exists(engine_path):\n",
    "    os.remove(engine_path)\n",
    "\n",
    "if not os.path.exists(engine_path):\n",
    "    gpt2_engine = GPT2ONNXFile(onnx_path, metadata).as_trt_engine(output_fpath=engine_path, profiles=profiles, preview_features=preview_features)\n",
    "else:\n",
    "    gpt2_engine = GPT2TRTEngine(engine_path, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82d909c9-6783-4a92-8ffd-7b0d598d8daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GPT2.export.GPT2TRTEngine at 0x7f0f8d8df640>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9ef7153-808d-4bd0-90c9-ebc44b1eb6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.905692"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(engine_path).stat().st_size / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3378c4dc-ad06-41ae-81ed-29cf7cfc533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {},
   "source": [
    "### Inference with TensorRT engine\n",
    "\n",
    "Great, if you have reached this stage, it means we now have an optimized TensorRT engine for the GPT-2 model, ready for us to carry out inference. \n",
    "\n",
    "The GPT-2 model with TensorRT backend can now be employed in place of the original HuggingFace GPT-2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae13aa-bf6f-4eb7-a453-389865562ae4",
   "metadata": {},
   "source": [
    "#### Single batch inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "967ed7e6-b893-4bf6-b31b-b029a07f899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport Llama\n",
    "%aimport Llama.trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30585cb4-1d08-43d3-84c6-ed0757dbc6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 64,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 128,\n",
       "  \"max_position_embeddings\": 128,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.34.0.dev0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "343b58f1-3d9f-4844-85c9-73058bd36a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Llama.trt import GPT2TRTDecoder\n",
    "from Llama.measurements import gpt2_inference, full_inference\n",
    "from NNDF.networks import TimingProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0cfda583-b684-48b1-9046-15ab022ef982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2_trt = GPT2TRTDecoder(gpt2_engine, metadata, xs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28fc60ad-73a7-46df-85d7-a292a8abbd80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006361159994412446"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Benchmarking TensorRT performance on single batch\n",
    "_, decoder_e2e_median_time = gpt2_inference(\n",
    "            gpt2_trt, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    "        )\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01d86d29-1c7b-4020-9ef2-b77ea5e52764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = gpt2_trt(input_ids=input_ids)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d32e0162-c9eb-473d-ace6-c4c61ff578b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0404, -0.2122, -0.0286,  ...,  0.2773, -0.1493,  0.1033],\n",
       "          [ 0.0950, -0.1624, -0.0799,  ...,  0.3198,  0.0576,  0.1143],\n",
       "          [ 0.1537, -0.1099, -0.1252,  ...,  0.2605, -0.1509,  0.1006],\n",
       "          ...,\n",
       "          [ 0.0129, -0.1873, -0.1846,  ...,  0.4131, -0.0952,  0.1332],\n",
       "          [ 0.0148,  0.0054,  0.0554,  ...,  0.4021, -0.0177,  0.1836],\n",
       "          [ 0.1213, -0.1300, -0.0356,  ...,  0.4036,  0.0311,  0.1273]]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " torch.Size([1, 8, 32000]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22122064-5a17-4990-bd6b-073fca5a3e9b",
   "metadata": {},
   "source": [
    "#### Open-end text generation\n",
    "Let's generate the same task again. Since GPT-2 is an open-ended model, a small turbulent in the model might have a very different result. Since we have done some format changes and input/output restriction while exporting the model, you might see a different result compared to raw HuggingFace model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "848bffb8-a7a4-4fcb-91c9-f4e9f7263e6c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.4 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sample_output = gpt2_trt.generate(input_ids.cuda(), max_length=max_length)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d23b3124-cefa-4470-90cc-66a94ef868ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is a dogashaashaasha sigloulsût sigloulsût Новût Нов'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output = gpt2_trt.generate(input_ids.cuda(), max_length=max_length)\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "068bdb2d-e59d-4ef1-8d5e-3c82931af03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"use_cache\": false\n",
       "}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_trt.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4c8bc4c-bf3e-4cb5-afc6-c0bd7d8655cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get complete decoder inference result and its timing profile\n",
    "# _, full_e2e_median_runtime = full_inference(\n",
    "#     gpt2_trt, input_ids.cuda(), tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "#     max_length=max_length\n",
    "# )\n",
    "# full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68a915-2c32-49e5-b1f6-e93d7618f637",
   "metadata": {},
   "source": [
    "You can now compare the output of the original PyTorch model and the TensorRT engine. Notice the speed difference. On an NVIDIA V100 32GB GPU, this results in about ~5x performance improvement for the GPT-2 model (from an average of 0.704s to 0.134s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2562388-d97b-45dd-8569-3f6c053f4e98",
   "metadata": {},
   "source": [
    "Now you have known how to convert a model to onnx, build TRT engine and optimize it. As you might have recalled, using kv cache and beam search are two important ways to improve the performance of the decoder models. We have recently added thse support to our HuggingFace demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4132e54-aba7-42ec-8324-c68d82c17296",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. Advanced Topic: KV Cache\n",
    "\n",
    "As you have seen above, we put `use_cache = False` in some code blocks. This is because in the simplified model, we only take `input_ids` as input and `logits` as output. `input_ids` is growing as the sequence goes longer. In reality, we sometimes cache the self-attentions for each layer and reuse them in the later computations. This allows us to only take the last generated `input_ids`. This is a trade-off between space and time. When the model is small or the sequence is small, the D2D data copy time usually outweights the performance improvement of the model. However, performance improvements have been found in larger models with larger sequence length like 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e33d1dcb-250f-4d86-9726-b114d4962fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m kv_config \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Config\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(GPT2_VARIANT, use_cache \u001b[38;5;241m=\u001b[39m use_cache)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPT2Config' is not defined"
     ]
    }
   ],
   "source": [
    "use_cache = True\n",
    "kv_config = GPT2Config.from_pretrained(GPT2_VARIANT, use_cache = use_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fdf0f-2da0-46c0-a948-e4e6e16b898a",
   "metadata": {},
   "source": [
    "#### Raw HuggingFace\n",
    "\n",
    "The model that we download from `GPT2LMHeadModel.from_pretrained` is dynamic in its inputs. It can take both kv and non-kv configurations. Changing `use_cache` will do it. You can see that changing this configuration, the output is changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3c51a-07ee-4936-b620-50766a45b945",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    model, input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, use_cache = use_cache\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14607bf-f449-4151-9076-d099ae1a3ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_output = model.generate(input_ids, max_length=max_length, use_cache = use_cache)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057ef83-0cdc-4631-9958-66d04fc7fc22",
   "metadata": {},
   "source": [
    "#### TensorRT\n",
    "\n",
    "For the 1st decoding step, we take `input_ids` and generate both `logits` and the kv cache. In other steps, we take the new `input_ids` with `past` kv-cache and the outputs are `logits` and the updated `present` kv-cache. Taking dynamic number of inputs for trt is not currently supported in our demo, so we need to output 2 onnx files and build 2 engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbcfad-9c9c-47e2-894a-731c7a3a04df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_metadata = NetworkMetadata(variant=GPT2_VARIANT, precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=use_cache))\n",
    "kv_gpt2 = GPT2TorchFile(model.to('cpu'), kv_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe680c5-d9ff-466f-87fe-a7bb0cbee944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_onnx_path = ('./models/{}/ONNX/{}-kv_cache.onnx'.format(GPT2_VARIANT, GPT2_VARIANT))\n",
    "kv_gpt2.as_onnx_model(kv_onnx_path, force_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f6824-286d-4afa-926b-7eed4cafafc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_onnx_model = onnx.load(kv_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f0012-7a2d-41be-a8d8-c818dcb7c244",
   "metadata": {},
   "source": [
    "We could see that the kv model has #inputs = #outputs = num_layers * 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579aeec-2c7a-43de-b8f7-beff8d3d7784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(kv_onnx_model.graph.input), len(kv_onnx_model.graph.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add1139-aab0-4531-b2ac-c3aca90e5d49",
   "metadata": {},
   "source": [
    "The next blocks will set up the profile and build the engine. The only difference is that we now have the profile for kv cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae055cb-41b7-4523-86bc-490bc9edf204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "disable_preview_dynamic_shapes = False\n",
    "\n",
    "engine_tag = \"bs{}\".format(batch_size)\n",
    "\n",
    "preview_features = [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-disableFasterDynamicShapes\"\n",
    "    preview_features = []\n",
    "\n",
    "use_input_length = False\n",
    "num_heads = kv_config.n_head\n",
    "embedding_size_per_head = kv_config.n_embd // num_heads\n",
    "num_layers = kv_config.n_layer\n",
    "\n",
    "max_sequence_length = max_length\n",
    "max_output_length = max_length\n",
    "if not use_input_length:\n",
    "    opt_input_seq_len = max_sequence_length // 2\n",
    "else:\n",
    "    opt_input_seq_len = input_ids.shape[1]\n",
    "\n",
    "opt_output_seq_len = max_output_length // 2\n",
    "\n",
    "# context phase uses the provided input_ids to generate hidden states and self attention kv cache\n",
    "# It is only used in the 1st decoder run.\n",
    "dec_profiles_context = Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, opt_output_seq_len),\n",
    "    max=(batch_size, max_output_length),\n",
    ")\n",
    "self_attention_profile_context = {\n",
    "    \"min\": (batch_size, num_heads, 0, embedding_size_per_head),\n",
    "    \"opt\": (batch_size, num_heads, 0, embedding_size_per_head),\n",
    "    \"max\": (batch_size, num_heads, 0, embedding_size_per_head),\n",
    "}\n",
    "\n",
    "# generation phase uses previous self attention kv cache with the last input_ids token to generate the next hidden states and self attention kv cache\n",
    "# This optimization profile is used after the 1st decoder run.\n",
    "dec_profiles_generation = Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, 1),\n",
    "    max=(batch_size, 1),\n",
    ")\n",
    "\n",
    "self_attention_profile_generation = {\n",
    "    \"min\": (batch_size, num_heads, 1, embedding_size_per_head),\n",
    "    \"opt\": (batch_size, num_heads, opt_output_seq_len - 1, embedding_size_per_head),\n",
    "    \"max\": (batch_size, num_heads, max_output_length - 1, embedding_size_per_head),\n",
    "}\n",
    "\n",
    "for i in range(num_layers):\n",
    "    dec_profiles_context = dec_profiles_context.add(\n",
    "        f\"past_key_values.{i}.decoder.key\",\n",
    "        **self_attention_profile_context\n",
    "    ).add(\n",
    "        f\"past_key_values.{i}.decoder.value\",\n",
    "        **self_attention_profile_context\n",
    "    )\n",
    "\n",
    "    dec_profiles_generation = dec_profiles_generation.add(\n",
    "        f\"past_key_values.{i}.decoder.key\",\n",
    "        **self_attention_profile_generation\n",
    "    ).add(\n",
    "        f\"past_key_values.{i}.decoder.value\",\n",
    "        **self_attention_profile_generation\n",
    "    )\n",
    "\n",
    "# TensorRT accepts multiple optimization engines for the same model.\n",
    "# Profile 1 is only used in the first decoder iterations.\n",
    "decoder_profiles = [dec_profiles_generation, dec_profiles_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadf843-9f60-41c7-90a9-098b33ce3603",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "kv_engine_path = os.path.join(trt_engine_folder, f\"{GPT2_VARIANT}-kv_cache_{engine_tag}.engine\")\n",
    "\n",
    "# Set up the trt engine with both kv input/output augmented\n",
    "if not os.path.exists(kv_engine_path):\n",
    "    kv_gpt2_engine = GPT2ONNXFile(kv_onnx_path, kv_metadata).as_trt_engine(kv_engine_path,profiles=decoder_profiles, preview_features=preview_features)\n",
    "else:\n",
    "    kv_gpt2_engine = GPT2TRTEngine(kv_engine_path, kv_metadata)\n",
    "\n",
    "    \n",
    "kv_gpt2_trt = GPT2TRTDecoder(\n",
    "    kv_gpt2_engine, kv_metadata, kv_config, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090007db-9a09-4b6d-95ed-8a688ea05798",
   "metadata": {},
   "source": [
    "Since we have 2 profiles, benchmarking single-run runtime does not make sense. We instead use `full_inference` to measure the time for the entire inference cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b93d88-21bb-4f87-9ff6-709d0babdf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    kv_gpt2_trt, input_ids.cuda(), tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, use_cache = use_cache\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ab217-9ee4-435c-b689-69d98cef1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_gpt2_trt.reset()\n",
    "kv_sample_output = kv_gpt2_trt.generate(input_ids.cuda(), max_length=max_length)\n",
    "tokenizer.decode(kv_sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b614fb8-63d6-4711-84cf-c69ca8b3f141",
   "metadata": {},
   "source": [
    "In this short example, kv cache performance does not improve the performance, and may even be slightly worse than non kv cache mode. However, when we have larger input sequences for the model, it will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764049f-0578-4305-b010-4e7a3156a377",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "\n",
    "## 5. Advanced Topic: Beam Search\n",
    "\n",
    "Beam search is a way to increase the model quality. It looks for the top `num_beams` number of possible words and pick the one that conditions the best to the current position. Similarly, the original HuggingFace PyTorch model supports beam search natively, while we need to build separate trt engine for different `num_beams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5808db-2cc0-4d88-aebe-1b6e17a023e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_config = GPT2Config.from_pretrained(GPT2_VARIANT, use_cache = False)\n",
    "beam_metadata = NetworkMetadata(variant=GPT2_VARIANT, precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=False))\n",
    "num_beams = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1403609-b24d-4e10-a8eb-852d3eab6fa0",
   "metadata": {},
   "source": [
    "#### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd992c8-1eeb-427c-ae32-2c63766c6a69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    model, input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, num_beams = num_beams\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09418760-84bd-4308-b06b-8540945a6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(input_ids, max_length=max_length, num_beams = num_beams)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8d9fa-d74a-40dd-94ce-d98551d24608",
   "metadata": {},
   "source": [
    "You could see that the output is very different from the original one. If you change `num_beams`, the result will also change significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba01e0ec-68ad-4682-8ca4-2ecde7d70f7f",
   "metadata": {},
   "source": [
    "#### TensorRT\n",
    "It uses the same onnx file as the original configuration, but the engine set up is differently, because it expands the inputs by `num_beams` for the first dimension of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055fb314-8e0f-4edd-bf78-16890d196de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimization profile for dynamic shape input. Can modify batch_size / max_sequence_length to build engines for different shapes\n",
    "batch_size = 1\n",
    "disable_preview_dynamic_shapes = False # preview_dynamic_shapes optimizes the trt engine building time\n",
    "# We can either use input length as the optimal length, or use max_length // 2. \n",
    "# In T5 or BART, input_length is better, but in GPT-2, max_length // 2 is better because we need to generate max_length number of tokens\n",
    "\n",
    "use_input_length = False\n",
    "opt_length = input_id.shape[1] if use_input_length else max_length // 2 \n",
    "# Create different engine tags for different configurations\n",
    "engine_tag = f\"bs{batch_size}-beam{num_beams}\"\n",
    "\n",
    "preview_features = [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "if disable_preview_dynamic_shapes:\n",
    "    engine_tag += \"-disableFasterDynamicShapes\"\n",
    "    preview_features = []\n",
    "    \n",
    "\n",
    "beam_profiles = [Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, opt_length), # Optimized based on the inputs. \n",
    "    max=(batch_size * num_beams, max_length),\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18986d0f-9509-463f-a489-a76dd4d28a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd04a7b-8aa6-4c97-8d85-96f14b06abbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beam_engine_path = os.path.join(trt_engine_folder, f\"{GPT2_VARIANT}-{engine_tag}.engine\")\n",
    "if not os.path.exists(beam_engine_path):\n",
    "    beam_gpt2_engine = GPT2ONNXFile(onnx_path, beam_metadata).as_trt_engine(output_fpath=beam_engine_path, profiles=beam_profiles, preview_features=preview_features)\n",
    "else:\n",
    "    beam_gpt2_engine = GPT2TRTEngine(beam_engine_path, beam_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe1dba-4e84-478e-9ea7-07c21856e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_gpt2_trt = GPT2TRTDecoder(beam_gpt2_engine, beam_metadata, beam_config, num_beams = num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42614e14-c962-4c31-a469-7e0343efbdbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    beam_gpt2_trt, input_ids.cuda(), tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, num_beams=num_beams\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ab05a-fe0d-42c3-9591-605ddab389ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_sample_output = beam_gpt2_trt.generate(input_ids.cuda(), max_length=max_length, num_beams=num_beams)\n",
    "tokenizer.decode(beam_sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543dbfd-4650-46f5-8f77-587dcb05785a",
   "metadata": {},
   "source": [
    "We could see that because of larger batch size, beam search will take slightly longer, but for most sequences, it will generate more meaningful outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc6c04-ca47-4fc6-9a12-ed500722bb4a",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch GPT-2 model to an optimized TensorRT engine for inference in 3 easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace GPT-2 model while providing significant speed up. \n",
    "\n",
    "If you are interested in further details of the conversion process, check out [GPT2/trt.py](../GPT2/trt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14079b8f-738e-4137-9ca3-6a4254e8f006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
